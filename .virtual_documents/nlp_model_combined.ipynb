## 경로 설정 
import os
from datetime import datetime, timezone

# 파라미터
BUILDING_ID = int(os.getenv("BUILDING_ID", "95"))
START_DATE  = os.getenv("START_DATE", "2025-12-01")
END_DATE    = os.getenv("END_DATE",   "2026-01-01")

# 파일 규칙
YYYYMM = START_DATE.replace("-", "")[:6]
RUN_ID = os.getenv("RUN_ID", datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ"))

# 기준 루트
BASE_DIR = os.getenv("BASE_DIR", "/home/ssm-user/jupyter")

NOTEBOOK_DIR = os.path.join(BASE_DIR, "notebooks")
OUT_BASE = os.path.join(BASE_DIR, "output")
LOG_DIR = os.path.join(BASE_DIR, "logs")

TAGGING_DIR = os.path.join(OUT_BASE, "tagging")
HTML_DIR = os.path.join(OUT_BASE, "html")
RUNS_DIR = os.path.join(OUT_BASE, "runs")

os.makedirs(NOTEBOOK_DIR, exist_ok=True)
os.makedirs(TAGGING_DIR, exist_ok=True)
os.makedirs(HTML_DIR, exist_ok=True)
os.makedirs(RUNS_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

TAGGING_CSV_PATH = os.getenv(
    "TAGGING_CSV_PATH",
    os.path.join(TAGGING_DIR, f"tagged_{BUILDING_ID}_{YYYYMM}_{RUN_ID}.csv")
)

HTML_PATH = os.getenv(
    "HTML_PATH",
    os.path.join(HTML_DIR, f"dashboard_{BUILDING_ID}_{YYYYMM}_{RUN_ID}.html")
)

LOG_PATH = os.getenv(
    "LOG_PATH",
    os.path.join(LOG_DIR, f"run_{BUILDING_ID}_{YYYYMM}_{RUN_ID}.log")
)



import os
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from datetime import datetime, timezone

# =========================================
# 0) 배치 파라미터 / DB 환경변수
# =========================================
BUILDING_ID = int(os.getenv("BUILDING_ID", "95"))
START_DATE  = os.getenv("START_DATE", "2025-12-01")  # inclusive
END_DATE    = os.getenv("END_DATE",   "2026-01-01")  # exclusive

RUN_ID = os.getenv("RUN_ID", datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ"))

OUT_DIR = os.getenv("OUT_DIR", "./output")
os.makedirs(OUT_DIR, exist_ok=True)
OUT_CSV_PATH = os.getenv("OUT_CSV_PATH", f"{OUT_DIR}/tagged_{BUILDING_ID}_{START_DATE}_{END_DATE}_{RUN_ID}.csv")

DB_HOST = os.getenv("DB_HOST", "")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "")
DB_USER = os.getenv("DB_USER", "")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")

if not (DB_HOST and DB_NAME and DB_USER and DB_PASSWORD):
    raise RuntimeError("DB 환경변수가 비어 있습니다. DB_HOST/DB_NAME/DB_USER/DB_PASSWORD를 설정해 주세요.")


# =========================================
# 1) Result 1 조회 (DB -> DataFrame)
#    reply_writer 포함
# =========================================
RESULT1_SQL = """
SELECT
    v.id AS voc_id,
    v.building_id,
    v.voc_date,
    v.title,
    v.request_contents,

    bf.name AS building_floor_name,
    z.name AS zone_name,

    r.reply,
    r.reply_write_date,
    bug.name AS user_group_name,
    a.name AS reply_writer

FROM voc v
LEFT JOIN building_floor bf
  ON v.building_floor_id = bf.id
LEFT JOIN building_floor_zone z
  ON v.building_floor_zone_id = z.id
LEFT JOIN (
    SELECT DISTINCT ON (voc_id) *
    FROM voc_reply
    ORDER BY voc_id, reply_write_date DESC
) r
  ON v.id = r.voc_id
LEFT JOIN account_group ag
  ON r.reply_writer_id = ag.account_id
LEFT JOIN building_user_group bug
  ON ag.group_id = bug.id
LEFT JOIN account a
  ON r.reply_writer_id = a.id
WHERE v.building_id = %(building_id)s
  AND v.title NOT LIKE '%%테스트%%'
  AND v.voc_date >= %(start_date)s
  AND v.voc_date <  %(end_date)s
ORDER BY v.voc_date ASC, v.id ASC;
"""

def fetch_result1_df(building_id: int, start_date: str, end_date: str) -> pd.DataFrame:
    conn = psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD
    )
    try:
        df = pd.read_sql_query(
            RESULT1_SQL,
            conn,
            params={"building_id": building_id, "start_date": start_date, "end_date": end_date},
        )
    finally:
        conn.close()

    # 태깅에서 바로 쓰기 좋게 빈 값 정리
    fill_cols = [
        "title", "request_contents", "reply", "reply_writer",
        "building_floor_name", "zone_name", "user_group_name"
    ]
    for c in fill_cols:
        if c in df.columns:
            df[c] = df[c].fillna("")

    return df


# =========================================
# 2) taxonomy 조회 (voc_taxonomy -> DataFrame)
#    엑셀 컬럼명과 동일하게 맞춤: 대분류/중분류/사례/키워드
# =========================================
def fetch_taxonomy_dfs(version: str | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:
    conn = psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD
    )
    try:
        if version:
            tax_df = pd.read_sql_query(
                """
                SELECT taxonomy_type, major, minor, keywords, priority
                FROM voc_taxonomy
                WHERE is_active = true
                  AND version = %(version)s
                """,
                conn,
                params={"version": version}
            )
        else:
            tax_df = pd.read_sql_query(
                """
                SELECT taxonomy_type, major, minor, keywords, priority
                FROM voc_taxonomy
                WHERE is_active = true
                """,
                conn
            )
    finally:
        conn.close()

    subj = tax_df[tax_df["taxonomy_type"] == "SUBJECT"].copy()
    work = tax_df[tax_df["taxonomy_type"] == "WORK"].copy()

    # 기존 태깅 로직(build_taxonomy)이 엑셀 컬럼명을 기대하므로 rename
    subj = subj.rename(columns={"major": "대분류", "minor": "중분류", "keywords": "사례/키워드"})
    work = work.rename(columns={"major": "대분류", "minor": "중분류", "keywords": "사례/키워드"})

    # dtype=str 통일(기존 코드가 dtype=str로 읽던 것과 동일 효과)
    for df in (subj, work):
        for c in ["대분류", "중분류", "사례/키워드"]:
            if c in df.columns:
                df[c] = df[c].fillna("").astype(str)

    # 빈 행 제거
    subj = subj[(subj["대분류"] != "") & (subj["중분류"] != "") & (subj["사례/키워드"] != "")]
    work = work[(work["대분류"] != "") & (work["중분류"] != "") & (work["사례/키워드"] != "")]

    return subj, work


# =========================================
# 3) 엑셀 의존 제거 버전: run_retagging_df
#    (기존 run_retagging()의 "엑셀 읽는 3줄"만 제거한 형태)
# =========================================
def run_retagging_df(
    result_df: pd.DataFrame,
    subject_tax: pd.DataFrame,
    work_tax: pd.DataFrame,
    # 분류기 파라미터 (기존 run_retagging과 동일)
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    decay_factor: float = 0.95,
    allow_multi: bool = False,
    gt_cols=None,
    show_progress: bool = True,
    show_logs: bool = True,
    log_every: int = 1,
    show_best_only: bool = True,
    # 사전 주입
    compound_words: dict = None,
    stop_words: set = None
) -> pd.DataFrame:
    """
    엑셀 없이 DataFrame 기반으로 재태깅 수행
    반환값: 갱신된 Result 1 데이터프레임
    """
    compound_words = compound_words or {}
    stop_words = stop_words or set()
    compound_replacers = build_compound_replacers(compound_words)

    # 필수 컬럼 체크
    for col in ["title", "request_contents", "reply"]:
        if col not in result_df.columns:
            raise KeyError(f"필수 컬럼이 없습니다: {col}")

    # SettingWithCopyWarning 방지 위해 copy + .loc
    result_df = result_df.copy()

    # all_text 구성 (원문 보존) - 기존 run_retagging과 동일
    result_df.loc[:, "title_clean"] = result_df["title"].map(whitelist_text).fillna("")
    result_df.loc[:, "all_text"] = (
        result_df["request_contents"].map(whitelist_text).fillna("") + " " +
        result_df["title_clean"] + " " +
        result_df["reply"].map(whitelist_text).fillna("")
    ).str.strip()

    # taxonomy 아이템 빌드 (기존 함수 사용)
    subject_items = build_taxonomy(subject_tax, compound_replacers, stop_words)
    work_items    = build_taxonomy(work_tax, compound_replacers, stop_words)
    subject_etc   = find_tax_etc(subject_items)
    work_etc      = find_tax_etc(work_items)

    # 분류 실행 (기존 notebook에 있는 classify_df를 그대로 사용)
    # classify_df는 result_df의 all_text/title_clean 등을 사용해서 *_pred 컬럼을 생성합니다.
    tagged_df = classify_df(
        result_df=result_df,
        subject_items=subject_items,
        work_items=work_items,
        subject_etc=subject_etc,
        work_etc=work_etc,
        pos_max=pos_max,
        pos_min=pos_min,
        title_boost=title_boost,
        title_overlap_ratio=title_overlap_ratio,
        short_len=short_len,
        long_len=long_len,
        short_penalty=short_penalty,
        long_penalty=long_penalty,
        decay_steps=decay_steps,
        decay_factor=decay_factor,
        allow_multi=allow_multi,
        gt_cols=gt_cols,
        show_progress=show_progress,
        show_logs=show_logs,
        log_every=log_every,
        show_best_only=show_best_only
    )

    return tagged_df


# =========================================
# 4) 실행: DB -> 태깅 -> CSV 저장
# =========================================
compound_words, stop_words = load_text_dict_from_db()

voc_df = fetch_result1_df(BUILDING_ID, START_DATE, END_DATE)
subject_tax_df, work_tax_df = fetch_taxonomy_dfs(version=os.getenv("TAXONOMY_VERSION") or None)

print(f"[INFO] voc_df rows={len(voc_df)} cols={len(voc_df.columns)}")
print(f"[INFO] subject_tax rows={len(subject_tax_df)} / work_tax rows={len(work_tax_df)}")

tagged_df = run_retagging_df(
    result_df=voc_df,
    subject_tax=subject_tax_df,
    work_tax=work_tax_df,
    compound_words=compound_words,
    stop_words=stop_words
)

# run_id/tagged_at 같이 넣어두면 이후 2번 노트북이 CSV 하나만 읽고도 충분히 작업 가능합니다.
tagged_df = tagged_df.copy()
tagged_df.loc[:, "run_id"] = RUN_ID
tagged_df.loc[:, "tagged_at"] = datetime.now(timezone.utc).isoformat()

tagged_df.to_csv(OUT_CSV_PATH, index=False, encoding="utf-8-sig")
print(f"[INFO] saved csv: {OUT_CSV_PATH}")

tagged_df.head(3)




