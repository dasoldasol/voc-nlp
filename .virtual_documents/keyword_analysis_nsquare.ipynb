


import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import matplotlib as mpl

# ì„¤ì¹˜ëœ ë‚˜ëˆ”ê³ ë”• ê²½ë¡œ í™•ì¸ í›„ ë“±ë¡
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
fm.fontManager.addfont(font_path)

# í°íŠ¸ ì´ë¦„ í™•ì¸
nanum_font = fm.FontProperties(fname=font_path).get_name()
# print(f"í°íŠ¸ ì´ë¦„: {nanum_font}")  # 'NanumGothic' ë‚˜ì™€ì•¼ í•¨

# matplotlibì— ì ìš©
plt.rc('font', family=nanum_font)
mpl.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€


from IPython.core.display import HTML

# ë…¸íŠ¸ë¶ CSS ìµœì í™”
HTML("""
<style>
img {
    max-width: 100%;
    height: auto;
}
</style>
""")


print("""
í˜„ì¥ëª… : ë‚¨ì‚°ìŠ¤í€˜ì–´
í˜„ì¥ì†Œì¥ : í™©í˜œì„± 
ë³´ê³ ëŒ€ìƒì›” : 2025.08
""")





import pandas as pd 
import psycopg2

DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'

conn = psycopg2.connect(
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        host=DB_HOST
    )

cursor = conn.cursor()

query = """
SELECT
    v.*,
    bf.name AS building_floor_name,
    v.building_floor_zone_id,
    z.name AS zone_name,
    r.*,
    bug.name AS user_group_name,
    a.name AS reply_writer
FROM voc v
         LEFT JOIN building_floor bf ON v.building_floor_id = bf.id
         LEFT JOIN building_floor_zone z ON v.building_floor_zone_id = z.id
         LEFT JOIN (
    SELECT DISTINCT ON (voc_id) *
    FROM voc_reply
    ORDER BY voc_id, reply_write_date DESC  -- ìµœì‹  ì‘ë‹µ í•˜ë‚˜ë§Œ
) r ON v.id = r.voc_id
         LEFT JOIN account_group ag ON r.reply_writer_id = ag.account_id
         LEFT JOIN building_user_group bug ON ag.group_id = bug.id
         LEFT JOIN account a ON r.reply_writer_id = a.id
WHERE v.building_id = 307 and title not like '%í…ŒìŠ¤íŠ¸%'
and voc_date > '2025-08-01' and voc_date < '2025-09-01'
""" 
cursor.execute(query)
rows = cursor.fetchall()
colnames = [desc[0] for desc in cursor.description]

df = pd.DataFrame(rows, columns=colnames, dtype=str)

cursor.close()
conn.close()


df.columns


from datetime import datetime


#  ë¬¸ìí˜• ì»¬ëŸ¼
text_columns = ['voc_user_name', 'voc_user_phone', 'request_contents', 'state', 'title', 
                'building_floor_name', 'zone_name', 'reply', 'user_group_name', 'reply_writer']
for col in text_columns:
    df[col] = df[col].astype(str)

#  ë²”ì£¼í˜• ì»¬ëŸ¼
category_columns = ['user_group_name', 'building_floor_name', 'zone_name', 'user_group_name']
for col in category_columns:
    df[col] = df[col].astype('category')

#  ë‚ ì§œ ë³€í™˜
date_columns = [col for col in df.columns if 'date' in col.lower()]
df[date_columns] = df[date_columns].apply(pd.to_datetime)



# ì „ì²´ í–‰ ìˆ˜ì™€ ì»¬ëŸ¼ ìˆ˜ ì¶œë ¥
# df = pd.read_excel("VOC_á„‡á…®á†«á„‰á…¥á†¨_á„‹á…­á„á…¥á†¼á„€á…¥á†«á„‹á…´á„á…©á†¼á„’á…¡á†¸_á„€á…§á†¯á„€á…ª.xlsx")
print(f"ì „ì²´ ë°ì´í„° ìˆ˜: {df.shape[0]}")

# 'ì ‘ìˆ˜ì‹œê°„' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
df['write_date'] = pd.to_datetime(df['write_date'])

# ì›ë˜ min, max
min_time = df['write_date'].min()
max_time = df['write_date'].max()

print(f"\n[ì›ì‹œë°ì´í„° ê¸°ì¤€]")
print(f"ìµœì†Œ ì ‘ìˆ˜ì‹œê°„: {min_time}")
print(f"ìµœëŒ€ ì ‘ìˆ˜ì‹œê°„: {max_time}")

# í˜„ì¬ ì‹œê° ê¸°ì¤€ í•„í„°ë§
# now = pd.Timestamp.now()
# df_past = df[df['write_date'] <= now]

# print(f"í‘œì¤€í™” ë°ì´í„° ìˆ˜: {df_past.shape[0]}")

# # ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ë§Œ ì¶œë ¥
# if not df_past.empty:
#     min_past = df_past['write_date'].min()
#     max_past = df_past['write_date'].max()

#     print(f"\n[í‘œì¤€í™”ë°ì´í„° ê¸°ì¤€]")
#     print(f"ìµœì†Œ ì ‘ìˆ˜ì‹œê°„: {min_past}")
#     print(f"ìµœëŒ€ ì ‘ìˆ˜ì‹œê°„: {max_past}")
# else:
#     print("\ní˜„ì¬ë³´ë‹¤ ê³¼ê±°ì— í•´ë‹¹í•˜ëŠ” ì ‘ìˆ˜ì‹œê°„ì´ ì—†ìŠµë‹ˆë‹¤.")








from datetime import datetime
import pandas as pd

# í˜„ì¬ ì‹œê°„
now = datetime.now()

# ë‚ ì§œ ì»¬ëŸ¼ ë²”ìœ„ ê³„ì‚°
date_range = {}
for col in ['write_date', 'reply_write_date']:
    valid_dates = df[col].dropna()
    # valid_dates = valid_dates[valid_dates <= now]
    if not valid_dates.empty:
        date_range[col] = (valid_dates.min(), valid_dates.max())
    else:
        date_range[col] = (None, None)

# DataFrame ìƒì„± í›„ ì»¬ëŸ¼ëª… ë³€ê²½
date_range_df = pd.DataFrame(date_range, index=['ìµœì´ˆì¼ì‹œ', 'ìµœì¢…ì¼ì‹œ'])
date_range_df = date_range_df.rename(columns={
    'write_date': 'ë°œìƒì¼ì‹œ',
    'reply_write_date': 'ì²˜ë¦¬ì¼ì‹œ'
})

date_range_df





import matplotlib.pyplot as plt
import pandas as pd

# 1. date_range ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì¼ ë²”ìœ„ ìƒì„±
start_date = date_range['write_date'][0].normalize()
end_date = date_range['write_date'][1].normalize()

all_days = pd.date_range(start=start_date, end=end_date, freq='D')

# 2. ì¼ë³„ VOC ë°œìƒ ê±´ìˆ˜ ê³„ì‚°
daily_counts = df['write_date'].dt.normalize().value_counts().sort_index()

# 3. ë¹ ì§„ ë‚ ì§œ ë³´ì¶© (ë¹ˆ ë‚ ì§œëŠ” 0ìœ¼ë¡œ)
daily_counts = daily_counts.reindex(all_days, fill_value=0)

# 4. ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ xì¶•ì— ì‚¬ìš©
daily_counts.index = daily_counts.index.strftime('%Y-%m-%d')

# 5. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
bars = plt.bar(daily_counts.index, daily_counts.values)

# ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ (ì„ íƒ)
for bar in bars:
    height = bar.get_height()
    if height > 0:
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            height,
            f'{int(height)}',
            ha='center',
            va='bottom'
        )

plt.title('ì¼ë³„ VOC ë°œìƒ ê±´ìˆ˜')
plt.xlabel('ë‚ ì§œ')
plt.ylabel('ê±´ìˆ˜')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()





import matplotlib.pyplot as plt
import pandas as pd
import re

# ì›ë³¸ ì „ì²´ ì¸µ ë¦¬ìŠ¤íŠ¸
all_floors = [
    'B3F', 'B1F', '2F', '3F', 'B2F', '1F', '5F', '7F', '14F', '23F', '24F', '6F',
    '8F', '10F', '12F', '9F', '13F', '15F', '19F', '21F', '22F', '11F', '16F',
    '17F', '18F', '20F', 'PH1', 'PH2'
]

# 1. ì¸µ ì •ë ¬ í•¨ìˆ˜ ì •ì˜
def floor_sort_key(floor):
    # ì§€í•˜ì¸µ: Bìˆ«ì â†’ ìŒìˆ˜ë¡œ ë³€í™˜í•´ì„œ ê°€ì¥ ì•ì— ì˜¤ê²Œ
    if floor.startswith('B'):
        num = int(re.findall(r'\d+', floor)[0])
        return -num
    # ì˜¥íƒ‘ì¸µ: PHìˆ«ì â†’ ë§¤ìš° í° ê°’ìœ¼ë¡œ í•´ì„œ ë§¨ ë’¤ì— ì˜¤ê²Œ
    elif floor.startswith('PH'):
        num = int(re.findall(r'\d+', floor)[0])
        return 1000 + num
    # ì¼ë°˜ì¸µ: ìˆ«ìì¸µ â†’ ê·¸ëŒ€ë¡œ ìˆ«ì
    else:
        num = int(re.findall(r'\d+', floor)[0])
        return num

# 2. ì •ë ¬ ì ìš©
sorted_floors = sorted(all_floors, key=floor_sort_key)

# 3. ë°œìƒìœ„ì¹˜ë³„ VOC ê±´ìˆ˜ ì§‘ê³„
location_counts = df['building_floor_name'].value_counts()

# 4. ê±´ìˆ˜ê°€ 1 ì´ìƒì¸ í•­ëª©ë§Œ í•„í„°ë§
location_counts = location_counts[location_counts > 1]

# 5. ì „ì²´ ì¸µ ê¸°ì¤€ìœ¼ë¡œ ëˆ„ë½ ì¸µ 0ìœ¼ë¡œ ì±„ì›€
location_counts_all = pd.Series(index=sorted_floors, dtype=int)
location_counts_all = location_counts_all.fillna(0)
location_counts_all.update(location_counts)

# 6. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 8))
bars = plt.barh(location_counts_all.index.astype(str), location_counts_all.values, color='skyblue')

# 7. ë¼ë²¨ ë¶™ì´ê¸°
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2,
             f'{int(width)}',
             ha='left', va='center')

# 8. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •
plt.title('ì¸µë³„ VOC ë°œìƒ ê±´ìˆ˜')
plt.xlabel('ê±´ìˆ˜')
plt.ylabel('ë°œìƒìœ„ì¹˜')
plt.tight_layout()
plt.show()





# 1. reply_write_dateê°€ ìˆê³ , user_group_nameì´ "None"ì´ ì•„ë‹Œ ê²½ìš° í•„í„°ë§ + ë³µì‚¬ë³¸ ìƒì„±
filtered_df = df[
    (df['reply_write_date'].notnull()) &
    (df['user_group_name'].astype(str).str.strip().str.lower() != 'none')
].copy()  # â† ì´ê±¸ ì¶”ê°€í•˜ë©´ ê²½ê³  ì‚¬ë¼ì§

# 2. Categorical íƒ€ì…ì´ë©´ ë¬¸ìì—´ë¡œ ë³€í™˜ (í•„ìš” ì‹œ)
filtered_df['user_group_name'] = filtered_df['user_group_name'].astype(str)

# 3. íŒ€ë³„ ê±´ìˆ˜ ì§‘ê³„
top20_final = filtered_df['user_group_name'].value_counts()

# 4. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
ax = top20_final.plot(kind='bar')
plt.title("íŒ€ë³„ VOC ì²˜ë¦¬ ê±´ìˆ˜ (ë‹µë³€ ì‘ì„± ê±´ìˆ˜)")
plt.xlabel("ë‹´ë‹¹íŒ€")
plt.ylabel("ê±´ìˆ˜")
plt.xticks(rotation=45)
plt.tight_layout()

# 5. ë§‰ëŒ€ ìœ„ ìˆ«ì í‘œì‹œ
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{int(height)}',
                xy=(p.get_x() + p.get_width() / 2, height),
                xytext=(0, 5),
                textcoords='offset points',
                ha='center', va='bottom')

plt.show()





import matplotlib.pyplot as plt
import pandas as pd

# 1. ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
df['ìš”ì¼'] = df['write_date'].dt.dayofweek  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼

# 2. ìš”ì¼ ì´ë¦„ ë§¤í•‘
weekday_map = {0: 'ì›”ìš”ì¼', 1: 'í™”ìš”ì¼', 2: 'ìˆ˜ìš”ì¼', 3: 'ëª©ìš”ì¼', 4: 'ê¸ˆìš”ì¼', 5: 'í† ìš”ì¼', 6: 'ì¼ìš”ì¼'}
df['ìš”ì¼'] = df['ìš”ì¼'].map(weekday_map)

# 3. ìš”ì¼ ìˆœì„œ ì •ì˜
weekday_order = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']

# 4. groupby + size + reindex
weekday_counts = df.groupby('ìš”ì¼').size().reindex(weekday_order, fill_value=0)

# âœ… ì¸ë±ìŠ¤ë¥¼ CategoricalIndex ë¡œ ê°•ì œ
weekday_counts.index = pd.CategoricalIndex(weekday_counts.index, categories=weekday_order, ordered=True)

# 5. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
bars = plt.barh(weekday_counts.index.astype(str), weekday_counts.values, color='skyblue')

# 6. ë§‰ëŒ€ ìœ„ì— ê±´ìˆ˜ ë¼ë²¨ ë¶™ì´ê¸°
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2,
             f'{int(width)}',
             ha='left', va='center')

# âœ… yì¶• ë’¤ì§‘ì–´ì„œ ì›” â†’ ì¼ ìˆœì„œë¡œ!
plt.gca().invert_yaxis()

# 7. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •
plt.title('ìš”ì¼ë³„ VOC ë°œìƒ ê±´ìˆ˜')
plt.xlabel('ê±´ìˆ˜')
plt.ylabel('ìš”ì¼')
plt.tight_layout()
plt.show()






import matplotlib.pyplot as plt
import pandas as pd

# ë‚ ì§œ ì»¬ëŸ¼ íŒŒì•…
date_columns = [col for col in df.columns if 'date' in col.lower()]

# ì•ˆì „í•˜ê²Œ ë‚ ì§œí˜•ìœ¼ë¡œ ë³€í™˜
for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors='coerce')

# 1. ì²˜ë¦¬ ì†Œìš” ì‹œê°„ ì»¬ëŸ¼ ìƒì„±
df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'] = df['reply_write_date'] - df['write_date']

# 2. ì¼ì ë‹¨ìœ„ë¡œ ë³€í™˜
df['ì ‘ìˆ˜ì¼'] = df['write_date'].dt.date

# 3. ì¼ë³„ í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„ ê³„ì‚°
daily_processing_time = df.groupby('ì ‘ìˆ˜ì¼')['ì²˜ë¦¬ì†Œìš”ì‹œê°„'].mean().sort_index()

# 4. ì „ì²´ ì¼ì ë²”ìœ„ ìƒì„± (ì—¬ê¸° ìˆ˜ì •)
start_date = df['write_date'].min().date()
end_date = df['write_date'].max().date()
all_dates = pd.date_range(start=start_date, end=end_date, freq='D')

# 5. ë¹ ì§„ ì¼ìëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
daily_processing_time = daily_processing_time.reindex(all_dates.date, fill_value=pd.Timedelta(0))


# 6. ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ xì¶•ìœ¼ë¡œ ì‚¬ìš©
daily_processing_time.index = daily_processing_time.index.astype(str)

# 7. timedelta â†’ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜
processing_time_minutes = daily_processing_time.apply(lambda x: x.total_seconds() / 60)

# 8. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
plt.plot(processing_time_minutes.index, processing_time_minutes.values, marker='o', linestyle='-', color='skyblue')
average_processing_time = processing_time_minutes.mean()
plt.axhline(average_processing_time, color='red', linestyle='--', label=f'í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {average_processing_time:.1f}ë¶„')

# 9. ê° ì ì— ë¼ë²¨ (ì„ íƒì : ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ ê°€ëŠ¥)
# for x, y in zip(processing_time_minutes.index, processing_time_minutes.values):
#     if y > 0:
#         plt.text(x, y, f'{y:.1f}ë¶„', ha='center', va='bottom', fontsize=8)

# 10. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •
plt.title('ì¼ë³„ í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„')
plt.xlabel('ì¼ì')
plt.ylabel('í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„ (ë¶„)')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()



import re, html
from datetime import timedelta

# HTML ì œê±° ì „ì²˜ë¦¬ í•¨ìˆ˜
def strip_html(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = str(text)
    s = html.unescape(s)
    # <script>/<style> ë¸”ë¡ ì œê±°
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)
    # ë‚˜ë¨¸ì§€ íƒœê·¸ ì œê±°
    s = re.sub(r"(?s)<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# timedelta â†’ "xì¼ yì‹œê°„ zë¶„" í˜•ì‹ (0ì¼ì€ ìƒëµ)
def format_timedelta(td: timedelta):
    if pd.isnull(td):
        return None
    days = td.days
    hours, remainder = divmod(td.seconds, 3600)
    minutes, _ = divmod(remainder, 60)

    parts = []
    if days > 0:
        parts.append(f"{days}ì¼")
    if hours > 0:
        parts.append(f"{hours}ì‹œê°„")
    if minutes > 0:
        parts.append(f"{minutes}ë¶„")

    return " ".join(parts) if parts else "0ë¶„"

# ì²˜ë¦¬ì†Œìš”ì‹œê°„ ê¸°ì¤€ ìƒìœ„ 5ê±´ ì¶”ì¶œ
top5_cases = df.sort_values(by='ì²˜ë¦¬ì†Œìš”ì‹œê°„', ascending=False).head(5)

# ì›í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ í›„ ë³´ê¸° ì¢‹ì€ í•œê¸€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
top5_df = top5_cases[['write_date', 'title', 'request_contents', 'reply', 'reply_write_date', 'ì²˜ë¦¬ì†Œìš”ì‹œê°„']].rename(
    columns={
        'write_date': 'ì ‘ìˆ˜ì¼',
        'title': 'VOCì œëª©',
        'request_contents': 'VOCë‚´ìš©',
        'reply': 'ë‹µë³€',
        'reply_write_date': 'ë‹µë³€ì™„ë£Œì‹œê°„',
        'ì²˜ë¦¬ì†Œìš”ì‹œê°„': 'ì²˜ë¦¬ì†Œìš”ì‹œê°„'
    }
)

# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì „ì²˜ë¦¬ (HTML/ê³µë°± ì œê±°)
for col in ['VOCì œëª©', 'VOCë‚´ìš©', 'ë‹µë³€']:
    top5_df[col] = top5_df[col].apply(strip_html)

# ì²˜ë¦¬ì†Œìš”ì‹œê°„ í¬ë§· ì ìš©
top5_df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'] = top5_df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'].apply(format_timedelta)

# ì ‘ìˆ˜ì¼/ë‹µë³€ì™„ë£Œì‹œê°„ì„ ë¶„ ë‹¨ìœ„ê¹Œì§€ë§Œ í‘œì‹œ
top5_df['ì ‘ìˆ˜ì¼'] = pd.to_datetime(top5_df['ì ‘ìˆ˜ì¼'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M')
top5_df['ë‹µë³€ì™„ë£Œì‹œê°„'] = pd.to_datetime(top5_df['ë‹µë³€ì™„ë£Œì‹œê°„'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M')

print("ì²˜ë¦¬ì†Œìš”ì‹œê°„ TOP 5")
display(top5_df)






## Load Data & Preprocessing 

import pandas as pd
import re
import html

# 0) ì—‘ì…€ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ: ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©
xlsx_path = "nsquare_2508_tagged.xlsx"
df = pd.read_excel(
    xlsx_path,
    sheet_name="Result 1",
    header=0,          # ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ
    dtype=str          # í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìœ„í•´ ë¬¸ìì—´ë¡œ í†µì¼
)
df.columns = [str(c).strip() for c in df.columns]  # ì»¬ëŸ¼ëª… ê³µë°± ì •ë¦¬

# í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
required = ["request_contents", "title", "reply"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")

# 1.2) HTML ì œê±° í•¨ìˆ˜
def strip_html(text: str) -> str:
    if text is None or pd.isna(text):
        return ""
    s = str(text)
    s = html.unescape(s)  # ì—”í‹°í‹° ë””ì½”ë”©
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)  # script/style ì œê±°
    s = re.sub(r"(?s)<[^>]+>", " ", s)  # íƒœê·¸ ì œê±°
    s = html.unescape(s)  # ì—”í‹°í‹° í•œ ë²ˆ ë” ë””ì½”ë”©
    s = re.sub(r"\s+", " ", s).strip()  # ê³µë°± ì •ê·œí™”
    return s

# 1.1) request_contents + title + reply ëª¨ë‘ í•©ì¹œ ì»¬ëŸ¼ ìƒì„±
df.loc[:, "keyword_text"] = (
    df["request_contents"].map(strip_html).fillna("") + " " +
    df["title"].map(strip_html).fillna("") + " " +
    df["reply"].map(strip_html).fillna("")
).str.strip()

# í™•ì¸ìš©(ì„ íƒ)
# print(df[["keyword_text"]].head())




## ë‹¨ì–´ ë¹ˆë„ë¶„ì„ (ì¼ë‹¨ ì œì™¸)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import re
import unicodedata
import html
from collections import Counter

# 0) ì„ íƒ: Komoran ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ìë™ íŒë³„
try:
    import konlpy  # ë‹¨ìˆœ ì¡´ì¬ í™•ì¸
    USE_KOMORAN = True
except Exception:
    USE_KOMORAN = False

# 1. í°íŠ¸ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_name = fm.FontProperties(fname=font_path).get_name()
plt.rc("font", family=font_name)
sns.set(font=font_name, rc={"axes.unicode_minus": False})

# 2) df/keyword_text ì¤€ë¹„ ì—¬ë¶€ í™•ì¸(ì´ì „ì— ë§Œë“¤ì–´ì§„ dfê°€ ì—†ë‹¤ë©´ ë¡œë“œ ë° í´ë°± ìƒì„±)
if "df" not in globals() or "keyword_text" not in getattr(globals().get("df", pd.DataFrame()), "columns", []):
    # ì—‘ì…€ì—ì„œ Result 1 ì‹œíŠ¸ ë¡œë“œ (ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©)
    df = pd.read_excel("nsquare_2508_tagged.xlsx", sheet_name="Result 1", header=0, dtype=str)
    df.columns = [str(c).strip() for c in df.columns]

    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    for col in ["request_contents", "title", "reply"]:
        if col not in df.columns:
            raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")

    # HTML ì œê±° + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì •ì œ í´ë°±(ì´ë¯¸ keyword_textê°€ ìˆë‹¤ë©´ ì´ ë‹¨ê³„ëŠ” ìƒëµë  ì˜ˆì •)
    def strip_html(text: str) -> str:
        if text is None or pd.isna(text):
            return ""
        s = str(text)
        s = html.unescape(s)
        s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)  # script/style ì‚­ì œ
        s = re.sub(r"(?s)<[^>]+>", " ", s)                       # ëª¨ë“  íƒœê·¸ ì‚­ì œ
        s = html.unescape(s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    ALLOWED_PATTERN = re.compile(r"[^ê°€-í£ã„±-ã…ã…-ã…£A-Za-z0-9\s\.\,\!\?\;\:\'\"\(\)\[\]\{\}\-\_\/\&\+\%\#\@]")
    def whitelist_text(text: str) -> str:
        if text is None or pd.isna(text):
            return ""
        s = strip_html(text)
        s = unicodedata.normalize("NFKC", s)
        s = re.sub(r"[\u200B-\u200D\uFE0E\uFE0F]", "", s)  # ì œë¡œí­/Variation Selector ì œê±°
        s = ALLOWED_PATTERN.sub(" ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    # í´ë°±: request_contents + title + reply í•©ì³ keyword_text ìƒì„±
    df.loc[:, "keyword_text"] = (
        df["request_contents"].map(whitelist_text).fillna("") + " " +
        df["title"].map(whitelist_text).fillna("") + " " +
        df["reply"].map(whitelist_text).fillna("")
    ).str.strip()


# === DBì—ì„œ ë³µí•©ì–´/ë¶ˆìš©ì–´ ë¶ˆëŸ¬ì˜¤ê¸° ===

DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'


def load_text_dict_from_db():
    import psycopg2
    try:
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()

        # compound_words: key, value ëª¨ë‘ strip
        cursor.execute("""
            SELECT word_key, word_value 
            FROM stat_text_dict 
            WHERE type = 'compound' AND is_active = TRUE
        """)
        compound_result = cursor.fetchall()
        compound_words = {
            row[0].strip(): row[1].strip()
            for row in compound_result if row[0] and row[1]
        }

        # stop_words
        cursor.execute("""
            SELECT word_key 
            FROM stat_text_dict 
            WHERE type = 'stop' AND is_active = TRUE
        """)
        stop_result = cursor.fetchall()
        stop_words = {row[0].strip() for row in stop_result if row[0]}

        cursor.close()
        connection.close()
        return compound_words, stop_words

    except Exception as e:
        # ì—°ê²° ì‹¤íŒ¨ ì‹œ ë¹ˆ ì‚¬ì „/ì…‹ìœ¼ë¡œ ì§„í–‰
        return {}, set()

compound_words, stop_words = load_text_dict_from_db()

# ìˆ˜ë™ ë¶ˆìš©ì–´ ë° ë³µí•©ì–´ ì‚¬ì „ ì ìš© í•¨ìˆ˜
def apply_manual_dicts(compound_words, stop_words):
    manual_stop_words = {
        "í›„", "ë‹¬ë¼", "ì—†ë‹¤ë‡¨", "ì²«", "ë¬¼ì´", "ê²¨", "ì²˜ë¦¬", "ì¡°ì¹˜", "ë™ì•ˆ", "ìœ„í•´",
        "ë°”ë¡œ", "ë‚´", "ì‹œ", "ì", "ì", "í¸", "í˜¸", "ì‹¤", "ìˆ˜", "ì¸µ", "ìª½", "ì „", "í™•ì¸", "ì•„ì´í"
    }
    stop_words.update(manual_stop_words)

    manual_compounds = {
        "íƒ•ë¹„ì‹¤": "íƒ•ë¹„ì‹¤",
        "ì•„ì´íë¹„ì•„": "ì•„ì´íë¹„ì•„",
        "ê³µìœ ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤",
        "ê³µìœ  ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤",
        "ì„ì›ì‹¤": "ì„ì›ì‹¤",
        "ë°©í–¥ì œ": "ë°©í–¥ì œ",
        "ìë™ë¬¸": "ìë™ë¬¸",
        "ë””ìŠ¤íœì„œ": "ë””ìŠ¤íœì„œ",
        "ì‹¤ì™¸ê¸°": "ì‹¤ì™¸ê¸°"
    }
    compound_words.update(manual_compounds)
    return compound_words, stop_words

compound_words, stop_words = apply_manual_dicts(compound_words, stop_words)

# í…ìŠ¤íŠ¸ ë°ì´í„° ì†ŒìŠ¤
text_data = df['keyword_text']

# í‚¤ì›Œë“œ ë¶„ì„ í•¨ìˆ˜
def keyword_analysis(texts, stop_words=None, compound_words=None, use_komoran=True,
                     min_token_len=2, drop_numeric=True):
    """
    texts: list/Series of str (ì˜ˆ: df['keyword_text'])
    stop_words: set(str)
    compound_words: dict(str -> str)  # ì˜ˆ: {"ê³µìœ  ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤", "ì‹¤ì™¸ê¸°": "ì‹¤ì™¸ê¸°"}
    use_komoran: Trueë©´ konlpy Komoran ëª…ì‚¬ ì¶”ì¶œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ í† í¬ë‚˜ì´ì €ë¡œ í´ë°±
    min_token_len: ìµœì†Œ í† í° ê¸¸ì´(ê¸°ë³¸ 2ì)
    drop_numeric: ìˆ«ì ì „ìš© í† í° ì œê±° ì—¬ë¶€
    return: List[str]  # ì „ì²´ ë¬¸ì„œì—ì„œ ìˆ˜ì§‘í•œ í† í° ë¦¬ìŠ¤íŠ¸
    """
    stop_words = stop_words or set()
    compound_words = compound_words or {}

    # 0) Komoran ì‹œë„
    komoran = None
    if use_komoran:
        try:
            from konlpy.tag import Komoran  # í™˜ê²½ì— ë”°ë¼ ë¯¸ì„¤ì¹˜ì¼ ìˆ˜ ìˆìŒ
            komoran = Komoran()
        except Exception:
            komoran = None

    # 1) ë³µí•©ì–´ ì¹˜í™˜ íŒ¨í„´(ê¸¸ì´ ê¸´ key ìš°ì„ )
    comp_items = sorted(compound_words.items(), key=lambda x: len(x[0]), reverse=True)
    comp_patterns = [(re.compile(re.escape(k)), v) for k, v in comp_items]

    # 2) ë³µí•©ì–´ êµ¬ì„±ìš”ì†Œ ë¸”ë™ë¦¬ìŠ¤íŠ¸
    comp_components = {}
    for k, v in compound_words.items():
        parts = [p for p in re.split(r"\s+", k.strip()) if p]
        if parts:
            comp_components.setdefault(v, set()).update(parts)

    # 3) ì •ê·œì‹ í† í¬ë‚˜ì´ì €(í´ë°±)
    token_regex = re.compile(r"[ê°€-í£]+|[A-Za-z]+|\d+")

    def normalize_text(s: str) -> str:
        s = "" if s is None else str(s)
        s = unicodedata.normalize("NFKC", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def apply_compounds(s: str) -> str:
        for pat, rep in comp_patterns:
            s = pat.sub(rep, s)
        return s

    def tokenize(s: str):
        if komoran:
            try:
                return komoran.nouns(s)  # ëª…ì‚¬ë§Œ
            except Exception:
                pass
        return token_regex.findall(s)

    def filter_tokens(tokens, present_compounds):
        out = []
        for t in tokens:
            if not t:
                continue
            if drop_numeric and t.isdigit():
                continue
            if len(t) < min_token_len:
                continue
            if t in stop_words:
                continue
            # ë¬¸ì„œ ë‚´ ë³µí•©ì–´ê°€ ìˆìœ¼ë©´ ê·¸ êµ¬ì„±ìš”ì†Œ ì œê±°
            remove = False
            for comp in present_compounds:
                parts = comp_components.get(comp, set())
                if t in parts:
                    remove = True
                    break
            if not remove:
                out.append(t)
        return out

    all_tokens = []
    for raw in texts:
        s = normalize_text(raw)
        s = apply_compounds(s)
        toks = tokenize(s)
        present_compounds = set(t for t in toks if t in comp_components)
        toks = filter_tokens(toks, present_compounds)
        all_tokens.extend(toks)

    return all_tokens


# í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
keyword_counts = keyword_analysis(
    text_data,
    stop_words=stop_words,
    compound_words=compound_words,
    use_komoran=USE_KOMORAN
)


# ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ì‹œê°í™”
if keyword_counts:
    word_freq = Counter(keyword_counts).most_common(20)
    keywords, counts = zip(*word_freq)

    plt.figure(figsize=(14, 6))
    bars = plt.bar(keywords, counts, color='salmon')
    plt.title('VOC í‚¤ì›Œë“œ ë¹ˆë„ ìƒìœ„ 20')
    plt.xticks(rotation=45)
    plt.tight_layout()

    for bar in bars:
        height = bar.get_height()
        plt.annotate(f'{int(height)}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 5),
                     textcoords='offset points',
                     ha='center', va='bottom')
    plt.show()
else:
    print("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")






from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# í…ìŠ¤íŠ¸ ì†ŒìŠ¤ í™•ì¸(ì• ì…€ì—ì„œ text_dataê°€ ì—†ë‹¤ë©´ df['keyword_text'] ì‚¬ìš©)
text_data = globals().get("text_data", df["keyword_text"])

# 1) í‚¤ì›Œë“œ í† í° ì¶”ì¶œ
tokens = keyword_analysis(
    text_data,
    stop_words=stop_words,
    compound_words=compound_words,
    use_komoran=USE_KOMORAN
)

# 2) ë¹ˆë„ ì‚¬ì „ ìƒì„±(WordCloudëŠ” ë¹ˆë„ ë”•ì…”ë„ˆë¦¬ í•„ìš”)
freq = Counter(tokens)

# í•„ìš”ì‹œ ìƒìœ„ Nê°œë§Œ ì‚¬ìš©
TOP_N = 200
freq = dict(freq.most_common(TOP_N))

if not freq:
    print("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì „ì²˜ë¦¬/ì‚¬ì „ ì ìš©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
else:
    # 3) ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wc = WordCloud(
        font_path="/usr/share/fonts/nanum/NanumGothic-Regular.ttf",  # í•œê¸€ í°íŠ¸
        width=800,
        height=400,
        background_color="white"
    ).generate_from_frequencies(freq)

    # 4) ì‹œê°í™”
    plt.figure(figsize=(14, 6))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title("VOC ì›Œë“œí´ë¼ìš°ë“œ")
    plt.tight_layout()
    plt.show()






import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm

# 1. í°íŠ¸ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_name = fm.FontProperties(fname=font_path).get_name()
plt.rc("font", family=font_name)
sns.set(font=font_name, rc={"axes.unicode_minus": False})

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# Result 1 ì‹œíŠ¸ì˜ ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ë¬¸ìì—´ë¡œ í†µì¼
df = pd.read_excel("nsquare_2508_tagged.xlsx", sheet_name="Result 1", header=0, dtype=str)
df.columns = [str(c).strip() for c in df.columns]

# 2-1. ê²°í•© ìœ í‹¸: "ëŒ€ë¶„ë¥˜-ì¤‘ë¶„ë¥˜" í˜•íƒœ, í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ ê°’, ëª¨ë‘ ì—†ìœ¼ë©´ "ë¯¸ë¶„ë¥˜"
def combine_major_minor(major, minor):
    a = "" if pd.isna(major) else str(major).strip()
    b = "" if pd.isna(minor) else str(minor).strip()
    if a and b:
        return f"{a}-{b}"
    return a or b or "ë¯¸ë¶„ë¥˜"

# 2-2. ì£¼ì œ/ì‘ì—…ìœ í˜• ê²°í•© ì»¬ëŸ¼ ìƒì„±
required_cols = ["ì£¼ì œ ëŒ€ë¶„ë¥˜", "ì£¼ì œ ì¤‘ë¶„ë¥˜", "ì‘ì—…ìœ í˜• ëŒ€ë¶„ë¥˜", "ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

df.loc[:, "ì£¼ì œ"] = [
    combine_major_minor(mj, mn)
    for mj, mn in zip(df["ì£¼ì œ ëŒ€ë¶„ë¥˜"], df["ì£¼ì œ ì¤‘ë¶„ë¥˜"])
]
# df.loc[:, "ì‘ì—…ìœ í˜•"] = [
#     combine_major_minor(mj, mn)
#     for mj, mn in zip(df["ì‘ì—…ìœ í˜• ëŒ€ë¶„ë¥˜"], df["ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"])
# ]

df.loc[:, "ì‘ì—…ìœ í˜•"] = [combine_major_minor(None, mn) for mn in df["ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"]]

# 3. ê·¸ë£¹ë³„ ìƒìœ„ í•­ëª© ì¶”ì¶œ í•¨ìˆ˜ (ê·¸ë£¹ë³„ ì •ë ¬ ê¸°ì¤€ ë³´ì™„)
def get_top_n_by_group(df, group_col, cat_col, n=3):
    counts = (
        df.groupby([group_col, cat_col])
          .size()
          .reset_index(name="count")
    )
    # ê·¸ë£¹ë³„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ê° ê·¸ë£¹ ìƒìœ„ nê°œ
    counts = counts.sort_values([group_col, "count"], ascending=[True, False])
    return counts.groupby(group_col).head(n)

# 4. ë§‰ëŒ€ ì˜†ì— í…ìŠ¤íŠ¸ ë¼ë²¨ í•¨ìˆ˜
def label_bars(ax, labels):
    for bar, label in zip(ax.patches, labels):
        width = bar.get_width()
        if width > 0:
            ax.text(width + 0.5, bar.get_y() + bar.get_height() / 2,
                    label, va='center', fontsize=9, color='black')

# 5. íŒ€ë³„ ì£¼ì œ Top3
if "user_group_name" not in df.columns:
    raise KeyError("í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: user_group_name")

team_topic_top3 = get_top_n_by_group(df, "user_group_name", "ì£¼ì œ", n=3)
team_order = (
    team_topic_top3.groupby("user_group_name")["count"]
    .sum()
    .sort_values(ascending=False)
    .index
    .tolist()
)
topic_order = team_topic_top3["ì£¼ì œ"].value_counts().index.tolist()

# 6. íˆíŠ¸ë§µ: ì£¼ì œ vs ì‘ì—…ìœ í˜•
topic_type_pivot = (
    df.groupby(["ì£¼ì œ", "ì‘ì—…ìœ í˜•"])
      .size()
      .unstack(fill_value=0)
)
plt.figure(figsize=(14, 8))
sns.heatmap(topic_type_pivot, annot=True, fmt=".0f", cmap="Blues")
plt.title("ì£¼ì œë³„ ì‘ì—…ìœ í˜• ë¶„í¬")
plt.yticks(rotation=0)
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# 7. íŒ€ë³„ ì£¼ì œ-ì‘ì—…ìœ í˜• íˆíŠ¸ë§µ
teams = df["user_group_name"].dropna().unique()
for team in teams:
    df_team = df[df["user_group_name"] == team]
    if df_team.empty:
        continue
    pivot = (
        df_team.groupby(["ì£¼ì œ", "ì‘ì—…ìœ í˜•"])
               .size()
               .unstack(fill_value=0)
    )
    if pivot.to_numpy().sum() == 0:
        continue

    plt.figure(figsize=(14, 6))
    sns.heatmap(pivot, annot=True, fmt=".0f", cmap="Oranges")
    plt.title(f"[{team}] ì£¼ì œë³„ ì‘ì—…ìœ í˜• ë¶„í¬")
    plt.yticks(rotation=0)
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()



import os
import nbformat
from bs4 import BeautifulSoup
from nbconvert import HTMLExporter

# === ì‚¬ìš©ì ì„¤ì • ===
notebook_input = "./keyword_analysis_nsquare.ipynb"
html_output_path = "./keyword_analysis_nsquare_2508.html"

# === 1ë‹¨ê³„: nbformat + HTMLExporterë¡œ HTML ë³€í™˜ (ì½”ë“œ ì…ë ¥ ì œê±°)
with open(notebook_input, encoding="utf-8") as f:
    notebook_node = nbformat.read(f, as_version=4)

html_exporter = HTMLExporter()
html_exporter.exclude_input = True  # ì½”ë“œ ì…ë ¥ ìˆ¨ê¹€
body, _ = html_exporter.from_notebook_node(notebook_node)

# === 2ë‹¨ê³„: BeautifulSoup ë¡œë“œ
soup = BeautifulSoup(body, "html.parser")

# === 3ë‹¨ê³„: 'Output hidden' í…ìŠ¤íŠ¸ ì œê±°
for output_hidden in soup.find_all(string=lambda text: isinstance(text, str) and "Output hidden;" in text):
    output_hidden.extract()

# === 4ë‹¨ê³„: ëª©ì°¨(TOC) ìƒì„±
toc_container = soup.new_tag("div", id="toc")
toc_title_tag = soup.new_tag("strong")
toc_title_tag.string = "ğŸ“– ëª©ì°¨"
toc_container.append(toc_title_tag)

toc_list = soup.new_tag("ul")
toc_container.append(toc_list)

header_tags = soup.find_all(["h1", "h2", "h3"])
current_h1 = None
current_h2 = None

for idx, header in enumerate(header_tags):
    if not header.has_attr("id"):
        header['id'] = f"toc_{idx}"

    link = soup.new_tag("a", href=f"#{header['id']}")
    link.string = header.get_text()
    list_item = soup.new_tag("li")
    list_item.append(link)

    if header.name == "h1":
        toc_list.append(list_item)
        current_h1 = list_item
        current_h2 = None
    elif header.name == "h2":
        if current_h1 is None:
            toc_list.append(list_item)
        else:
            if not current_h1.find('ul'):
                current_h1.append(soup.new_tag("ul"))
            current_h1.find('ul').append(list_item)
        current_h2 = list_item
    elif header.name == "h3":
        if current_h2 is not None:
            if not current_h2.find('ul'):
                current_h2.append(soup.new_tag("ul"))
            current_h2.find('ul').append(list_item)
        elif current_h1 is not None:
            if not current_h1.find('ul'):
                current_h1.append(soup.new_tag("ul"))
            current_h1.find('ul').append(list_item)
        else:
            toc_list.append(list_item)

# === 5ë‹¨ê³„: TOC ìŠ¤íƒ€ì¼ ì‚½ì…
style_tag = soup.new_tag("style")
style_tag.string = """
#toc {
    position: fixed;
    top: 20px;
    right: 20px;
    width: 250px;
    background: #f9f9f9;
    border: 1px solid #ddd;
    padding: 10px;
    max-height: 90vh;
    overflow-y: auto;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    z-index: 1000;
    font-family: sans-serif;
    font-size: 14px;
}
#toc ul {
    list-style: none;
    padding-left: 0;
}
#toc li {
    margin: 5px 0;
}
#toc li ul {
    padding-left: 15px;
}
#toc li ul li ul {
    padding-left: 15px;
}
#toc a {
    text-decoration: none;
    color: #333;
}
#toc a:hover {
    text-decoration: underline;
}
"""
soup.head.append(style_tag)

if soup.body:
    soup.body.insert(0, toc_container)

# === 6ë‹¨ê³„: ìµœì¢… HTML ì €ì¥
with open(html_output_path, "w", encoding="utf-8") as f:
    f.write(str(soup))




import seaborn as sns
sns.color_palette()



