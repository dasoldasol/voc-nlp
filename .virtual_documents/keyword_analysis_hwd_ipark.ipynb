


import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import matplotlib as mpl

# ì„¤ì¹˜ëœ ë‚˜ëˆ”ê³ ë”• ê²½ë¡œ í™•ì¸ í›„ ë“±ë¡
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
fm.fontManager.addfont(font_path)

# í°íŠ¸ ì´ë¦„ í™•ì¸
nanum_font = fm.FontProperties(fname=font_path).get_name()
# print(f"í°íŠ¸ ì´ë¦„: {nanum_font}")  # 'NanumGothic' ë‚˜ì™€ì•¼ í•¨

# matplotlibì— ì ìš©
plt.rc('font', family=nanum_font)
mpl.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¹¨ì§ ë°©ì§€


from IPython.core.display import HTML

# ë…¸íŠ¸ë¶ CSS ìµœì í™”
HTML("""
<style>
img {
    max-width: 100%;
    height: auto;
}
</style>
""")


print("""
í˜„ì¥ëª… : H ì˜¤í”¼ìŠ¤ ì—…ë¬´ë™
í˜„ì¥ì†Œì¥ : ê¹€** 
ë³´ê³ ëŒ€ìƒì›” : 2025.08
""")





import pandas as pd 
import psycopg2

DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'

conn = psycopg2.connect(
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        host=DB_HOST
    )

cursor = conn.cursor()

query = """
SELECT
    v.*,
    bf.name AS building_floor_name,
    v.building_floor_zone_id,
    z.name AS zone_name,
    r.*,
    bug.name AS user_group_name,
    a.name AS reply_writer
FROM voc v
         LEFT JOIN building_floor bf ON v.building_floor_id = bf.id
         LEFT JOIN building_floor_zone z ON v.building_floor_zone_id = z.id
         LEFT JOIN (
    SELECT DISTINCT ON (voc_id) *
    FROM voc_reply
    ORDER BY voc_id, reply_write_date DESC  -- ìµœì‹  ì‘ë‹µ í•˜ë‚˜ë§Œ
) r ON v.id = r.voc_id
         LEFT JOIN account_group ag ON r.reply_writer_id = ag.account_id
         LEFT JOIN building_user_group bug ON ag.group_id = bug.id
         LEFT JOIN account a ON r.reply_writer_id = a.id
WHERE v.building_id = 95 and title not like '%í…ŒìŠ¤íŠ¸%'
and voc_date > '2025-08-01' and voc_date < '2025-09-01'
""" 
cursor.execute(query)
rows = cursor.fetchall()
colnames = [desc[0] for desc in cursor.description]

df = pd.DataFrame(rows, columns=colnames, dtype=str)

cursor.close()
conn.close()


from datetime import datetime


#  ë¬¸ìí˜• ì»¬ëŸ¼
text_columns = ['voc_user_name', 'voc_user_phone', 'request_contents', 'state', 'title', 
                'building_floor_name', 'zone_name', 'reply', 'user_group_name', 'reply_writer']
for col in text_columns:
    df[col] = df[col].astype(str)

#  ë²”ì£¼í˜• ì»¬ëŸ¼
category_columns = ['user_group_name', 'building_floor_name', 'zone_name', 'user_group_name']
for col in category_columns:
    df[col] = df[col].astype('category')

#  ë‚ ì§œ ë³€í™˜
date_columns = [col for col in df.columns if 'date' in col.lower()]
df[date_columns] = df[date_columns].apply(pd.to_datetime)



# ì „ì²´ í–‰ ìˆ˜ì™€ ì»¬ëŸ¼ ìˆ˜ ì¶œë ¥
# df = pd.read_excel("VOC_á„‡á…®á†«á„‰á…¥á†¨_á„‹á…­á„á…¥á†¼á„€á…¥á†«á„‹á…´á„á…©á†¼á„’á…¡á†¸_á„€á…§á†¯á„€á…ª.xlsx")
print(f"ì „ì²´ ë°ì´í„° ìˆ˜: {df.shape[0]}")

# 'ì ‘ìˆ˜ì‹œê°„' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)
df['write_date'] = pd.to_datetime(df['write_date'])

# ì›ë˜ min, max
min_time = df['write_date'].min()
max_time = df['write_date'].max()

print(f"\n[ì›ì‹œë°ì´í„° ê¸°ì¤€]")
print(f"ìµœì†Œ ì ‘ìˆ˜ì‹œê°„: {min_time}")
print(f"ìµœëŒ€ ì ‘ìˆ˜ì‹œê°„: {max_time}")

# í˜„ì¬ ì‹œê° ê¸°ì¤€ í•„í„°ë§
# now = pd.Timestamp.now()
# df_past = df[df['write_date'] <= now]

# print(f"í‘œì¤€í™” ë°ì´í„° ìˆ˜: {df_past.shape[0]}")

# # ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ë§Œ ì¶œë ¥
# if not df_past.empty:
#     min_past = df_past['write_date'].min()
#     max_past = df_past['write_date'].max()

#     print(f"\n[í‘œì¤€í™”ë°ì´í„° ê¸°ì¤€]")
#     print(f"ìµœì†Œ ì ‘ìˆ˜ì‹œê°„: {min_past}")
#     print(f"ìµœëŒ€ ì ‘ìˆ˜ì‹œê°„: {max_past}")
# else:
#     print("\ní˜„ì¬ë³´ë‹¤ ê³¼ê±°ì— í•´ë‹¹í•˜ëŠ” ì ‘ìˆ˜ì‹œê°„ì´ ì—†ìŠµë‹ˆë‹¤.")








from datetime import datetime
import pandas as pd

# í˜„ì¬ ì‹œê°„
now = datetime.now()

# ë‚ ì§œ ì»¬ëŸ¼ ë²”ìœ„ ê³„ì‚°
date_range = {}
for col in ['write_date', 'reply_write_date']:
    valid_dates = df[col].dropna()
    # valid_dates = valid_dates[valid_dates <= now]
    if not valid_dates.empty:
        date_range[col] = (valid_dates.min(), valid_dates.max())
    else:
        date_range[col] = (None, None)

# DataFrame ìƒì„± í›„ ì»¬ëŸ¼ëª… ë³€ê²½
date_range_df = pd.DataFrame(date_range, index=['ìµœì´ˆì¼ì‹œ', 'ìµœì¢…ì¼ì‹œ'])
date_range_df = date_range_df.rename(columns={
    'write_date': 'ë°œìƒì¼ì‹œ',
    'reply_write_date': 'ì²˜ë¦¬ì¼ì‹œ'
})

date_range_df





### ì›”ë³„ VOC ë°œìƒ ê±´ìˆ˜ 
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np

# # write_dateê°€ datetimeí˜•ì´ ì•„ë‹ˆë¼ë©´ ë³€í™˜
# if not np.issubdtype(df['write_date'].dtype, np.datetime64):
#     df.loc[:, 'write_date'] = pd.to_datetime(df['write_date'], errors='coerce')

# # 1. date_range ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ "ì›”" ë²”ìœ„ ìƒì„±
# start_date = date_range['write_date'][0].normalize()
# end_date   = date_range['write_date'][1].normalize()

# # ì›” ì‹œì‘ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
# start_month = start_date.to_period('M')
# end_month   = end_date.to_period('M')

# # ì „ì²´ ì›” PeriodIndex ìƒì„±
# all_months = pd.period_range(start=start_month, end=end_month, freq='M')

# # 2. ì›”ë³„ VOC ë°œìƒ ê±´ìˆ˜ ê³„ì‚° (Period[M] ë‹¨ìœ„ë¡œ ì„¸ê¸°)
# monthly_counts = df['write_date'].dt.to_period('M').value_counts().sort_index()

# # 3. ë¹ ì§„ ì›” ë³´ì¶© (ë¹ˆ ì›”ì€ 0ìœ¼ë¡œ)
# monthly_counts = monthly_counts.reindex(all_months, fill_value=0)

# # 4. xì¶•ìš© ë¼ë²¨(YYYY-MM)
# x_labels = monthly_counts.index.astype(str)

# # 5. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
# plt.figure(figsize=(14, 6))
# bars = plt.bar(x_labels, monthly_counts.values)

# # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ (ì„ íƒ)
# for bar in bars:
#     height = bar.get_height()
#     if height > 0:
#         plt.text(
#             bar.get_x() + bar.get_width() / 2,
#             height,
#             f'{int(height)}',
#             ha='center',
#             va='bottom'
#         )

# plt.title('ì›”ë³„ VOC ë°œìƒ ê±´ìˆ˜')
# plt.xlabel('ì›”')
# plt.ylabel('ê±´ìˆ˜')
# plt.xticks(rotation=45)
# plt.tight_layout()
# plt.show()



import matplotlib.pyplot as plt
import pandas as pd

# 1. date_range ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì¼ ë²”ìœ„ ìƒì„±
start_date = date_range['write_date'][0].normalize()
end_date = date_range['write_date'][1].normalize()

all_days = pd.date_range(start=start_date, end=end_date, freq='D')

# 2. ì¼ë³„ VOC ë°œìƒ ê±´ìˆ˜ ê³„ì‚°
daily_counts = df['write_date'].dt.normalize().value_counts().sort_index()

# 3. ë¹ ì§„ ë‚ ì§œ ë³´ì¶© (ë¹ˆ ë‚ ì§œëŠ” 0ìœ¼ë¡œ)
daily_counts = daily_counts.reindex(all_days, fill_value=0)

# 4. ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ xì¶•ì— ì‚¬ìš©
daily_counts.index = daily_counts.index.strftime('%Y-%m-%d')

# 5. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
bars = plt.bar(daily_counts.index, daily_counts.values)

# ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ (ì„ íƒ)
for bar in bars:
    height = bar.get_height()
    if height > 0:
        plt.text(
            bar.get_x() + bar.get_width() / 2,
            height,
            f'{int(height)}',
            ha='center',
            va='bottom'
        )

plt.title('ì¼ë³„ VOC ë°œìƒ ê±´ìˆ˜')
plt.xlabel('ë‚ ì§œ')
plt.ylabel('ê±´ìˆ˜')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()





import matplotlib.pyplot as plt
import pandas as pd
import re

# ì›ë³¸ ì „ì²´ ì¸µ ë¦¬ìŠ¤íŠ¸
all_floors = [
    '2F', '6F', '8F', '7F', '9F', '3F',
    'B1F', 'B2F', '4F', 'B3F', 'B4F', 'B6F', 'B5F',
    '5F', '1F'
]

# 1. ì¸µ ì •ë ¬ í•¨ìˆ˜ ì •ì˜
def floor_sort_key(floor):
    # ì§€í•˜ì¸µ: Bìˆ«ì â†’ ìŒìˆ˜ë¡œ ë³€í™˜í•´ì„œ ê°€ì¥ ì•ì— ì˜¤ê²Œ
    if floor.startswith('B'):
        num = int(re.findall(r'\d+', floor)[0])
        return -num
    # ì˜¥íƒ‘ì¸µ: PHìˆ«ì â†’ ë§¤ìš° í° ê°’ìœ¼ë¡œ í•´ì„œ ë§¨ ë’¤ì— ì˜¤ê²Œ
    elif floor.startswith('PH'):
        num = int(re.findall(r'\d+', floor)[0])
        return 1000 + num
    # ì¼ë°˜ì¸µ: ìˆ«ìì¸µ â†’ ê·¸ëŒ€ë¡œ ìˆ«ì
    else:
        num = int(re.findall(r'\d+', floor)[0])
        return num

# 2. ì •ë ¬ ì ìš©
sorted_floors = sorted(all_floors, key=floor_sort_key)

# 3. ë°œìƒìœ„ì¹˜ë³„ VOC ê±´ìˆ˜ ì§‘ê³„
location_counts = df['building_floor_name'].value_counts()

# 4. ê±´ìˆ˜ê°€ 1 ì´ìƒì¸ í•­ëª©ë§Œ í•„í„°ë§
location_counts = location_counts[location_counts > 1]

# 5. ì „ì²´ ì¸µ ê¸°ì¤€ìœ¼ë¡œ ëˆ„ë½ ì¸µ 0ìœ¼ë¡œ ì±„ì›€
location_counts_all = pd.Series(index=sorted_floors, dtype=int)
location_counts_all = location_counts_all.fillna(0)
location_counts_all.update(location_counts)

# 6. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 8))
bars = plt.barh(location_counts_all.index.astype(str), location_counts_all.values, color='skyblue')

# 7. ë¼ë²¨ ë¶™ì´ê¸°
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2,
             f'{int(width)}',
             ha='left', va='center')

# 8. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •
plt.title('ì¸µë³„ VOC ë°œìƒ ê±´ìˆ˜')
plt.xlabel('ê±´ìˆ˜')
plt.ylabel('ë°œìƒìœ„ì¹˜')
plt.tight_layout()
plt.show()





# 1. reply_write_dateê°€ ìˆê³ , user_group_nameì´ "None"ì´ ì•„ë‹Œ ê²½ìš° í•„í„°ë§ + ë³µì‚¬ë³¸ ìƒì„±
filtered_df = df[
    (df['reply_write_date'].notnull()) &
    (df['reply_writer'].astype(str).str.strip().str.lower() != 'none')
].copy()  # â† ì´ê±¸ ì¶”ê°€í•˜ë©´ ê²½ê³  ì‚¬ë¼ì§

import re

# 2-1. ì´ë¦„ ìµëª… ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def anonymize_name(name: str) -> str:
    # í•œê¸€ 3ê¸€ìì¸ ê²½ìš°ë§Œ ê°€ìš´ë° ê¸€ìë¥¼ *ë¡œ ì¹˜í™˜
    if re.fullmatch(r'[ê°€-í£]{3}', name):
        return name[0] + '*' + name[2]
    return name

# 2-2. reply_writerì— ì ìš©
filtered_df['reply_writer'] = filtered_df['reply_writer'].astype(str).map(anonymize_name)

# 3. íŒ€ë³„ ê±´ìˆ˜ ì§‘ê³„
top20_final = filtered_df['reply_writer'].value_counts()

# 4. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
ax = top20_final.plot(kind='bar')
plt.title("ì¸ì›ë³„ VOC ì²˜ë¦¬ ê±´ìˆ˜ (ë‹µë³€ ì‘ì„± ê±´ìˆ˜)")
plt.xlabel("ë‹´ë‹¹ì")
plt.ylabel("ê±´ìˆ˜")
plt.xticks(rotation=45)
plt.tight_layout()

# 5. ë§‰ëŒ€ ìœ„ ìˆ«ì í‘œì‹œ
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{int(height)}',
                xy=(p.get_x() + p.get_width() / 2, height),
                xytext=(0, 5),
                textcoords='offset points',
                ha='center', va='bottom')

plt.show()





import matplotlib.pyplot as plt
import pandas as pd

# 1. ìš”ì¼ ì»¬ëŸ¼ ì¶”ê°€ (0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼)
df['ìš”ì¼'] = df['write_date'].dt.dayofweek  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼

# 2. ìš”ì¼ ì´ë¦„ ë§¤í•‘
weekday_map = {0: 'ì›”ìš”ì¼', 1: 'í™”ìš”ì¼', 2: 'ìˆ˜ìš”ì¼', 3: 'ëª©ìš”ì¼', 4: 'ê¸ˆìš”ì¼', 5: 'í† ìš”ì¼', 6: 'ì¼ìš”ì¼'}
df['ìš”ì¼'] = df['ìš”ì¼'].map(weekday_map)

# 3. ìš”ì¼ ìˆœì„œ ì •ì˜
weekday_order = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']

# 4. groupby + size + reindex
weekday_counts = df.groupby('ìš”ì¼').size().reindex(weekday_order, fill_value=0)

# âœ… ì¸ë±ìŠ¤ë¥¼ CategoricalIndex ë¡œ ê°•ì œ
weekday_counts.index = pd.CategoricalIndex(weekday_counts.index, categories=weekday_order, ordered=True)

# 5. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
bars = plt.barh(weekday_counts.index.astype(str), weekday_counts.values, color='skyblue')

# 6. ë§‰ëŒ€ ìœ„ì— ê±´ìˆ˜ ë¼ë²¨ ë¶™ì´ê¸°
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height() / 2,
             f'{int(width)}',
             ha='left', va='center')

# âœ… yì¶• ë’¤ì§‘ì–´ì„œ ì›” â†’ ì¼ ìˆœì„œë¡œ!
plt.gca().invert_yaxis()

# 7. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •
plt.title('ìš”ì¼ë³„ VOC ë°œìƒ ê±´ìˆ˜')
plt.xlabel('ê±´ìˆ˜')
plt.ylabel('ìš”ì¼')
plt.tight_layout()
plt.show()






# ## ì›”ë³„ ì²˜ë¦¬ ì†Œìš” ì‹œê°„ 
# import matplotlib.pyplot as plt
# import pandas as pd
# import numpy as np

# # ë‚ ì§œ ì»¬ëŸ¼ íŒŒì•… ë° ì•ˆì „ ë³€í™˜
# date_columns = [col for col in df.columns if 'date' in col.lower()]
# for col in date_columns:
#     df[col] = pd.to_datetime(df[col], errors='coerce')

# # 1) ì²˜ë¦¬ ì†Œìš” ì‹œê°„ ì»¬ëŸ¼ ìƒì„±
# df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'] = df['reply_write_date'] - df['write_date']

# # 2) ì›” í‚¤ ìƒì„± (write_date ê¸°ì¤€)
# df['ì ‘ìˆ˜ì›”'] = df['write_date'].dt.to_period('M')

# # 3) ì›”ë³„ í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„ ê³„ì‚°
# monthly_processing_time = df.groupby('ì ‘ìˆ˜ì›”')['ì²˜ë¦¬ì†Œìš”ì‹œê°„'].mean()

# # 4) ì „ì²´ ì›” ë²”ìœ„ ìƒì„± ë° ëˆ„ë½ ì›” 0ìœ¼ë¡œ ë³´ì¶©
# start_month = df['write_date'].min().to_period('M')
# end_month   = df['write_date'].max().to_period('M')
# all_months = pd.period_range(start=start_month, end=end_month, freq='M')

# monthly_processing_time = (
#     monthly_processing_time
#     .reindex(all_months, fill_value=pd.Timedelta(0))
#     .fillna(pd.Timedelta(0))  # í•´ë‹¹ ì›”ì— ê°’ì´ ì „ë¶€ NaTì˜€ë˜ ê²½ìš° ëŒ€ë¹„
# )

# # 5) ì¸ë±ìŠ¤ ë¼ë²¨(YYYY-MM)ë¡œ ë³€í™˜
# x_labels = monthly_processing_time.index.astype(str)

# # 6) timedelta â†’ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜
# processing_time_minutes = monthly_processing_time.dt.total_seconds() / 60

# # 7) ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
# plt.figure(figsize=(14, 6))
# plt.plot(x_labels, processing_time_minutes.values, marker='o', linestyle='-')

# avg_minutes = processing_time_minutes.mean()
# plt.axhline(avg_minutes, linestyle='--', label=f'í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {avg_minutes:.1f}ë¶„')

# plt.title('ì›”ë³„ í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„')
# plt.xlabel('ì›”')
# plt.ylabel('í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„ (ë¶„)')
# plt.xticks(rotation=45)
# plt.legend()
# plt.tight_layout()
# plt.show()



import matplotlib.pyplot as plt
import pandas as pd

# ë‚ ì§œ ì»¬ëŸ¼ íŒŒì•…
date_columns = [col for col in df.columns if 'date' in col.lower()]

# ì•ˆì „í•˜ê²Œ ë‚ ì§œí˜•ìœ¼ë¡œ ë³€í™˜
for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors='coerce')

# 1. ì²˜ë¦¬ ì†Œìš” ì‹œê°„ ì»¬ëŸ¼ ìƒì„±
df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'] = df['reply_write_date'] - df['write_date']

# 2. ì¼ì ë‹¨ìœ„ë¡œ ë³€í™˜
df['ì ‘ìˆ˜ì¼'] = df['write_date'].dt.date

# 3. ì¼ë³„ í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„ ê³„ì‚°
daily_processing_time = df.groupby('ì ‘ìˆ˜ì¼')['ì²˜ë¦¬ì†Œìš”ì‹œê°„'].mean().sort_index()

# 4. ì „ì²´ ì¼ì ë²”ìœ„ ìƒì„± (ì—¬ê¸° ìˆ˜ì •)
start_date = df['write_date'].min().date()
end_date = df['write_date'].max().date()
all_dates = pd.date_range(start=start_date, end=end_date, freq='D')

# 5. ë¹ ì§„ ì¼ìëŠ” 0ìœ¼ë¡œ ì±„ìš°ê¸°
daily_processing_time = daily_processing_time.reindex(all_dates.date, fill_value=pd.Timedelta(0))


# 6. ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ xì¶•ìœ¼ë¡œ ì‚¬ìš©
daily_processing_time.index = daily_processing_time.index.astype(str)

# 7. timedelta â†’ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜
processing_time_minutes = daily_processing_time.apply(lambda x: x.total_seconds() / 60)

# 8. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(14, 6))
plt.plot(processing_time_minutes.index, processing_time_minutes.values, marker='o', linestyle='-', color='skyblue')
average_processing_time = processing_time_minutes.mean()
plt.axhline(average_processing_time, color='red', linestyle='--', label=f'í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„: {average_processing_time:.1f}ë¶„')

# 9. ê° ì ì— ë¼ë²¨ (ì„ íƒì : ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ ê°€ëŠ¥)
# for x, y in zip(processing_time_minutes.index, processing_time_minutes.values):
#     if y > 0:
#         plt.text(x, y, f'{y:.1f}ë¶„', ha='center', va='bottom', fontsize=8)

# 10. ê·¸ë˜í”„ ì„¸ë¶€ ì„¤ì •
plt.title('ì¼ë³„ í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„')
plt.xlabel('ì¼ì')
plt.ylabel('í‰ê·  ì²˜ë¦¬ ì†Œìš” ì‹œê°„ (ë¶„)')
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()
plt.show()


import re, html
from datetime import timedelta

# HTML ì œê±° ì „ì²˜ë¦¬ í•¨ìˆ˜
def strip_html(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = str(text)
    s = html.unescape(s)
    # <script>/<style> ë¸”ë¡ ì œê±°
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)
    # ë‚˜ë¨¸ì§€ íƒœê·¸ ì œê±°
    s = re.sub(r"(?s)<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# timedelta â†’ "xì¼ yì‹œê°„ zë¶„" í˜•ì‹ (0ì¼ì€ ìƒëµ)
def format_timedelta(td: timedelta):
    if pd.isnull(td):
        return None
    days = td.days
    hours, remainder = divmod(td.seconds, 3600)
    minutes, _ = divmod(remainder, 60)

    parts = []
    if days > 0:
        parts.append(f"{days}ì¼")
    if hours > 0:
        parts.append(f"{hours}ì‹œê°„")
    if minutes > 0:
        parts.append(f"{minutes}ë¶„")

    return " ".join(parts) if parts else "0ë¶„"

# ì²˜ë¦¬ì†Œìš”ì‹œê°„ ê¸°ì¤€ ìƒìœ„ 5ê±´ ì¶”ì¶œ
top5_cases = df.sort_values(by='ì²˜ë¦¬ì†Œìš”ì‹œê°„', ascending=False).head(5)

# ì›í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ í›„ ë³´ê¸° ì¢‹ì€ í•œê¸€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
top5_df = top5_cases[['write_date', 'title', 'request_contents', 'reply', 'reply_write_date', 'ì²˜ë¦¬ì†Œìš”ì‹œê°„']].rename(
    columns={
        'write_date': 'ì ‘ìˆ˜ì¼',
        'title': 'VOCì œëª©',
        'request_contents': 'VOCë‚´ìš©',
        'reply': 'ë‹µë³€',
        'reply_write_date': 'ë‹µë³€ì™„ë£Œì‹œê°„',
        'ì²˜ë¦¬ì†Œìš”ì‹œê°„': 'ì²˜ë¦¬ì†Œìš”ì‹œê°„'
    }
)

# í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì „ì²˜ë¦¬ (HTML/ê³µë°± ì œê±°)
for col in ['VOCì œëª©', 'VOCë‚´ìš©', 'ë‹µë³€']:
    top5_df[col] = top5_df[col].apply(strip_html)

# ì²˜ë¦¬ì†Œìš”ì‹œê°„ í¬ë§· ì ìš©
top5_df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'] = top5_df['ì²˜ë¦¬ì†Œìš”ì‹œê°„'].apply(format_timedelta)

# ì ‘ìˆ˜ì¼/ë‹µë³€ì™„ë£Œì‹œê°„ì„ ë¶„ ë‹¨ìœ„ê¹Œì§€ë§Œ í‘œì‹œ
top5_df['ì ‘ìˆ˜ì¼'] = pd.to_datetime(top5_df['ì ‘ìˆ˜ì¼'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M')
top5_df['ë‹µë³€ì™„ë£Œì‹œê°„'] = pd.to_datetime(top5_df['ë‹µë³€ì™„ë£Œì‹œê°„'], errors='coerce').dt.strftime('%Y-%m-%d %H:%M')

print("ì²˜ë¦¬ì†Œìš”ì‹œê°„ TOP 5")
display(top5_df)






## Load Data & Preprocessing 

import pandas as pd
import re
import html

# 0) ì—‘ì…€ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ: ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©
xlsx_path = "Result_1_processed_2.xlsx"
df = pd.read_excel(
    xlsx_path,
    sheet_name="Result 1",
    header=0,          # ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ
    dtype=str          # í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìœ„í•´ ë¬¸ìì—´ë¡œ í†µì¼
)
df.columns = [str(c).strip() for c in df.columns]  # ì»¬ëŸ¼ëª… ê³µë°± ì •ë¦¬

# í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
required = ["request_contents", "title", "reply"]
missing = [c for c in required if c not in df.columns]
if missing:
    raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}. í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")

# 1.2) HTML ì œê±° í•¨ìˆ˜
def strip_html(text: str) -> str:
    if text is None or pd.isna(text):
        return ""
    s = str(text)
    s = html.unescape(s)  # ì—”í‹°í‹° ë””ì½”ë”©
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)  # script/style ì œê±°
    s = re.sub(r"(?s)<[^>]+>", " ", s)  # íƒœê·¸ ì œê±°
    s = html.unescape(s)  # ì—”í‹°í‹° í•œ ë²ˆ ë” ë””ì½”ë”©
    s = re.sub(r"\s+", " ", s).strip()  # ê³µë°± ì •ê·œí™”
    return s

# 1.1) request_contents + title + reply ëª¨ë‘ í•©ì¹œ ì»¬ëŸ¼ ìƒì„±
df.loc[:, "keyword_text"] = (
    df["request_contents"].map(strip_html).fillna("") + " " +
    df["title"].map(strip_html).fillna("") + " " +
    df["reply"].map(strip_html).fillna("")
).str.strip()

# í™•ì¸ìš©(ì„ íƒ)
# print(df[["keyword_text"]].head())




## ë‹¨ì–´ ë¹ˆë„ë¶„ì„ (ì¼ë‹¨ ì œì™¸)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import re
import unicodedata
import html
from collections import Counter

# 0) ì„ íƒ: Komoran ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ìë™ íŒë³„
try:
    import konlpy  # ë‹¨ìˆœ ì¡´ì¬ í™•ì¸
    USE_KOMORAN = True
except Exception:
    USE_KOMORAN = False

# 1. í°íŠ¸ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_name = fm.FontProperties(fname=font_path).get_name()
plt.rc("font", family=font_name)
sns.set(font=font_name, rc={"axes.unicode_minus": False})

# 2) df/keyword_text ì¤€ë¹„ ì—¬ë¶€ í™•ì¸(ì´ì „ì— ë§Œë“¤ì–´ì§„ dfê°€ ì—†ë‹¤ë©´ ë¡œë“œ ë° í´ë°± ìƒì„±)
if "df" not in globals() or "keyword_text" not in getattr(globals().get("df", pd.DataFrame()), "columns", []):
    # ì—‘ì…€ì—ì„œ Result 1 ì‹œíŠ¸ ë¡œë“œ (ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©)
    df = pd.read_excel("Result_1_processed_2.xlsx", sheet_name="Result 1", header=0, dtype=str)
    df.columns = [str(c).strip() for c in df.columns]

        # voc_dateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
    df['voc_date'] = pd.to_datetime(df['voc_date'], errors='coerce')
    
    # 2025ë…„ 8ì›” ë°ì´í„°ë§Œ ì¶”ì¶œ
    df = df[
        (df['voc_date'].dt.year == 2025) & (df['voc_date'].dt.month == 8)
    ]

    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
    for col in ["request_contents", "title", "reply"]:
        if col not in df.columns:
            raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")

    # HTML ì œê±° + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì •ì œ í´ë°±(ì´ë¯¸ keyword_textê°€ ìˆë‹¤ë©´ ì´ ë‹¨ê³„ëŠ” ìƒëµë  ì˜ˆì •)
    def strip_html(text: str) -> str:
        if text is None or pd.isna(text):
            return ""
        s = str(text)
        s = html.unescape(s)
        s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)  # script/style ì‚­ì œ
        s = re.sub(r"(?s)<[^>]+>", " ", s)                       # ëª¨ë“  íƒœê·¸ ì‚­ì œ
        s = html.unescape(s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    ALLOWED_PATTERN = re.compile(r"[^ê°€-í£ã„±-ã…ã…-ã…£A-Za-z0-9\s\.\,\!\?\;\:\'\"\(\)\[\]\{\}\-\_\/\&\+\%\#\@]")
    def whitelist_text(text: str) -> str:
        if text is None or pd.isna(text):
            return ""
        s = strip_html(text)
        s = unicodedata.normalize("NFKC", s)
        s = re.sub(r"[\u200B-\u200D\uFE0E\uFE0F]", "", s)  # ì œë¡œí­/Variation Selector ì œê±°
        s = ALLOWED_PATTERN.sub(" ", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    # í´ë°±: request_contents + title + reply í•©ì³ keyword_text ìƒì„±
    df.loc[:, "keyword_text"] = (
        df["request_contents"].map(whitelist_text).fillna("") + " " +
        df["title"].map(whitelist_text).fillna("") + " " +
        df["reply"].map(whitelist_text).fillna("")
    ).str.strip()


# === DBì—ì„œ ë³µí•©ì–´/ë¶ˆìš©ì–´ ë¶ˆëŸ¬ì˜¤ê¸° ===

DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'


def load_text_dict_from_db():
    import psycopg2
    try:
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()

        # compound_words: key, value ëª¨ë‘ strip
        cursor.execute("""
            SELECT word_key, word_value 
            FROM stat_text_dict 
            WHERE type = 'compound' AND is_active = TRUE
        """)
        compound_result = cursor.fetchall()
        compound_words = {
            row[0].strip(): row[1].strip()
            for row in compound_result if row[0] and row[1]
        }

        # stop_words
        cursor.execute("""
            SELECT word_key 
            FROM stat_text_dict 
            WHERE type = 'stop' AND is_active = TRUE
        """)
        stop_result = cursor.fetchall()
        stop_words = {row[0].strip() for row in stop_result if row[0]}

        cursor.close()
        connection.close()
        return compound_words, stop_words

    except Exception as e:
        # ì—°ê²° ì‹¤íŒ¨ ì‹œ ë¹ˆ ì‚¬ì „/ì…‹ìœ¼ë¡œ ì§„í–‰
        return {}, set()

compound_words, stop_words = load_text_dict_from_db()

# ìˆ˜ë™ ë¶ˆìš©ì–´ ë° ë³µí•©ì–´ ì‚¬ì „ ì ìš© í•¨ìˆ˜
def apply_manual_dicts(compound_words, stop_words):
    manual_stop_words = {
        "í›„", "ë‹¬ë¼", "ì—†ë‹¤ë‡¨", "ì²«", "ë¬¼ì´", "ê²¨", "ì²˜ë¦¬", "ì¡°ì¹˜", "ë™ì•ˆ", "ìœ„í•´",
        "ë°”ë¡œ", "ë‚´", "ì‹œ", "ì", "ì", "í¸", "í˜¸", "ì‹¤", "ìˆ˜", "ì¸µ", "ìª½", "ì „", "í™•ì¸"
    }
    stop_words.update(manual_stop_words)

    manual_compounds = {
        "íƒ•ë¹„ì‹¤": "íƒ•ë¹„ì‹¤",
        "ì•„ì´íë¹„ì•„": "ì•„ì´íë¹„ì•„",
        "ê³µìœ ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤",
        "ê³µìœ  ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤",
        "ì„ì›ì‹¤": "ì„ì›ì‹¤",
        "ë°©í–¥ì œ": "ë°©í–¥ì œ",
        "ìë™ë¬¸": "ìë™ë¬¸",
        "ë””ìŠ¤íœì„œ": "ë””ìŠ¤íœì„œ",
        "ì‹¤ì™¸ê¸°": "ì‹¤ì™¸ê¸°"
    }
    compound_words.update(manual_compounds)
    return compound_words, stop_words

compound_words, stop_words = apply_manual_dicts(compound_words, stop_words)

# í…ìŠ¤íŠ¸ ë°ì´í„° ì†ŒìŠ¤
text_data = df['keyword_text']

# í‚¤ì›Œë“œ ë¶„ì„ í•¨ìˆ˜
def keyword_analysis(texts, stop_words=None, compound_words=None, use_komoran=True,
                     min_token_len=2, drop_numeric=True):
    """
    texts: list/Series of str (ì˜ˆ: df['keyword_text'])
    stop_words: set(str)
    compound_words: dict(str -> str)  # ì˜ˆ: {"ê³µìœ  ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤", "ì‹¤ì™¸ê¸°": "ì‹¤ì™¸ê¸°"}
    use_komoran: Trueë©´ konlpy Komoran ëª…ì‚¬ ì¶”ì¶œ ì‹œë„, ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ í† í¬ë‚˜ì´ì €ë¡œ í´ë°±
    min_token_len: ìµœì†Œ í† í° ê¸¸ì´(ê¸°ë³¸ 2ì)
    drop_numeric: ìˆ«ì ì „ìš© í† í° ì œê±° ì—¬ë¶€
    return: List[str]  # ì „ì²´ ë¬¸ì„œì—ì„œ ìˆ˜ì§‘í•œ í† í° ë¦¬ìŠ¤íŠ¸
    """
    stop_words = stop_words or set()
    compound_words = compound_words or {}

    # 0) Komoran ì‹œë„
    komoran = None
    if use_komoran:
        try:
            from konlpy.tag import Komoran  # í™˜ê²½ì— ë”°ë¼ ë¯¸ì„¤ì¹˜ì¼ ìˆ˜ ìˆìŒ
            komoran = Komoran()
        except Exception:
            komoran = None

    # 1) ë³µí•©ì–´ ì¹˜í™˜ íŒ¨í„´(ê¸¸ì´ ê¸´ key ìš°ì„ )
    comp_items = sorted(compound_words.items(), key=lambda x: len(x[0]), reverse=True)
    comp_patterns = [(re.compile(re.escape(k)), v) for k, v in comp_items]

    # 2) ë³µí•©ì–´ êµ¬ì„±ìš”ì†Œ ë¸”ë™ë¦¬ìŠ¤íŠ¸
    comp_components = {}
    for k, v in compound_words.items():
        parts = [p for p in re.split(r"\s+", k.strip()) if p]
        if parts:
            comp_components.setdefault(v, set()).update(parts)

    # 3) ì •ê·œì‹ í† í¬ë‚˜ì´ì €(í´ë°±)
    token_regex = re.compile(r"[ê°€-í£]+|[A-Za-z]+|\d+")

    def normalize_text(s: str) -> str:
        s = "" if s is None else str(s)
        s = unicodedata.normalize("NFKC", s)
        s = re.sub(r"\s+", " ", s).strip()
        return s

    def apply_compounds(s: str) -> str:
        for pat, rep in comp_patterns:
            s = pat.sub(rep, s)
        return s

    def tokenize(s: str):
        if komoran:
            try:
                return komoran.nouns(s)  # ëª…ì‚¬ë§Œ
            except Exception:
                pass
        return token_regex.findall(s)

    def filter_tokens(tokens, present_compounds):
        out = []
        for t in tokens:
            if not t:
                continue
            if drop_numeric and t.isdigit():
                continue
            if len(t) < min_token_len:
                continue
            if t in stop_words:
                continue
            # ë¬¸ì„œ ë‚´ ë³µí•©ì–´ê°€ ìˆìœ¼ë©´ ê·¸ êµ¬ì„±ìš”ì†Œ ì œê±°
            remove = False
            for comp in present_compounds:
                parts = comp_components.get(comp, set())
                if t in parts:
                    remove = True
                    break
            if not remove:
                out.append(t)
        return out

    all_tokens = []
    for raw in texts:
        s = normalize_text(raw)
        s = apply_compounds(s)
        toks = tokenize(s)
        present_compounds = set(t for t in toks if t in comp_components)
        toks = filter_tokens(toks, present_compounds)
        all_tokens.extend(toks)

    return all_tokens


# í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
keyword_counts = keyword_analysis(
    text_data,
    stop_words=stop_words,
    compound_words=compound_words,
    use_komoran=USE_KOMORAN
)


# ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ì‹œê°í™”
if keyword_counts:
    word_freq = Counter(keyword_counts).most_common(20)
    keywords, counts = zip(*word_freq)

    plt.figure(figsize=(14, 6))
    bars = plt.bar(keywords, counts, color='salmon')
    plt.title('VOC í‚¤ì›Œë“œ ë¹ˆë„ ìƒìœ„ 20')
    plt.xticks(rotation=45)
    plt.tight_layout()

    for bar in bars:
        height = bar.get_height()
        plt.annotate(f'{int(height)}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 5),
                     textcoords='offset points',
                     ha='center', va='bottom')
    plt.show()
else:
    print("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")






from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# í…ìŠ¤íŠ¸ ì†ŒìŠ¤ í™•ì¸(ì• ì…€ì—ì„œ text_dataê°€ ì—†ë‹¤ë©´ df['keyword_text'] ì‚¬ìš©)
text_data = globals().get("text_data", df["keyword_text"])

# 1) í‚¤ì›Œë“œ í† í° ì¶”ì¶œ
tokens = keyword_analysis(
    text_data,
    stop_words=stop_words,
    compound_words=compound_words,
    use_komoran=USE_KOMORAN
)

# 2) ë¹ˆë„ ì‚¬ì „ ìƒì„±(WordCloudëŠ” ë¹ˆë„ ë”•ì…”ë„ˆë¦¬ í•„ìš”)
freq = Counter(tokens)

# í•„ìš”ì‹œ ìƒìœ„ Nê°œë§Œ ì‚¬ìš©
TOP_N = 200
freq = dict(freq.most_common(TOP_N))

if not freq:
    print("í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ì „ì²˜ë¦¬/ì‚¬ì „ ì ìš©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
else:
    # 3) ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wc = WordCloud(
        font_path="/usr/share/fonts/nanum/NanumGothic-Regular.ttf",  # í•œê¸€ í°íŠ¸
        width=800,
        height=400,
        background_color="white"
    ).generate_from_frequencies(freq)

    # 4) ì‹œê°í™”
    plt.figure(figsize=(14, 6))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title("VOC ì›Œë“œí´ë¼ìš°ë“œ")
    plt.tight_layout()
    plt.show()



## HTML ë³€í™˜ !! 

import os
import nbformat
from bs4 import BeautifulSoup
from nbconvert import HTMLExporter

# === ì‚¬ìš©ì ì„¤ì • ===
notebook_input = "./keyword_analysis_hwd_ipark.ipynb"
html_output_path = "./keyword_analysis_hwd_ipark_2508.html"

# === 1ë‹¨ê³„: nbformat + HTMLExporterë¡œ HTML ë³€í™˜ (ì½”ë“œ ì…ë ¥ ì œê±°)
with open(notebook_input, encoding="utf-8") as f:
    notebook_node = nbformat.read(f, as_version=4)

html_exporter = HTMLExporter()
html_exporter.exclude_input = True  # ì½”ë“œ ì…ë ¥ ìˆ¨ê¹€
body, _ = html_exporter.from_notebook_node(notebook_node)

# === 2ë‹¨ê³„: BeautifulSoup ë¡œë“œ
soup = BeautifulSoup(body, "html.parser")

# === 3ë‹¨ê³„: 'Output hidden' í…ìŠ¤íŠ¸ ì œê±°
for output_hidden in soup.find_all(string=lambda text: isinstance(text, str) and "Output hidden;" in text):
    output_hidden.extract()

# === 4ë‹¨ê³„: ëª©ì°¨(TOC) ìƒì„±
toc_container = soup.new_tag("div", id="toc")
toc_title_tag = soup.new_tag("strong")
toc_title_tag.string = "ğŸ“– ëª©ì°¨"
toc_container.append(toc_title_tag)

toc_list = soup.new_tag("ul")
toc_container.append(toc_list)

header_tags = soup.find_all(["h1", "h2", "h3"])
current_h1 = None
current_h2 = None

for idx, header in enumerate(header_tags):
    if not header.has_attr("id"):
        header['id'] = f"toc_{idx}"

    link = soup.new_tag("a", href=f"#{header['id']}")
    link.string = header.get_text()
    list_item = soup.new_tag("li")
    list_item.append(link)

    if header.name == "h1":
        toc_list.append(list_item)
        current_h1 = list_item
        current_h2 = None
    elif header.name == "h2":
        if current_h1 is None:
            toc_list.append(list_item)
        else:
            if not current_h1.find('ul'):
                current_h1.append(soup.new_tag("ul"))
            current_h1.find('ul').append(list_item)
        current_h2 = list_item
    elif header.name == "h3":
        if current_h2 is not None:
            if not current_h2.find('ul'):
                current_h2.append(soup.new_tag("ul"))
            current_h2.find('ul').append(list_item)
        elif current_h1 is not None:
            if not current_h1.find('ul'):
                current_h1.append(soup.new_tag("ul"))
            current_h1.find('ul').append(list_item)
        else:
            toc_list.append(list_item)

# === 5ë‹¨ê³„: TOC ìŠ¤íƒ€ì¼ ì‚½ì…
style_tag = soup.new_tag("style")
style_tag.string = """
#toc {
    position: fixed;
    top: 20px;
    right: 20px;
    width: 250px;
    background: #f9f9f9;
    border: 1px solid #ddd;
    padding: 10px;
    max-height: 90vh;
    overflow-y: auto;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    z-index: 1000;
    font-family: sans-serif;
    font-size: 14px;
}
#toc ul {
    list-style: none;
    padding-left: 0;
}
#toc li {
    margin: 5px 0;
}
#toc li ul {
    padding-left: 15px;
}
#toc li ul li ul {
    padding-left: 15px;
}
#toc a {
    text-decoration: none;
    color: #333;
}
#toc a:hover {
    text-decoration: underline;
}
"""
soup.head.append(style_tag)

if soup.body:
    soup.body.insert(0, toc_container)

# === 6ë‹¨ê³„: ìµœì¢… HTML ì €ì¥
with open(html_output_path, "w", encoding="utf-8") as f:
    f.write(str(soup))







# import pandas as pd
# import re
# from konlpy.tag import Okt
# import matplotlib.pyplot as plt
# import seaborn as sns
# import matplotlib.font_manager as fm
# from io import BytesIO
# import base64


# def clean_text(text):
#     if not isinstance(text, str):
#         return ""
#     text = re.sub(r'<[^>]+>', '', text)
#     text = re.sub(r'&[a-z]+;', ' ', text)
#     text = re.sub(r'\(.*?\)', '', text)
#     text = re.sub(r'[\n\t\r]', ' ', text)
#     text = re.sub(r'\s+', ' ', text)
#     return text.strip()

# # í…ìŠ¤íŠ¸ í†µí•© ë° ì „ì²˜ë¦¬
# df["full_text"] = df["title"] + " " + df["request_contents"] + " " + df["reply"]
# df["full_text"] = df["full_text"].apply(clean_text)



# def classify_topic_and_type(text):
#     text = text.lower()

#     if any(word in text for word in ["í˜•ê´‘ë“±", "ì „ë“±", "ì¡°ëª…", "ë“± ë‚˜ê°”", "ë“± ê³ ì¥", "ë§¤ë¦½ë“±", "ë¶ˆì´ ì•ˆ", "ë¶ˆì´ ë‚˜ê°"]):
#         return "ì¡°ëª… ë¬¸ì œ", "ì „ë“± êµì²´"
#     elif "íœ´ì§€" in text:
#         return "íœ´ì§€ ë¶€ì¡±", "ìì¬ ì ê²€"
#     elif any(word in text for word in ["ë°”í€´ë²Œë ˆ", "íŒŒë¦¬", "ëª¨ê¸°", "ë²Œë ˆ", "í•´ì¶©"]):
#         return "í•´ì¶© ë¬¸ì œ", "ë°©ì—­"
#     elif any(word in text for word in ["ë³€ê¸° ë§‰í˜", "ë³€ê¸° ë§‰í˜”", "ë³€ê¸° ë¬¼ ì•ˆë‚´ë ¤ê°", "ë³€ê¸° ì•ˆ ë‚´ë ¤ê°€", "ë³€ê¸° ì´ìƒ", "ë³€ê¸° ê³ ì¥", "ë³€ê¸° ë¬¼ì´", "ë³€ê¸° ì—­ë¥˜"]):
#         return "ë³€ê¸° ë§‰í˜", "ë°°ê´€ ì ê²€"
#     elif any(word in text for word in ["ì—ì–´ì»¨", "ê³µì¡°ê¸°", "ì¶”ì›Œìš”", "ë”ì›Œìš”", "ëƒ‰ë°©", "ë‚œë°©", "ë¥ìŠµë‹ˆë‹¤", "ì¶¥ìŠµë‹ˆë‹¤", "ë¥ë‹¤", "ì¶¥ë‹¤", "ì‹œì›í•˜ì§€", "ë”ìš´ ë°”ëŒ", "ì°¬ ë°”ëŒ"]):
#         return "ì˜¨ë„ ë¯¼ì›", "ì˜¨ë„ ì¡°ì ˆ"
#     elif "ì†ì„¸ì •ì œ" in text:
#         return "ìœ„ìƒìš©í’ˆ ìš”ì²­", "ìš”ì²­ ì ê²€"
#     elif "ë¹„ë° ì„¤ì¹˜" in text or "ë¹„ë°ê°€ ì—†" in text:
#         return "ë¹„ë° ì„¤ì¹˜ ìš”ì²­", "ìš”ì²­ ì ê²€"
#     elif "ë¹„ë°" in text and any(w in text for w in ["ê³ ì¥", "ì•ˆ ë‚˜ì™€", "ì´ìƒ", "ì‘ë™ì•ˆ", "ì‚¬ìš© ë¶ˆê°€", "ëˆŒëŸ¬ë„", "ì•ˆë¨"]):
#         return "ë¹„ë° ê³ ì¥", "ì‹œì„¤ ì ê²€"
#     elif any(word in text for word in ["ì„¸ë©´ëŒ€", "ìˆ˜ì „", "ë¬¼ ì•ˆë‚˜ì˜´", "ë¬¼ ì•ˆ ë‚˜ì™€", "ìˆ˜ì••ì´ ì•½í•¨", "ë¬¼ì´ ì•ˆ ë‚˜ì˜´"]):
#         return "ì„¸ë©´ëŒ€ ë¬¸ì œ", "ìˆ˜ë„ ì ê²€"
#     elif any(word in text for word in ["ë¬¸ì´ ì•ˆ ì—´ë¦¼", "ë¬¸ ì•ˆì—´ë¦¼", "ë¬¸ì´ ì ê¹€", "ë¬¸ì´ ê³ ì¥", "ë¬¸ì´ ì•ˆ ì ê¹€", "ë„ì–´ ê³ ì¥"]):
#         return "ë„ì–´ ë¬¸ì œ", "ë„ì–´ ì ê²€"
#     elif any(word in text for word in ["ì„¤ì¹˜í•´", "ê±´ì˜", "ìš”ì²­", "ì˜ê²¬", "ì¶”ê°€ ì„¤ì¹˜", "ë¹„ì¹˜ ìš”ì²­", "êµ¬ë¹„ ìš”ì²­", "ìˆìœ¼ë©´ ì¢‹ê² "]):
#         return "ìš”ì²­ ë° ê±´ì˜", "ìš”ì²­ ì ê²€"
#     elif any(word in text for word in ["ê³ ì¥", "ì´ìƒ", "ì‘ë™ ì•ˆ", "ì•ˆë¨", "ë©ˆì¶¤", "ë¬¸ì œ ë°œìƒ"]) and not any(
#         excl in text for excl in ["ë¹„ë°", "ì—ì–´ì»¨", "ì „ë“±", "ë³€ê¸°", "ì„¸ë©´ëŒ€", "ë„ì–´"]
#     ):
#         return "ì‹œì„¤ ê³ ì¥", "ì‹œì„¤ ì ê²€"
#     else:
#         return "ê¸°íƒ€", "ê¸°íƒ€"

# # 4. ë¶„ë¥˜ ì ìš©
# df[["ì£¼ì œ", "ì‘ì—…ìœ í˜•"]] = df["full_text"].apply(lambda x: pd.Series(classify_topic_and_type(x)))



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm

# 1. í°íŠ¸ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_name = fm.FontProperties(fname=font_path).get_name()
plt.rc("font", family=font_name)
sns.set(font=font_name, rc={"axes.unicode_minus": False})

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# Result 1 ì‹œíŠ¸ì˜ ì²« í–‰ì„ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , ë¬¸ìì—´ë¡œ í†µì¼
df = pd.read_excel("Result_1_processed_2.xlsx", sheet_name="Result 1", header=0, dtype=str)
# df = pd.read_excel("retagged_calibrated.xlsx", sheet_name="Sheet1", header=0, dtype=str)
df.columns = [str(c).strip() for c in df.columns]

# voc_dateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
df['voc_date'] = pd.to_datetime(df['voc_date'], errors='coerce')

# 2025ë…„ 8ì›” ë°ì´í„°ë§Œ ì¶”ì¶œ
df = df[
    (df['voc_date'].dt.year == 2025) & (df['voc_date'].dt.month == 8)
]

# 2-1. ê²°í•© ìœ í‹¸: "ëŒ€ë¶„ë¥˜-ì¤‘ë¶„ë¥˜" í˜•íƒœ, í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ê·¸ ê°’, ëª¨ë‘ ì—†ìœ¼ë©´ "ë¯¸ë¶„ë¥˜"
def combine_major_minor(major, minor):
    a = "" if pd.isna(major) else str(major).strip()
    b = "" if pd.isna(minor) else str(minor).strip()
    if a and b:
        return f"{a}-{b}"
    return a or b or "ë¯¸ë¶„ë¥˜"

# 2-2. ì£¼ì œ/ì‘ì—…ìœ í˜• ê²°í•© ì»¬ëŸ¼ ìƒì„±
required_cols = ["ì£¼ì œ ëŒ€ë¶„ë¥˜", "ì£¼ì œ ì¤‘ë¶„ë¥˜", "ì‘ì—…ìœ í˜• ëŒ€ë¶„ë¥˜", "ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}")

df.loc[:, "ì£¼ì œ"] = [
    combine_major_minor(mj, mn)
    for mj, mn in zip(df["ì£¼ì œ ëŒ€ë¶„ë¥˜"], df["ì£¼ì œ ì¤‘ë¶„ë¥˜"])
]
# df.loc[:, "ì‘ì—…ìœ í˜•"] = [
#     combine_major_minor(mj, mn)
#     for mj, mn in zip(df["ì‘ì—…ìœ í˜• ëŒ€ë¶„ë¥˜"], df["ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"])
# ]
df.loc[:, "ì‘ì—…ìœ í˜•"] = [combine_major_minor(None, mn) for mn in df["ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"]]

# 3. ì „ì²´ íˆíŠ¸ë§µ: ì£¼ì œ vs ì‘ì—…ìœ í˜•
topic_type_pivot = (
    df.groupby(["ì£¼ì œ", "ì‘ì—…ìœ í˜•"])
      .size()
      .unstack(fill_value=0)
)

plt.figure(figsize=(14, 8))
sns.heatmap(topic_type_pivot, annot=True, fmt=".0f", cmap="Blues")
plt.title("ì£¼ì œë³„ ì‘ì—…ìœ í˜• ë¶„í¬ (ì „ì²´)")
plt.yticks(rotation=0)
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()



# import pandas as pd
# from konlpy.tag import Okt
# from collections import Counter
# import psycopg2

# okt = Okt()


# # ìˆ˜ë™ ë¶ˆìš©ì–´ ë° ë³µí•©ì–´ ì‚¬ì „ ì ìš© í•¨ìˆ˜
# def apply_manual_dicts(compound_words, stop_words):
#     manual_stop_words = {
#         "í›„", "ë‹¬ë¼", "ì—†ë‹¤ë‡¨", "ì²«", "ë¬¼ì´", "ê²¨", "ì²˜ë¦¬", "ì¡°ì¹˜", "ë™ì•ˆ", "ìœ„í•´",
#         "ë°”ë¡œ", "ë‚´", "ì‹œ", "ì", "ì", "í¸", "í˜¸", "ì‹¤", "ìˆ˜", "ì¸µ", "ìª½", "ì „", "ì•„ì´íë¹„ì•„", "ìš”ì¦˜", "í™•ì¸", "ì™„ë£Œ", "ê¹€ì†Œí˜„", "ì•„ì´í", "ë¹„ì•„"
#     }
#     stop_words.update(manual_stop_words)

#     manual_compounds = {
#         "íƒ•ë¹„ì‹¤": "íƒ•ë¹„ì‹¤",
#         "ê³µìœ ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤",
#         "ê³µìœ  ì˜¤í”¼ìŠ¤": "ê³µìœ ì˜¤í”¼ìŠ¤",
#         "ì„ì›ì‹¤": "ì„ì›ì‹¤",
#         "ë°©í–¥ì œ": "ë°©í–¥ì œ",
#         "ìë™ë¬¸": "ìë™ë¬¸",
#         "ë””ìŠ¤íœì„œ": "ë””ìŠ¤íœì„œ",
#         "ì‹¤ì™¸ê¸°":"ì‹¤ì™¸ê¸°"
#     }
#     compound_words.update(manual_compounds)
#     return compound_words, stop_words


# # í‚¤ì›Œë“œ ë¶„ì„ í•¨ìˆ˜
# def keyword_analysis(text, stop_words, compound_words):
#     try:
#         if pd.isna(text):
#             return {}

#         extracted_compounds = []
#         matched_words = []

#         for word in compound_words.keys():
#             if word in text:
#                 matched_words.append(word)
#                 extracted_compounds.append(compound_words[word])
#                 text = text.replace(word, '')

#         nouns = okt.nouns(text)
#         all_nouns = extracted_compounds + [n for n in nouns if n not in stop_words]
#         return dict(Counter(all_nouns))

#     except Exception as e:
#         return {}


# # í‚¤ì›Œë“œ í…Œì´ë¸” ìƒì„± í•¨ìˆ˜
# def generate_keyword_table(df):
#     df_clean = df.dropna(subset=["clean_text"])
#     compound_words, stop_words = load_text_dict_from_db()
#     compound_words, stop_words = apply_manual_dicts(compound_words, stop_words)

#     result_rows = []

#     teams = df_clean["user_group_name"].dropna().unique()
#     for team in teams:
#         df_team = df_clean[df_clean["user_group_name"] == team]
#         grouped = df_team.groupby(["ì£¼ì œ", "ì‘ì—…ìœ í˜•"])

#         for (topic, work_type), sub_df in grouped:
#             if sub_df.empty:
#                 continue
#             full_text = " ".join(sub_df["clean_text"].dropna().astype(str).tolist())
#             freq_dict = keyword_analysis(full_text, stop_words, compound_words)

#             if not freq_dict:
#                 continue

#             top_keywords = sorted(freq_dict.items(), key=lambda x: x[1], reverse=True)[:10]
#             keyword_str = ", ".join([kw for kw, _ in top_keywords])
#             result_rows.append({
#                 "íŒ€": team,
#                 "ì£¼ì œ": topic,
#                 "ì‘ì—…ìœ í˜•": work_type,
#                 "í‚¤ì›Œë“œ Top10": keyword_str
#             })

#     result_df = pd.DataFrame(result_rows)
#     result_df.sort_values(by=["íŒ€", "ì£¼ì œ", "ì‘ì—…ìœ í˜•"], inplace=True)
#     result_df.set_index(["íŒ€", "ì£¼ì œ", "ì‘ì—…ìœ í˜•"], inplace=True)
#     return result_df


# # ì‹¤í–‰
# df = pd.read_excel("2506271256_VOC_á„‡á…®á†«á„‰á…¥á†¨_á„‹á…­á„á…¥á†¼á„€á…¥á†«á„‹á…´á„á…©á†¼á„’á…¡á†¸_á„€á…§á†¯á„€á…ª.xlsx")
# df_keywords = generate_keyword_table(df)

# # ì¶œë ¥
# display(df_keywords)

