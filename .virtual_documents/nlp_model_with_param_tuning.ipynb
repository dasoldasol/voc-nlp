








import pandas as pd
import numpy as np
import re, html, unicodedata
from pathlib import Path

# ===== ì„ íƒ: DBì—ì„œ compound/stop ì‚¬ì „ì„ ì½ëŠ” í•¨ìˆ˜ (ì‚¬ìš©ì ì œê³µ ì½”ë“œ) =====

def load_text_dict_from_db():
    import psycopg2
    try:
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()

        # compound_words: key, value ëª¨ë‘ strip
        cursor.execute("""
            SELECT word_key, word_value 
            FROM stat_text_dict 
            WHERE type = 'compound' AND is_active = TRUE
        """)
        compound_result = cursor.fetchall()
        compound_words = {
            row[0].strip(): row[1].strip()
            for row in compound_result if row[0] and row[1]
        }

        # stop_words
        cursor.execute("""
            SELECT word_key 
            FROM stat_text_dict 
            WHERE type = 'stop' AND is_active = TRUE
        """)
        stop_result = cursor.fetchall()
        stop_words = {row[0].strip() for row in stop_result if row[0]}

        cursor.close()
        connection.close()
        return compound_words, stop_words

    except Exception:
        # ì—°ê²° ì‹¤íŒ¨ ì‹œ ë¹ˆ ì‚¬ì „/ì…‹ìœ¼ë¡œ ì§„í–‰
        return {}, set()

# ===== ì „ì²˜ë¦¬ ìœ í‹¸ =====
def strip_html(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = str(text)
    s = html.unescape(s)
    # <script>/<style> ë¸”ë¡ ì œê±°
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)
    # ë‚˜ë¨¸ì§€ íƒœê·¸ ì œê±°
    s = re.sub(r"(?s)<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# í—ˆìš© ë¬¸ì í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
ALLOWED_PATTERN = re.compile(r"[^ê°€-í£ã„±-ã…ã…-ã…£A-Za-z0-9\s\.\,\!\?\;\:\'\"\(\)\[\]\{\}\-\_\/\&\+\%\#\@]")
def whitelist_text(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = strip_html(text)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[\u200B-\u200D\uFE0E\uFE0F]", "", s)  # ì œë¡œí­/variation selector ì œê±°
    s = ALLOWED_PATTERN.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# ===== taxonomy íŒŒì‹± =====
def detect_taxonomy_columns(df: pd.DataFrame):
    cols = list(df.columns)
    def find_first(patterns):
        for c in cols:
            for p in patterns:
                if re.search(p, str(c), re.I):
                    return c
        return None
    major_col = find_first([r"ëŒ€ë¶„ë¥˜"])
    minor_col = find_first([r"ì¤‘ë¶„ë¥˜"])
    example_cols = [c for c in cols if re.search(r"ì‚¬ë¡€|ì˜ˆì‹œ", str(c), re.I)]
    keyword_cols = [c for c in cols if re.search(r"í‚¤ì›Œë“œ", str(c), re.I)]
    if not example_cols and not keyword_cols:
        keyword_cols = [c for c in cols if re.search(r"íŒ¨í„´|ë£°|rule|pattern", str(c), re.I)]
    return major_col, minor_col, example_cols, keyword_cols

def row_patterns(row: pd.Series, cols):
    patterns = []
    for c in cols:
        if c is None or c not in row or pd.isna(row[c]): 
            continue
        val = str(row[c]).strip()
        if not val:
            continue
        parts = re.split(r"[,\;\|\n\r\t]+", val)
        parts = [p.strip() for p in parts if p.strip()]
        if not parts:
            parts = [val]
        patterns.extend(parts)
    return list(dict.fromkeys(patterns))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°

def find_tax_etc(items):
    # taxonomy ì•ˆì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” 'ê¸°íƒ€'ë§Œ í—ˆìš©
    for it in items:
        if it["major"] == "ê¸°íƒ€" and it["minor"] == "ê¸°íƒ€":
            return ("ê¸°íƒ€", "ê¸°íƒ€")
    for it in items:
        if it["major"] == "ê¸°íƒ€":
            return (it["major"], it["minor"] if it["minor"] else "ê¸°íƒ€")
    return None

# ===== í† í°/ë¬¸ì¥ ë¶„í•´ =====
SENT_SPLIT = re.compile(r"[\.!\?â€¦\n\r]+|[ã€‚ï¼ï¼Ÿ]")
TOKEN_RE = re.compile(r"[ê°€-í£A-Za-z0-9]+")

# ===== ê¸¸ì´ ê°€ì¤‘ =====
def _length_weight(token_count: int, short_len: int, long_len: int, short_penalty: float, long_penalty: float):
    if token_count <= short_len:
        return short_penalty
    if token_count >= long_len:
        return long_penalty
    return 1.0

# ===== ë³µí•©ì–´/ë¶ˆìš©ì–´ ì •ê·œí™” íŒŒì´í”„ë¼ì¸ (ì¶”ê°€) =====
def build_compound_replacers(compound_words: dict):
    """
    compound_wordsì˜ keyë¥¼ ê¸´ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ì¹˜í™˜ìš© ì •ê·œì‹ ë¦¬ìŠ¤íŠ¸ë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    """
    if not compound_words:
        return []
    keys_sorted = sorted([k for k in compound_words.keys() if k], key=len, reverse=True)
    replacers = []
    for k in keys_sorted:
        v = compound_words[k]
        pattern = re.compile(re.escape(k), flags=re.IGNORECASE)
        replacers.append((pattern, v))
    return replacers

def apply_compound(text: str, replacers: list):
    if not text or not replacers:
        return text or ""
    s = text
    for rgx, rep in replacers:
        s = rgx.sub(rep, s)
    return s

def remove_stopwords(text: str, stop_words: set):
    """
    TOKEN_RE ê¸°ì¤€ìœ¼ë¡œ í† í°í™” í›„ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  ë‹¤ì‹œ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
    """
    if not text:
        return ""
    tokens = TOKEN_RE.findall(text)
    if not tokens:
        return text.strip()
    if not stop_words:
        return " ".join(tokens)
    kept = [t for t in tokens if t not in stop_words]
    return " ".join(kept)

def preprocess_for_match(raw_text: str, compound_replacers: list, stop_words: set):
    """
    ë§¤ì¹­/ìœ ì‚¬ë„ ê³„ì‚° ì „ìš© ì •ê·œí™” íŒŒì´í”„ë¼ì¸.
    1) HTML/ì´ìƒë¬¸ì ì œê±° + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(whitelist_text)
    2) NFKC ì •ê·œí™”(whitelist_text ë‚´ë¶€)
    3) ì†Œë¬¸ìí™”
    4) ë³µí•©ì–´ ì¹˜í™˜
    5) ë¶ˆìš©ì–´ ì œê±°
    """
    base = whitelist_text(raw_text)
    base = base.lower()
    base = apply_compound(base, compound_replacers)
    base = remove_stopwords(base, stop_words)
    base = re.sub(r"\s+", " ", base).strip()
    return base

# ===== ì¤‘ìš”ë„ ê¸°ë°˜ ë¶„ë¥˜ (êµì²´ëœ í•¨ìˆ˜ë“¤) =====
def _overlap_with_title(sent_norm: str, title_norm: str, overlap_ratio: float):
    """
    ì •ê·œí™”ëœ ë¬¸ì¥/ì œëª©ì„ í† í°ì§‘í•©ìœ¼ë¡œ ë¹„êµí•˜ì—¬ ê²¹ì¹¨ ë¹„ìœ¨ë¡œ ìœ ì‚¬ë„ë¥¼ íŒì •.
    """
    if not title_norm:
        return False
    stoks = set(TOKEN_RE.findall(sent_norm))
    ttoks = set(TOKEN_RE.findall(title_norm))
    if not stoks or not ttoks:
        return False
    inter = len(stoks & ttoks)
    return inter >= max(1, int(overlap_ratio * len(stoks)))

def build_taxonomy(tax_df: pd.DataFrame, compound_replacers: list = None, stop_words: set = None):
    """
    ê¸°ì¡´ itemsì— 'norm_patterns'ë¥¼ ì¶”ê°€.
    norm_patternsëŠ” íŒ¨í„´ë§ˆë‹¤ preprocess_for_matchë¥¼ ì ìš©í•œ ê²°ê³¼.
    """
    compound_replacers = compound_replacers or []
    stop_words = stop_words or set()

    major_col, minor_col, ex_cols, kw_cols = detect_taxonomy_columns(tax_df)
    items = []
    for _, row in tax_df.iterrows():
        major = str(row[major_col]).strip() if (major_col and major_col in row and not pd.isna(row[major_col])) else ""
        minor = str(row[minor_col]).strip() if (minor_col and minor_col in row and not pd.isna(row[minor_col])) else ""
        ex_patterns = row_patterns(row, ex_cols) if ex_cols else []
        kw_patterns = row_patterns(row, kw_cols) if kw_cols else []
        patterns = list(dict.fromkeys(ex_patterns + kw_patterns))

        # íŒ¨í„´ ì •ê·œí™”ë³¸
        norm_patterns = []
        for p in patterns:
            pn = preprocess_for_match(p, compound_replacers, stop_words)
            if pn:
                norm_patterns.append(pn)

        if major or minor or patterns:
            items.append({
                "major": major,
                "minor": minor,
                "patterns": patterns,
                "norm_patterns": norm_patterns
            })
    return items

def classify_weighted_general(
    all_text: str,
    items: list,
    title_text: str = None,
    # ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„°
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # ì¶”ê°€ íŒŒë¼ë¯¸í„°
    compound_replacers: list = None,
    stop_words: set = None
):
    """
    ë°˜í™˜: (major, minor, score)
    ë¬¸ì¥ ì¤‘ìš”ë„ = ìœ„ì¹˜ ê°€ì¤‘ Ã— ì œëª© ìœ ì‚¬ ê°€ì¤‘ Ã— ê¸¸ì´ ë³´ì •
    í•­ëª© ì ìˆ˜ = Î£[ ë¬¸ì¥ê°€ì¤‘ì¹˜ Ã— (íŒ¨í„´ë§¤ì¹­ê°œìˆ˜ + length_bonus_scale * ë§¤ì¹­íŒ¨í„´ê¸¸ì´í•©) ]
    ë§¤ì¹­ì€ 'ì •ê·œí™”ëœ ë¬¸ì¥' ëŒ€ 'ì •ê·œí™”ëœ íŒ¨í„´(norm_patterns)'ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    compound_replacers = compound_replacers or []
    stop_words = stop_words or set()

    text = "" if all_text is None else str(all_text)
    sentences = [s.strip() for s in SENT_SPLIT.split(text) if s.strip()]
    if not sentences:
        sentences = [text]

    # ì •ê·œí™”ëœ ì œëª©
    title_norm = preprocess_for_match(title_text or "", compound_replacers, stop_words)

    item_scores = np.zeros(len(items), dtype=float)

    for idx_s, sent in enumerate(sentences):
        # ì›ë¬¸ ê¸°ì¤€ í† í° ê¸¸ì´(ê¸¸ì´ ë³´ì •ì€ ì›ë¬¸ í† í° ìˆ˜ë¡œ ê³„ì‚°)
        tok_cnt = len(TOKEN_RE.findall(sent.lower()))
        # ë§¤ì¹­/ìœ ì‚¬ë„ëŠ” ì •ê·œí™” í…ìŠ¤íŠ¸ë¡œ
        s_norm = preprocess_for_match(sent, compound_replacers, stop_words)

        # 1) ìœ„ì¹˜ ê°€ì¤‘ì¹˜: pos_max -> pos_min ì„ í˜•ê°ì‡ 
        step = min(idx_s, max(1, decay_steps))
        pos_w = pos_max - step * ((pos_max - pos_min) / max(1, decay_steps))
        pos_w = max(pos_min, min(pos_max, pos_w))

        # 2) ì œëª© ìœ ì‚¬ ê°€ì¤‘ì¹˜
        title_w = (title_boost if _overlap_with_title(s_norm, title_norm, title_overlap_ratio) else 1.0)

        # 3) ê¸¸ì´ ë³´ì •
        len_w = _length_weight(tok_cnt, short_len, long_len, short_penalty, long_penalty)

        sent_w = pos_w * title_w * len_w

        # í•­ëª©ë³„ ë§¤ì¹­ ì ìˆ˜
        for i, it in enumerate(items):
            norm_ps = it.get("norm_patterns")
            if norm_ps is None:
                # í•˜ìœ„í˜¸í™˜: patternsë¥¼ ê·¸ë•Œê·¸ë•Œ ì •ê·œí™”
                norm_ps = [preprocess_for_match(p, compound_replacers, stop_words) for p in it.get("patterns", [])]

            hits = 0
            length_bonus = 0
            for p in norm_ps:
                if not p:
                    continue
                if p in s_norm:
                    hits += 1
                    length_bonus += len(p)
            if hits > 0:
                item_scores[i] += sent_w * (hits + length_bonus_scale * length_bonus)

    if item_scores.max() > 0:
        best_i = int(item_scores.argmax())
        return items[best_i]["major"], items[best_i]["minor"], float(item_scores[best_i])
    return "", "", 0.0

# ===== ì¬íƒœê¹… ë©”ì¸ (êµì²´ëœ run_retagging) =====
def run_retagging(
    input_path: Path,
    output_path: Path,
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "ì£¼ì œ taxonomy",
    work_tax_sheet: str = "ì‘ì—…ìœ í˜• taxonomy",
    # ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # ì‚¬ì „ ì£¼ì…
    compound_words: dict = None,
    stop_words: set = None
) -> pd.DataFrame:
    """
    ì…ë ¥ íŒŒì¼ì˜ taxonomy ì‹œíŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Result 1 ì‹œíŠ¸ë¥¼ ì¬íƒœê¹…í•˜ê³ ,
    output_pathë¡œ ëª¨ë“  ì‹œíŠ¸ë¥¼ ë³´ì¡´í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    ë°˜í™˜ê°’: ê°±ì‹ ëœ Result 1 ë°ì´í„°í”„ë ˆì„
    """
    compound_words = compound_words or {}
    stop_words = stop_words or set()
    compound_replacers = build_compound_replacers(compound_words)

    xls = pd.ExcelFile(input_path)
    sheet_names = xls.sheet_names

    result_df = pd.read_excel(xls, sheet_name=result_sheet, header=0, dtype=str)
    subject_tax = pd.read_excel(xls, sheet_name=subject_tax_sheet, header=0, dtype=str)
    work_tax    = pd.read_excel(xls, sheet_name=work_tax_sheet, header=0, dtype=str)

    for col in ["title", "request_contents", "reply"]:
        if col not in result_df.columns:
            raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {col}")

    # all_text êµ¬ì„± (ì›ë¬¸ ë³´ì¡´). SettingWithCopyWarning ë°©ì§€ ìœ„í•´ .loc ì‚¬ìš©
    result_df.loc[:, "title_clean"] = result_df["title"].map(whitelist_text).fillna("")
    result_df.loc[:, "all_text"] = (
        result_df["request_contents"].map(whitelist_text).fillna("") + " " +
        result_df["title_clean"] + " " +
        result_df["reply"].map(whitelist_text).fillna("")
    ).str.strip()

    # taxonomy ì•„ì´í…œ: ì •ê·œí™” íŒ¨í„´ í¬í•¨
    subject_items = build_taxonomy(subject_tax, compound_replacers, stop_words)
    work_items    = build_taxonomy(work_tax, compound_replacers, stop_words)
    subject_etc   = find_tax_etc(subject_items)
    work_etc      = find_tax_etc(work_items)

    # ì£¼ì œ ê³ ì • title 
    fixed_mapping = {
        "ë”ì›Œìš” ğŸ¥µ": ("ì‹œì„¤", "ëƒ‰ë‚œë°©/ê³µì¡°"),
        "ì¶”ì›Œìš” ğŸ¥¶": ("ì‹œì„¤", "ëƒ‰ë‚œë°©/ê³µì¡°"),
        "ì²­ì†Œí•´ì£¼ì„¸ìš” ğŸ§¹": ("í™˜ê²½", "ì²­ê²°/ë¯¸í™”"),
        "ì „ì› í™•ì¸ ğŸ”Œ": ("ì‹œì„¤", "ì „ê¸°/ì¡°ëª…"),
        "ëƒ„ìƒˆ ì‹¬í•´ìš” ğŸ’©": ("í™˜ê²½", "ì†ŒìŒ/ëƒ„ìƒˆ/í™˜ê¸°"),
    }

    # ê³ ì • ì—¬ë¶€ í”Œë˜ê·¸ ì»¬ëŸ¼ ì´ˆê¸°í™”
    if "ì£¼ì œ_ë¶„ë¥˜_ê³ ì •" not in result_df.columns:
        result_df.loc[:, "ì£¼ì œ_ë¶„ë¥˜_ê³ ì •"] = False

    # ë¶„ë¥˜
    sub_major, sub_minor, work_major, work_minor = [], [], [], []
    sub_score, work_score = [], []

    # ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ìˆœíšŒí•˜ì—¬ ê³ ì • íƒ€ì´í‹€ì„ ë¨¼ì € ì²˜ë¦¬
    for idx, (all_text, title_text) in enumerate(zip(result_df["all_text"], result_df["title_clean"])):

        # ===== FIX START: ì›ë¬¸ title ì™„ì „ ì¼ì¹˜ ì‹œì—ë§Œ ë¶„ë¥˜ ê³ ì • =====
        raw_title = result_df.at[idx, "title"]
        mj_fixed, mn_fixed = None, None

        if isinstance(raw_title, str) and raw_title in fixed_mapping:
            mj_fixed, mn_fixed = fixed_mapping[raw_title]

        if mj_fixed is not None:
            # ì£¼ì œëŠ” ê³ ì •, ì‘ì—…ìœ í˜•ì€ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ë¶„ë¥˜
            sub_major.append(mj_fixed); sub_minor.append(mn_fixed); sub_score.append(float("inf"))
            result_df.at[idx, "ì£¼ì œ_ë¶„ë¥˜_ê³ ì •"] = True

            # ì‘ì—…ìœ í˜• ë¶„ë¥˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰
            mj2, mn2, sc2 = classify_weighted_general(
                all_text, work_items, title_text=title_text,
                pos_max=pos_max, pos_min=pos_min,
                title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
                short_len=short_len, long_len=long_len,
                short_penalty=short_penalty, long_penalty=long_penalty,
                decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
                compound_replacers=compound_replacers, stop_words=stop_words
            )
            if mj2 == "":
                mj2, mn2 = (work_etc if work_etc is not None else ("", ""))
            work_major.append(mj2); work_minor.append(mn2); work_score.append(sc2)
            continue
        # ===== FIX END =====

        # ê³ ì •ì´ ì•„ë‹ˆë©´ ê¸°ì¡´ ë¡œì§ìœ¼ë¡œ ë¶„ë¥˜
        mj, mn, sc = classify_weighted_general(
            all_text, subject_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj == "":
            mj, mn = (subject_etc if subject_etc is not None else ("", ""))
        sub_major.append(mj); sub_minor.append(mn); sub_score.append(sc)

        mj2, mn2, sc2 = classify_weighted_general(
            all_text, work_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj2 == "":
            mj2, mn2 = (work_etc if work_etc is not None else ("", ""))
        work_major.append(mj2); work_minor.append(mn2); work_score.append(sc2)

    result_df.loc[:, "ì£¼ì œ ëŒ€ë¶„ë¥˜"] = sub_major
    result_df.loc[:, "ì£¼ì œ ì¤‘ë¶„ë¥˜"] = sub_minor
    result_df.loc[:, "ì‘ì—…ìœ í˜• ëŒ€ë¶„ë¥˜"] = work_major
    result_df.loc[:, "ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜"] = work_minor
    # ì ìˆ˜ëŠ” ì°¸ê³ ìš©
    result_df.loc[:, "ì£¼ì œ_score"] = sub_score
    result_df.loc[:, "ì‘ì—…ìœ í˜•_score"] = work_score

    # ëª¨ë“  ì‹œíŠ¸ ìœ ì§€í•˜ì—¬ ì €ì¥
    with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
        for name in sheet_names:
            if name == result_sheet:
                result_df.to_excel(writer, index=False, sheet_name=name)
            else:
                pd.read_excel(xls, sheet_name=name).to_excel(writer, index=False, sheet_name=name)

    return result_df





compound_words, stop_words = load_text_dict_from_db()  # í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì • ì‹œ {} / set() ë°˜í™˜

_ = run_retagging(
    input_path=Path("nsqure_2508.xlsx"),
    output_path=Path("nsquare_2508_tagged.xlsx"),
    # í•„ìš” ì‹œ ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„° íŠœë‹
    pos_max=2.5, pos_min=1.1,
    title_boost=3.0, title_overlap_ratio=0.1,
    short_len=10, long_len=65,
    short_penalty=0.75, long_penalty=1.0,
    decay_steps=10, length_bonus_scale=0.15,
    # ì‚¬ì „ ì£¼ì…
    compound_words=compound_words,
    stop_words=stop_words
)

# 'pos_max': 2.5, 'pos_min': 1.1, 
#  'title_boost': 3.0, 'title_overlap_ratio': 0.1, 
#  'short_len': 10, 'long_len': 65, 
#  'short_penalty': 0.75, 'long_penalty': 1.0, 
#  'decay_steps': 10, 'length_bonus_scale': 0.15






from dataclasses import dataclass
from typing import Dict, Any, Tuple, List, Optional
import itertools, random, numpy as np
import pandas as pd

@dataclass
class ClassifierParams:
    pos_max: float = 1.5
    pos_min: float = 1.0
    title_boost: float = 2.0
    title_overlap_ratio: float = 0.3
    short_len: int = 5
    long_len: int = 60
    short_penalty: float = 0.9
    long_penalty: float = 0.9
    decay_steps: int = 5
    length_bonus_scale: float = 0.1

# ê²€ìƒ‰ ê³µê°„(í•©ë¦¬ì  ë²”ìœ„ ì˜ˆì‹œ)
SPACE = {
    "pos_max": (1.2, 2.5),
    "pos_min": (0.8, 1.1),
    "title_boost": (1.0, 3.0),
    "title_overlap_ratio": (0.1, 0.6),
    "short_len": (3, 10),
    "long_len": (40, 90),
    "short_penalty": (0.5, 1.0),
    "long_penalty": (0.5, 1.0),
    "decay_steps": (3, 10),
    "length_bonus_scale": (0.05, 0.25),
}

def _linspace_int(lo:int, hi:int, k:int)->List[int]:
    if k<=1:
        return [int(round((lo+hi)/2))]
    arr = np.linspace(lo, hi, num=k)
    return list(sorted({int(round(x)) for x in arr}))

def _linspace_float(lo:float, hi:float, k:int)->List[float]:
    if k<=1:
        return [round((lo+hi)/2, 4)]
    arr = np.linspace(lo, hi, num=k)
    return [round(float(x), 4) for x in arr]

def make_grid(space:Dict[str, Tuple[float,float]], density:int=3)->Dict[str, List[Any]]:
    grid = {}
    for k,(lo,hi) in space.items():
        if isinstance(lo, int) and isinstance(hi, int):
            grid[k] = _linspace_int(lo, hi, density)
        else:
            grid[k] = _linspace_float(lo, hi, density)
    return grid

def sample_random(space:Dict[str, Tuple[float,float]], n:int, seed:int=42)->List[Dict[str, Any]]:
    rnd = random.Random(seed)
    samples = []
    for _ in range(n):
        cand = {}
        for k,(lo,hi) in space.items():
            if isinstance(lo, int) and isinstance(hi, int):
                cand[k] = rnd.randint(lo, hi)
            else:
                cand[k] = round(rnd.uniform(lo, hi), 4)
        samples.append(cand)
    return samples

def _safe_col(df:pd.DataFrame, name:str)->bool:
    return (name in df.columns) and (df[name].notna().any())

def evaluate_param_set(
    result_df: pd.DataFrame,
    subject_items: list,
    work_items: list,
    compound_replacers: list,
    stop_words: set,
    params: Dict[str, Any],
    gt_subject_major_col: str = "ì£¼ì œ ëŒ€ë¶„ë¥˜_GT",
    gt_subject_minor_col: str = "ì£¼ì œ ì¤‘ë¶„ë¥˜_GT",
    gt_work_major_col: str = "ì‘ì—…ìœ í˜• ëŒ€ë¶„ë¥˜_GT",
    gt_work_minor_col: str = "ì‘ì—…ìœ í˜• ì¤‘ë¶„ë¥˜_GT",
) -> Dict[str, Any]:
    """
    ì •ë‹µ ì—´ì´ ìˆìœ¼ë©´ ì •í™•ë„ ê¸°ë°˜ìœ¼ë¡œ, ì—†ìœ¼ë©´ í”„ë¡ì‹œ ìŠ¤ì½”ì–´(í‰ê·  best score)ë¡œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    # íŒŒë¼ë¯¸í„° ì–¸íŒ¨í‚¹
    cp = ClassifierParams(**params)

    preds_subject_major, preds_subject_minor = [], []
    preds_work_major, preds_work_minor = [], []
    subject_scores, work_scores = [], []

    for all_text, title_text in zip(result_df["all_text"], result_df["title_clean"]):
        mj, mn, sc = classify_weighted_general(
            all_text, subject_items, title_text=title_text,
            pos_max=cp.pos_max, pos_min=cp.pos_min,
            title_boost=cp.title_boost, title_overlap_ratio=cp.title_overlap_ratio,
            short_len=cp.short_len, long_len=cp.long_len,
            short_penalty=cp.short_penalty, long_penalty=cp.long_penalty,
            decay_steps=cp.decay_steps, length_bonus_scale=cp.length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        preds_subject_major.append(mj); preds_subject_minor.append(mn); subject_scores.append(sc)

        mj2, mn2, sc2 = classify_weighted_general(
            all_text, work_items, title_text=title_text,
            pos_max=cp.pos_max, pos_min=cp.pos_min,
            title_boost=cp.title_boost, title_overlap_ratio=cp.title_overlap_ratio,
            short_len=cp.short_len, long_len=cp.long_len,
            short_penalty=cp.short_penalty, long_penalty=cp.long_penalty,
            decay_steps=cp.decay_steps, length_bonus_scale=cp.length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        preds_work_major.append(mj2); preds_work_minor.append(mn2); work_scores.append(sc2)

    has_gt = any([
        _safe_col(result_df, gt_subject_major_col),
        _safe_col(result_df, gt_subject_minor_col),
        _safe_col(result_df, gt_work_major_col),
        _safe_col(result_df, gt_work_minor_col),
    ])

    metrics = {}
    if has_gt:
        # ì •í™•ë„ ê³„ì‚° (ì¡´ì¬í•˜ëŠ” GT ì—´ë§Œ)
        if _safe_col(result_df, gt_subject_major_col):
            acc_sub_major = np.mean((pd.Series(preds_subject_major) == result_df[gt_subject_major_col]).values)
            metrics["acc_subject_major"] = float(acc_sub_major)
        if _safe_col(result_df, gt_subject_minor_col):
            acc_sub_minor = np.mean((pd.Series(preds_subject_minor) == result_df[gt_subject_minor_col]).values)
            metrics["acc_subject_minor"] = float(acc_sub_minor)
        if _safe_col(result_df, gt_work_major_col):
            acc_work_major = np.mean((pd.Series(preds_work_major) == result_df[gt_work_major_col]).values)
            metrics["acc_work_major"] = float(acc_work_major)
        if _safe_col(result_df, gt_work_minor_col):
            acc_work_minor = np.mean((pd.Series(preds_work_minor) == result_df[gt_work_minor_col]).values)
            metrics["acc_work_minor"] = float(acc_work_minor)
        # ì¢…í•© ìŠ¤ì½”ì–´: ì¡´ì¬í•˜ëŠ” ì§€í‘œì˜ í‰ê· 
        if metrics:
            metrics["score"] = float(np.mean(list(metrics.values())))
        else:
            metrics["score"] = float(np.mean(subject_scores + work_scores))
    else:
        # í”„ë¡ì‹œ: í‰ê·  best score (ì£¼ì œ/ì‘ì—…ìœ í˜• í•©ì‚° í‰ê· )
        metrics["proxy_subject_mean"] = float(np.mean(subject_scores)) if subject_scores else 0.0
        metrics["proxy_work_mean"] = float(np.mean(work_scores)) if work_scores else 0.0
        metrics["score"] = float(np.mean([metrics["proxy_subject_mean"], metrics["proxy_work_mean"]]))

    metrics["params"] = params
    return metrics


def search_best_params(
    result_df: pd.DataFrame,
    subject_items: list,
    work_items: list,
    compound_replacers: list,
    stop_words: set,
    space: Dict[str, Tuple[float,float]] = SPACE,
    grid_density: int = 3,
    n_random: int = 30,
    seed: int = 42
) -> Dict[str, Any]:
    """
    ê°„ë‹¨í•œ í˜¼í•© íƒìƒ‰: ì†Œê·œëª¨ ê·¸ë¦¬ë“œ + ëœë¤ ìƒ˜í”Œ
    ë°˜í™˜: {"best": <metric dict>, "trials": [<metric dict>...]}
    """
    # ----- 1. ê·¸ë¦¬ë“œ ìƒ˜í”Œ ìƒì„± -----
    print("[INFO] ê·¸ë¦¬ë“œ ìƒ˜í”Œ ìƒì„± ì‹œì‘...")
    grid = make_grid(space, density=grid_density)
    keys = list(grid.keys())

    grid_trials = []
    rnd = random.Random(seed)
    n_grid_trials = 40  # ê·¸ë¦¬ë“œ ê¸°ë°˜ ì œí•œ ìƒ˜í”Œ ìˆ˜
    for _ in range(n_grid_trials):
        cand = {k: rnd.choice(grid[k]) for k in keys}
        grid_trials.append(cand)
    print(f"[INFO] ê·¸ë¦¬ë“œ ìƒ˜í”Œ ìˆ˜: {len(grid_trials)}")

    # ----- 2. ëœë¤ ìƒ˜í”Œ ìƒì„± -----
    print("[INFO] ëœë¤ ìƒ˜í”Œ ìƒì„± ì‹œì‘...")
    rand_trials = sample_random(space, n_random, seed=seed + 1)
    print(f"[INFO] ëœë¤ ìƒ˜í”Œ ìˆ˜: {len(rand_trials)}")

    # ----- 3. ì „ì²´ ìƒ˜í”Œ í•©ì¹˜ê¸° -----
    all_trials = grid_trials + rand_trials
    total_trials = len(all_trials)
    print(f"[INFO] ì´ íƒìƒ‰ ìƒ˜í”Œ ìˆ˜: {total_trials}\n")

    # ----- 4. íƒìƒ‰ ì‹¤í–‰ -----
    metrics = []
    best = None
    for idx, cand in enumerate(all_trials, start=1):
        print(f"[{idx}/{total_trials}] íƒìƒ‰ ì¤‘... íŒŒë¼ë¯¸í„°: {cand}")
        m = evaluate_param_set(result_df, subject_items, work_items, compound_replacers, stop_words, cand)
        metrics.append(m)

        # í˜„ì¬ ê²°ê³¼ ì¶œë ¥
        print(f"   â†’ í˜„ì¬ ì ìˆ˜: {m['score']:.4f}")

        # ìµœê³  ì ìˆ˜ ê°±ì‹  ì‹œ ì¶œë ¥
        if (best is None) or (m["score"] > best["score"]):
            best = m
            print(f"   â˜… ìµœê³  ì ìˆ˜ ê°±ì‹ ! Best Score: {best['score']:.4f}")

    print("\n[INFO] íƒìƒ‰ ì™„ë£Œ!")
    print(f"[INFO] ìµœì¢… Best Score: {best['score']:.4f}")
    print(f"[INFO] Best Params: {best['params']}")
    
    return {"best": best, "trials": metrics}





from pathlib import Path
xls = pd.ExcelFile(Path("nsquare_all.xlsx"))
result_df = pd.read_excel(xls, sheet_name="Result 1", header=0, dtype=str)
result_df.loc[:, "title_clean"] = result_df["title"].map(whitelist_text).fillna("")
result_df.loc[:, "all_text"] = (
    result_df["request_contents"].map(whitelist_text).fillna("") + " " +
    result_df["title_clean"] + " " +
    result_df["reply"].map(whitelist_text).fillna("")
).str.strip()

compound_words, stop_words = load_text_dict_from_db()
compound_replacers = build_compound_replacers(compound_words)

subject_tax = pd.read_excel(xls, sheet_name="ì£¼ì œ taxonomy", header=0, dtype=str)
work_tax    = pd.read_excel(xls, sheet_name="ì‘ì—…ìœ í˜• taxonomy", header=0, dtype=str)
subject_items = build_taxonomy(subject_tax, compound_replacers, stop_words)
work_items    = build_taxonomy(work_tax, compound_replacers, stop_words)

result = search_best_params(
    result_df=result_df,
    subject_items=subject_items,
    work_items=work_items,
    compound_replacers=compound_replacers,
    stop_words=stop_words,
    grid_density=3,
    n_random=30,
    seed=42
)

best = result["best"]
print("Best score:", best["score"])
print("Best params:", best["params"])

trials_df = pd.DataFrame(result["trials"]).sort_values("score", ascending=False)
# from caas_jupyter_tools import display_dataframe_to_user
# display_dataframe_to_user("Param Search Trials", trials_df)



trials_df



