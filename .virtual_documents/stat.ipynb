





import psycopg2
import pandas as pd

DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'

conn = psycopg2.connect(
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        host=DB_HOST
    )

cursor = conn.cursor()

query = "select a.stat_id, a.value, p.period_start, p.period_end  from stat_keyword_analysis a join stat_period p on a.period_id = p.period_id where a.dimension_id = 760" 
cursor.execute(query)
rows = cursor.fetchall()
colnames = [desc[0] for desc in cursor.description]

keyword_df = pd.DataFrame(rows, columns=colnames, dtype=str)

cursor.close()
conn.close()


keyword_df[:3]





import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import json
from scipy.stats import entropy

# âœ… font path: ì§ì ‘ ì§€ì • + FontPropertiesë¡œ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_prop = fm.FontProperties(fname=font_path)

# âœ… matplotlib ê¸°ë³¸ ì„¤ì •ì—ì„œ font.family ì„¤ì • ì•ˆ í•¨ (ê²½ê³  ë°©ì§€)
plt.rcParams['axes.unicode_minus'] = False

# ë‚ ì§œ íƒ€ìž… ë³€í™˜
keyword_df['period_start'] = pd.to_datetime(keyword_df['period_start'])

# ê¸°ê°„ + stat_id í•„í„°ë§
jan_df = keyword_df[
    (keyword_df['period_start'].dt.month == 1) & 
    (keyword_df['stat_id'] == '10')
]
may_df = keyword_df[
    (keyword_df['period_start'].dt.month == 5) & 
    (keyword_df['stat_id'] == '10')
]

# ì œì™¸í•  í‚¤ì›Œë“œ ëª©ë¡
exclude_keywords = {'ì ê²€', 'ì‹¤', 'í˜¸', 'ì•ž', 'ë¬¼', 'ì°¨', 'ë¹„ì‹¤', 'í›„', 'ì •'}

def extract_keywords(df):
    freq_dict = {}
    for value_str in df['value']:
        keyword_dict = json.loads(value_str)
        for k, v in keyword_dict.items():
            if k in exclude_keywords:
                continue
            freq_dict[k] = freq_dict.get(k, 0) + v
    return pd.DataFrame(freq_dict.items(), columns=['keyword', 'freq'])

early_df = extract_keywords(jan_df)
current_df = extract_keywords(may_df)

# ì‹œê°í™”
def plot_top_keywords(df, title):
    top_df = df.sort_values('freq', ascending=False).head(20)
    plt.figure(figsize=(10, 5))
    plt.barh(top_df['keyword'], top_df['freq'])
    plt.title(title, fontproperties=font_prop)
    plt.xlabel('ë¹ˆë„', fontproperties=font_prop)
    plt.ylabel('í‚¤ì›Œë“œ', fontproperties=font_prop)
    plt.yticks(fontproperties=font_prop)
    plt.xticks(fontproperties=font_prop)
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

plot_top_keywords(early_df, 'ìž‘ì—…ì§€ì‹œ 1ì›” í‚¤ì›Œë“œ Top 20')
plot_top_keywords(current_df, 'ìž‘ì—…ì§€ì‹œ 5ì›” í‚¤ì›Œë“œ Top 20')


# ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ 
early_total_freq = early_df['freq'].sum()
current_total_freq = current_df['freq'].sum()

print(f"1ì›” ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜: {early_total_freq}")
print(f"5ì›” ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜: {current_total_freq}")

# 5. Shannon entropy ê¸°ë°˜ ë‹¤ì–‘ì„±
def calc_entropy(df):
    prob = df['freq'] / df['freq'].sum()
    return entropy(prob)

print("1ì›” í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Entropy):", calc_entropy(early_df))
print("5ì›” í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Entropy):", calc_entropy(current_df))

# 6. Top 20 ì§‘ì¤‘ë„
def top_n_ratio(df, n=10):
    top_sum = df.sort_values('freq', ascending=False).head(n)['freq'].sum()
    return top_sum / df['freq'].sum()

print("1ì›” ìƒìœ„10 ì§‘ì¤‘ë„:", top_n_ratio(early_df))
print("5ì›” ìƒìœ„10 ì§‘ì¤‘ë„:", top_n_ratio(current_df))


def unique_keyword_count(df):
    return df['keyword'].nunique()

print("1ì›” ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜:", unique_keyword_count(early_df))
print("5ì›” ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜:", unique_keyword_count(current_df))


def keywords_above_median(df):
    median_val = df['freq'].median()
    return (df['freq'] >= median_val).sum()

print("1ì›” ì¤‘ìœ„ìˆ˜ ì´ìƒ í‚¤ì›Œë“œ ìˆ˜:", keywords_above_median(early_df))
print("5ì›” ì¤‘ìœ„ìˆ˜ ì´ìƒ í‚¤ì›Œë“œ ìˆ˜:", keywords_above_median(current_df))


get_ipython().run_line_magic("pip", " install wordcloud")


from wordcloud import WordCloud

# ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜
def show_wordcloud_above_median(df, title):
    median_val = df['freq'].median()
    filtered_df = df[df['freq'] >= median_val]
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    word_freq = dict(zip(filtered_df['keyword'], filtered_df['freq']))
    wordcloud = WordCloud(
        font_path=font_path,
        background_color='white',
        width=800,
        height=400
    ).generate_from_frequencies(word_freq)

    # ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontproperties=font_prop, fontsize=18)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

show_wordcloud_above_median(early_df, '1ì›” ì¤‘ìœ„ìˆ˜ ì´ìƒ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ')
show_wordcloud_above_median(current_df, '5ì›” ì¤‘ìœ„ìˆ˜ ì´ìƒ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ')


def show_wordcloud_top20_above_median_fixed_scale(df, title, global_max):
    median_val = df['freq'].median()
    
    # ì¤‘ìœ„ìˆ˜ ì´ìƒ í•„í„°ë§ í›„ ìƒìœ„ 20ê°œ ì„ íƒ
    filtered_df = df[df['freq'] >= median_val].sort_values(by='freq', ascending=False)
    
    # ì •ê·œí™” (0~1 ë¹„ìœ¨ë¡œ ì¡°ì •)
    filtered_df['normalized_freq'] = filtered_df['freq'] / global_max
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    word_freq = dict(zip(filtered_df['keyword'], filtered_df['normalized_freq']))
    wordcloud = WordCloud(
        font_path=font_path,
        background_color='white',
        width=800,
        height=400,
        max_font_size=100  # ë¹„ìœ¨ ìœ ì§€
    ).generate_from_frequencies(word_freq)

    # ì‹œê°í™”
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, fontproperties=font_prop, fontsize=18)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

# ë‘ DataFrameì—ì„œ freq ìµœëŒ€ê°’ ê³„ì‚°
max_freq = max(early_df['freq'].max(), current_df['freq'].max())

# ì •ê·œí™”ëœ ì›Œë“œí´ë¼ìš°ë“œ ì¶œë ¥
show_wordcloud_top20_above_median_fixed_scale(early_df, '1ì›” ì›Œë“œí´ë¼ìš°ë“œ', max_freq)
show_wordcloud_top20_above_median_fixed_scale(current_df, '5ì›” ì›Œë“œí´ë¼ìš°ë“œ', max_freq)


def top30_keywords_above_median_with_ratio(df):
    total_freq = df['freq'].sum()  # ì „ì²´ ë¹ˆë„ í•©
    median_val = df['freq'].median()
    
    # ì¤‘ìœ„ìˆ˜ ì´ìƒ í•„í„°ë§ + ìƒìœ„ 30ê°œ
    filtered_df = df[df['freq'] >= median_val].sort_values(by='freq', ascending=False).head(30).copy()
    
    # ë¹„ìœ¨ ê³„ì‚° (í¼ì„¼íŠ¸ ë¬¸ìžì—´ë¡œ í‘œì‹œ)
    filtered_df['ratio'] = ((filtered_df['freq'] / total_freq) * 100).round(2).astype(str) + '%'
    
    return filtered_df.reset_index(drop=True)

print("1ì›” í‚¤ì›Œë“œ Top 30")
display(top30_keywords_above_median_with_ratio(early_df))

print("\n5ì›” í‚¤ì›Œë“œ Top 30")
display(top30_keywords_above_median_with_ratio(current_df))


# ê²°ê³¼ ìƒì„±
early_top30_df = top30_keywords_above_median_with_ratio(early_df)
may_top30_df = top30_keywords_above_median_with_ratio(current_df)

# ì›” êµ¬ë¶„ ì»¬ëŸ¼ ì¶”ê°€
early_top30_df['period'] = '1ì›”'
may_top30_df['period'] = '5ì›”'

# ë‘ ê°œë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
combined_df = pd.concat([early_top30_df, may_top30_df], ignore_index=True)

# ì—´ ìˆœì„œ ì¡°ì • (ì„ íƒ)
combined_df = combined_df[['period', 'keyword', 'freq', 'ratio']]

# CSVë¡œ ì €ìž¥
combined_df.to_csv('ìž‘ì—…ì§€ì‹œ_í‚¤ì›Œë“œ_TOP30_ë¹„êµ.csv', index=False, encoding='utf-8-sig')








import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import json
from scipy.stats import entropy

# âœ… font path: ì§ì ‘ ì§€ì • + FontPropertiesë¡œ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_prop = fm.FontProperties(fname=font_path)

# âœ… matplotlib ê¸°ë³¸ ì„¤ì •ì—ì„œ font.family ì„¤ì • ì•ˆ í•¨ (ê²½ê³  ë°©ì§€)
plt.rcParams['axes.unicode_minus'] = False

# ë‚ ì§œ íƒ€ìž… ë³€í™˜
keyword_df['period_start'] = pd.to_datetime(keyword_df['period_start'])

# ê¸°ê°„ + stat_id í•„í„°ë§
jan_df = keyword_df[
    (keyword_df['period_start'].dt.month == 1) & 
    (keyword_df['stat_id'] == '9')
]
may_df = keyword_df[
    (keyword_df['period_start'].dt.month == 5) & 
    (keyword_df['stat_id'] == '9')
]

# str â†’ dict ë³€í™˜ + ëˆ„ì 
def extract_keywords(df):
    freq_dict = {}
    for value_str in df['value']:
        keyword_dict = json.loads(value_str)
        for k, v in keyword_dict.items():
            freq_dict[k] = freq_dict.get(k, 0) + v
    return pd.DataFrame(freq_dict.items(), columns=['keyword', 'freq'])

early_df = extract_keywords(jan_df)
current_df = extract_keywords(may_df)

# ì‹œê°í™”
def plot_top_keywords(df, title):
    top_df = df.sort_values('freq', ascending=False).head(20)
    plt.figure(figsize=(10, 5))
    plt.barh(top_df['keyword'], top_df['freq'])
    plt.title(title, fontproperties=font_prop)
    plt.xlabel('ë¹ˆë„', fontproperties=font_prop)
    plt.ylabel('í‚¤ì›Œë“œ', fontproperties=font_prop)
    plt.yticks(fontproperties=font_prop)
    plt.xticks(fontproperties=font_prop)
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

plot_top_keywords(early_df, 'VOC 1ì›” í‚¤ì›Œë“œ Top 20')
plot_top_keywords(current_df, 'VOC 5ì›” í‚¤ì›Œë“œ Top 20')


# ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ 
early_total_freq = early_df['freq'].sum()
current_total_freq = current_df['freq'].sum()

print(f"1ì›” ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜: {early_total_freq}")
print(f"5ì›” ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜: {current_total_freq}")

# 5. Shannon entropy ê¸°ë°˜ ë‹¤ì–‘ì„±
def calc_entropy(df):
    prob = df['freq'] / df['freq'].sum()
    return entropy(prob)

print("1ì›” í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Entropy):", calc_entropy(early_df))
print("5ì›” í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Entropy):", calc_entropy(current_df))

# 6. Top 20 ì§‘ì¤‘ë„
def top_n_ratio(df, n=10):
    top_sum = df.sort_values('freq', ascending=False).head(n)['freq'].sum()
    return top_sum / df['freq'].sum()

print("1ì›” ìƒìœ„10 ì§‘ì¤‘ë„:", top_n_ratio(early_df))
print("5ì›” ìƒìœ„10 ì§‘ì¤‘ë„:", top_n_ratio(current_df))








import os
import psycopg2
import pandas as pd
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from zoneinfo import ZoneInfo
from collections import Counter
from konlpy.tag import Okt
import json
import boto3
import zipfile
import gc

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'

STAT_ID = 10  # stat_name = work_order_keyword(ìž‘ì—…ì§€ì‹œ í‚¤ì›Œë“œ ê±´ìˆ˜)

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì „ì—­ ë³€ìˆ˜
compound_words = {
    'ë‚¨ìží™”ìž¥ì‹¤': 'ë‚¨ìží™”ìž¥ì‹¤', 'ì—¬ìží™”ìž¥ì‹¤': 'ì—¬ìží™”ìž¥ì‹¤',
    'ë‚¨ìž í™”ìž¥ì‹¤': 'ë‚¨ìží™”ìž¥ì‹¤', 'ì—¬ìž í™”ìž¥ì‹¤': 'ì—¬ìží™”ìž¥ì‹¤',
    'ìˆ˜ë³€ì „': 'ìˆ˜ë³€ì „', 'ëƒ‰ë‚œë°©ê¸°': 'ëƒ‰ë‚œë°©ê¸°', 'ë‚œë°©ê¸°': 'ë‚œë°©ê¸°',
    'ì—°ìˆ˜ê¸°': 'ì—°ìˆ˜ê¸°', 'ìŒìˆ˜ê¸°': 'ìŒìˆ˜ê¸°', 'ì˜¨ìˆ˜ê¸°': 'ì˜¨ìˆ˜ê¸°', 'ëƒ‰ë°©ê¸°': 'ëƒ‰ë°©ê¸°',
    'ì¢Œë³€ê¸°': 'ì¢Œë³€ê¸°', 'ê³µì¡°ê¸°':'ê³µì¡°ê¸°', 'ë¶„ì „ë°˜':'ë¶„ì „ë°˜', 'ì •í™”ì¡°':'ì •í™”ì¡°'
}
stop_words = {'ì˜', 'ê³¼', 'ì™€', 'ì—', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'í™•ì¸', 'ìž‘ì—…', 'ìƒíƒœ', 'ìš”ì²­', 'ìœ¼ë¡œ', 'ê²ƒ', 'ë°', 'í˜¸ê¸°', 'ì¸µ', 'ë“±', 'ë‚´'}

okt = Okt()


# Load Data 

def get_base_dates():
    # UTC -> KST ë³€í™˜
    utc_now = datetime.utcnow()
    today = utc_now.replace(tzinfo=ZoneInfo("UTC")).astimezone(ZoneInfo("Asia/Seoul"))
    # first_day_of_last_month = (today.replace(day=1) - timedelta(days=1)).replace(day=1, hour=0, minute=0, second=0)
    first_day_of_last_month = datetime.strptime('2025-01-01 00:00:00', '%Y-%m-%d %H:%M:%S') # backfill
    last_day_of_last_month = first_day_of_last_month + relativedelta(months=1) - timedelta(seconds=1)
    return first_day_of_last_month.strftime('%Y-%m-%d %H:%M:%S'), last_day_of_last_month.strftime('%Y-%m-%d %H:%M:%S')

def load_data(sql, start_date=None, end_date=None):
    try:
        # 1. ë‚ ì§œ ê¸°ë³¸ê°’ ì„¤ì • (ìµœê·¼ 6ê°œì›”)
        if start_date is None or end_date is None:
            today = datetime.now(ZoneInfo("Asia/Seoul"))
            start_date = (today - relativedelta(months=6)).replace(day=1, hour=0, minute=0, second=0)
            end_date = today.replace(hour=23, minute=59, second=59)
        else:
            start_date = pd.to_datetime(start_date).replace(tzinfo=ZoneInfo("Asia/Seoul"))
            end_date = pd.to_datetime(end_date).replace(tzinfo=ZoneInfo("Asia/Seoul"))

        # 2. í¬ë§· ë¬¸ìžì—´ë¡œ ë³€í™˜
        start_date_str = start_date.strftime('%Y-%m-%d %H:%M:%S')
        end_date_str = end_date.strftime('%Y-%m-%d %H:%M:%S')
        print(f"ë°ì´í„° ì¡°íšŒ ê¸°ê°„: {start_date_str} ~ {end_date_str}")

        # 3. DB ì—°ê²° ë° ì¿¼ë¦¬ ì‹¤í–‰
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()
        cursor.execute(sql, (start_date_str, end_date_str))
        data = cursor.fetchall()
        column_names = [desc[0] for desc in cursor.description]

        # 4. DataFrame ìƒì„± ë° ì „ì²˜ë¦¬ ì¤€ë¹„
        df = pd.DataFrame(data, columns=column_names, dtype=str)
        df['write_date'] = pd.to_datetime(df['write_date'])
        df['year_month'] = df['write_date'].dt.to_period('M').astype(str)
        df['building_id'] = df['building_id'].astype(str)

        return df

    except Exception as e:
        print(f"Error loading data: {e}")
        return None
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'connection' in locals():
            connection.close()


load_sql = """
            SELECT write_date, building_id, name as title, description, type 
            FROM work_order
            WHERE write_date BETWEEN %s AND %s;
        """
# save_sql = """
#     INSERT INTO stat_keyword_analysis (stat_id, period_id, dimension_id, value)
#     SELECT %s, sp.period_id, sd.dimension_id, %s
#     FROM stat_period sp
#     JOIN stat_dimension sd ON sd.building_id = %s
#     WHERE sp.period_start = %s AND sp.period_end = %s;
# """
# ì˜ˆ: 2025ë…„ 1ì›” ~ 5ì›” ì „ì²´ ì¡°íšŒ
df = load_data(load_sql, start_date='2025-01-01', end_date='2025-05-31')


df[:3]


# Preprocessing 

import pandas as pd
import re
from bs4 import BeautifulSoup

def clean_text(text: str) -> str:
    """
    ë¬¸ìž¥ ë‹¨ìœ„ í‚¤ì›Œë“œ ë¶„ì„ì„ ìœ„í•´ HTML ì œê±° í›„
    íŠ¹ìˆ˜ë¬¸ìžë§Œ ì œê±°í•˜ê³ , ê³µë°±ì€ ìœ ì§€í•¨.
    """
    if not isinstance(text, str):
        return ""
    
    # ì—°ì†ëœ ê³µë°±ì€ í•˜ë‚˜ë¡œ ì¤„ìž„
    text = re.sub(r'\s+', ' ', text)
    
    # í•œê¸€, ìˆ«ìž, ì˜ë¬¸, ê³µë°±ë§Œ ë‚¨ê¹€ (ê³µë°±ì€ ìœ ì§€)
    text = re.sub(r'[^\w\sã„±-ã…Žã…-ã…£ê°€-íž£]', ' ', text)

    # ì—¬ëŸ¬ ê³µë°± â†’ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

def preprocess_work_order(row):
    """
    work_order í…Œì´ë¸”ì˜ ê° rowì—ì„œ nameê³¼ descriptionì„ ì „ì²˜ë¦¬í•˜ì—¬ ê²°í•©
    HTMLì´ í¬í•¨ëœ descriptionì€ ì œê±° í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    """
    name = row.get('title', '')
    description = row.get('description', '')

    # nameì€ ë‹¨ìˆœ ì „ì²˜ë¦¬
    name_cleaned = clean_text(name)

    # descriptionì€ HTML íƒœê·¸ ì œê±° + ì „ì²˜ë¦¬
    if isinstance(description, str):
        soup = BeautifulSoup(description, 'html.parser')
        description_text = soup.get_text(separator=' ')
        description_cleaned = clean_text(description_text)
    else:
        description_cleaned = ""

    # name + description í•©ì¹˜ê¸°
    return f"{name_cleaned} {description_cleaned}".strip()


# ì „ì²˜ë¦¬ ì ìš©
df['text_for_analysis'] = df.apply(preprocess_work_order, axis=1)


df[['title', 'description', 'text_for_analysis']][:3]


def keyword_analysis(title, stop_words):
    try:
        if pd.isna(title):
            return {}

        extracted_compounds = []
        for word in compound_words.keys():
            if word in title:
                extracted_compounds.append(compound_words[word])
                title = title.replace(word, '')

        nouns = okt.nouns(title)
     
        all_nouns = extracted_compounds + [noun for noun in nouns if noun not in stop_words]
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(all_nouns)
        
        return dict(word_counts)
    except Exception as e:
        print(f"Error in keyword_analysis: {e}")
        return {}

def aggregate_and_filter(grouped):
    def filter_frequency(frequency_dict):
        return {k: v for k , v in frequency_dict.items() if v > 1}
    
    total_frequency = Counter()
    for freq_dict in grouped['frequency']:
        if not isinstance(freq_dict, dict):
            raise ValueError(f"Invalid frequency format: {freq_dict}")   

        filtered = filter_frequency(freq_dict)
        total_frequency.update(filtered)
    
    return dict(total_frequency)


def run_monthly_keyword_analysis(df, stop_words):
    """
    ì›”ë³„ë¡œ textë¥¼ ê·¸ë£¹í™”í•˜ê³  keyword ë¶„ì„ì„ ìˆ˜í–‰
    """
    grouped_results = []

    for month, group_df in df.groupby('year_month'):
        combined_text = ' '.join(group_df['text_for_analysis'].dropna())
        freq = keyword_analysis(combined_text, stop_words)
        grouped_results.append({
            'year_month': month,
            'frequency': freq
        })

    return pd.DataFrame(grouped_results)

        
grouped = run_monthly_keyword_analysis(df, stop_words)
total_frequency_json = aggregate_and_filter(grouped)


import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from scipy.stats import entropy

# âœ… font ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_prop = fm.FontProperties(fname=font_path)
plt.rcParams['axes.unicode_minus'] = False

# âœ… year_month ë¬¸ìžì—´ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
early_dict = grouped.loc[grouped['year_month'] == '2025-01', 'frequency'].values
current_dict = grouped.loc[grouped['year_month'] == '2025-05', 'frequency'].values

# âœ… dict â†’ DataFrame ë³€í™˜ í•¨ìˆ˜
def dict_to_freq_df(freq_dict):
    if len(freq_dict) == 0:
        return pd.DataFrame(columns=['keyword', 'freq'])
    return pd.DataFrame(freq_dict[0].items(), columns=['keyword', 'freq'])

early_df = dict_to_freq_df(early_dict)
current_df = dict_to_freq_df(current_dict)

# âœ… ì‹œê°í™” í•¨ìˆ˜
def plot_top_keywords(df, title):
    top_df = df.sort_values('freq', ascending=False).head(20)
    plt.figure(figsize=(10, 5))
    plt.barh(top_df['keyword'], top_df['freq'])
    plt.title(title, fontproperties=font_prop)
    plt.xlabel('ë¹ˆë„', fontproperties=font_prop)
    plt.ylabel('í‚¤ì›Œë“œ', fontproperties=font_prop)
    plt.yticks(fontproperties=font_prop)
    plt.xticks(fontproperties=font_prop)
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

# âœ… ì‹œê°í™” ì‹¤í–‰
plot_top_keywords(early_df, 'ìž‘ì—…ì§€ì‹œ 1ì›” í‚¤ì›Œë“œ Top 20')
plot_top_keywords(current_df, 'ìž‘ì—…ì§€ì‹œ 5ì›” í‚¤ì›Œë“œ Top 20')

# âœ… ì „ì²´ í‚¤ì›Œë“œ ìˆ˜
early_total_freq = early_df['freq'].sum()
current_total_freq = current_df['freq'].sum()

print(f"1ì›” ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜: {early_total_freq}")
print(f"5ì›” ì „ì²´ í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜: {current_total_freq}")

# âœ… Shannon entropy ë‹¤ì–‘ì„±
def calc_entropy(df):
    prob = df['freq'] / df['freq'].sum()
    return entropy(prob)

print("1ì›” í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Entropy):", calc_entropy(early_df))
print("5ì›” í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Entropy):", calc_entropy(current_df))

# âœ… ìƒìœ„ 10 ì§‘ì¤‘ë„
def top_n_ratio(df, n=10):
    top_sum = df.sort_values('freq', ascending=False).head(n)['freq'].sum()
    return top_sum / df['freq'].sum()

print("1ì›” ìƒìœ„10 ì§‘ì¤‘ë„:", top_n_ratio(early_df))
print("5ì›” ìƒìœ„10 ì§‘ì¤‘ë„:", top_n_ratio(current_df))



def unique_keyword_count(df):
    return df['keyword'].nunique()

print("1ì›” ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜:", unique_keyword_count(early_df))
print("5ì›” ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜:", unique_keyword_count(current_df))


def keywords_above_median(df):
    median_val = df['freq'].median()
    return (df['freq'] >= median_val).sum()

print("1ì›” ì¤‘ìœ„ìˆ˜ ì´ìƒ í‚¤ì›Œë“œ ìˆ˜:", keywords_above_median(early_df))
print("5ì›” ì¤‘ìœ„ìˆ˜ ì´ìƒ í‚¤ì›Œë“œ ìˆ˜:", keywords_above_median(current_df))


def get_top_keywords(df, n=20):
    return set(df.sort_values('freq', ascending=False).head(n)['keyword'])

top10_jan = get_top_keywords(early_df, 20)
top10_may = get_top_keywords(current_df, 20)

overlap = len(top10_jan & top10_may)
print("1ì›”-5ì›” Top20 í‚¤ì›Œë“œ ì¤‘ë³µ ìˆ˜:", overlap)
print("ë¹„ì¤‘:", overlap / 20)


font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_prop = fm.FontProperties(fname=font_path)
plt.rcParams['axes.unicode_minus'] = False

def plot_freq_distribution(df, title):
    sorted_freq = df['freq'].sort_values(ascending=False).reset_index(drop=True)
    plt.figure(figsize=(8, 4))
    plt.plot(sorted_freq)
    plt.title(f"{title} - í‚¤ì›Œë“œ ë¹ˆë„ ë¶„í¬", fontproperties=font_prop)
    plt.xlabel("í‚¤ì›Œë“œ ìˆœìœ„", fontproperties=font_prop)
    plt.ylabel("ë¹ˆë„ìˆ˜", fontproperties=font_prop)
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_freq_distribution(early_df, "1ì›”")
plot_freq_distribution(current_df, "5ì›”")


import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from scipy.stats import entropy

# âœ… í°íŠ¸ ì„¤ì •
font_path = '/usr/share/fonts/nanum/NanumGothic-Regular.ttf'
font_prop = fm.FontProperties(fname=font_path)
plt.rcParams['axes.unicode_minus'] = False


# âœ… dict â†’ freq DataFrame ë³€í™˜
def dict_to_freq_df(freq_dict):
    if not freq_dict:
        return pd.DataFrame(columns=['keyword', 'freq'])
    return pd.DataFrame(freq_dict.items(), columns=['keyword', 'freq'])

# âœ… Shannon entropy ê³„ì‚°
def calc_entropy(df):
    if df.empty or df['freq'].sum() == 0:
        return 0
    prob = df['freq'] / df['freq'].sum()
    return entropy(prob)

# âœ… Top-N ì§‘ì¤‘ë„ ê³„ì‚°
def top_n_ratio(df, n=10):
    if df.empty or df['freq'].sum() == 0:
        return 0
    top_sum = df.sort_values('freq', ascending=False).head(n)['freq'].sum()
    return top_sum / df['freq'].sum()

# âœ… grouped â†’ ì›”ë³„ entropy / ì§‘ì¤‘ë„ ì¶”ì¶œ
entropy_results = []
concentration_results = []

for _, row in grouped.iterrows():
    ym = row['year_month']
    freq_df = dict_to_freq_df(row['frequency'])
    
    entropy_val = calc_entropy(freq_df)
    top10_ratio = top_n_ratio(freq_df, n=10)
    
    entropy_results.append((ym, entropy_val))
    concentration_results.append((ym, top10_ratio))

# âœ… DataFrameìœ¼ë¡œ ë³€í™˜
entropy_df = pd.DataFrame(entropy_results, columns=['year_month', 'entropy']).sort_values('year_month')
concentration_df = pd.DataFrame(concentration_results, columns=['year_month', 'top10_ratio']).sort_values('year_month')

# âœ… ì‹œê°í™” 1: Entropy ë³€í™” ì¶”ì´
plt.figure(figsize=(10, 5))
plt.plot(entropy_df['year_month'], entropy_df['entropy'], marker='o', label='Entropy')
plt.title('ì›”ë³„ í‚¤ì›Œë“œ ë‹¤ì–‘ì„± (Shannon Entropy)', fontproperties=font_prop)
plt.xlabel('ì›”', fontproperties=font_prop)
plt.ylabel('Entropy', fontproperties=font_prop)
plt.xticks(rotation=45, fontproperties=font_prop)
plt.yticks(fontproperties=font_prop)
plt.grid(True)
plt.tight_layout()
plt.show()

# âœ… ì‹œê°í™” 2: Top-N ì§‘ì¤‘ë„ ë³€í™” ì¶”ì´
plt.figure(figsize=(10, 5))
plt.plot(concentration_df['year_month'], concentration_df['top10_ratio'], marker='s', color='orange', label='Top 10 ì§‘ì¤‘ë„')
plt.title('ì›”ë³„ Top 10 í‚¤ì›Œë“œ ì§‘ì¤‘ë„ ë³€í™”', fontproperties=font_prop)
plt.xlabel('ì›”', fontproperties=font_prop)
plt.ylabel('Top 10 ì§‘ì¤‘ë„', fontproperties=font_prop)
plt.xticks(rotation=45, fontproperties=font_prop)
plt.yticks(fontproperties=font_prop)
plt.grid(True)
plt.tight_layout()
plt.show()



DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'

def load_text_dict_from_db():
    try:
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()

        cursor.execute("""
            SELECT word_key, word_value 
            FROM stat_text_dict 
            WHERE type = 'compound' AND is_active = TRUE
        """)
        compound_result = cursor.fetchall()
        compound_words = {row[0]: row[1] for row in compound_result}

        cursor.execute("""
            SELECT word_key 
            FROM stat_text_dict 
            WHERE type = 'stop' AND is_active = TRUE
        """)
        stop_result = cursor.fetchall()
        stop_words = {row[0] for row in stop_result}

        cursor.close()
        conn.close()

        return compound_words, stop_words
    except Exception as e:
        print(f"Error in load_text_dict_from_db: {e}")
        return {}, set()   
    finally:
        if cursor:
            cursor.close()
        if connection:
            connection.close()

compound_words, stop_words = load_text_dict_from_db()


compound_words


# import pandas as pd
# import psycopg2

# DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
# DB_NAME = 'csp_prd'
# DB_USER = 'csp_admin'
# DB_PASSWORD = 'hdci12#$'

# # CSV íŒŒì¼ ë¡œë“œ
# csv_path = 'compound_words.csv'
# df = pd.read_csv(csv_path, encoding='cp949')

# # DB ì—°ê²° ë° INSERT ì‹¤í–‰ í•¨ìˆ˜
# def insert_compound_words_from_csv(df):
#     try:
#         conn = psycopg2.connect(
#             dbname=DB_NAME,
#             user=DB_USER,
#             password=DB_PASSWORD,
#             host=DB_HOST
#         )
#         cursor = conn.cursor()

#         insert_query = """
#             INSERT INTO public.stat_text_dict (type, word_key, word_value, description)
#             VALUES (%s, %s, %s, %s)
#             ON CONFLICT (type, word_key) DO NOTHING;
#         """

#         for _, row in df.iterrows():
#             cursor.execute(insert_query, ('compound', row['word_key'], row['word_value'], 'ì„¤ë¹„'))

#         conn.commit()
#         cursor.close()
#         conn.close()
#         print("ë°ì´í„° ì‚½ìž… ì™„ë£Œ")

#     except Exception as e:
#         print(f"ë°ì´í„° ì‚½ìž… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# # ì‹¤í–‰
# insert_compound_words_from_csv(df)


import os
import sys
import subprocess
from bs4 import BeautifulSoup

# === ì„¤ì • ===
notebook_input = "keyword_analysis_nsquare.ipynb"
temp_html_path = "temp.html"
html_output_path = "250701_keyword_analysis_nsquare.html"

# === Step 1: nbconvert ì‹¤í–‰
print("nbconvert ì‹¤í–‰ ì¤‘...")
result = subprocess.run(
    [sys.executable, "-m", "nbconvert", "--to", "html", "--no-input",
     notebook_input, "--output", os.path.splitext(temp_html_path)[0]],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True
)

# === Step 2: ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸
if result.returncode != 0 or not os.path.exists(temp_html_path):
    print("nbconvert ì‹¤íŒ¨ ë¡œê·¸:")
    print(result.stderr)
    raise FileNotFoundError(
        f"nbconvertê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ {temp_html_path} íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        f"ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì§ì ‘ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•´ë³´ì„¸ìš”:\n"
        f"{sys.executable} -m nbconvert --to html --no-input \"{notebook_input}\" --output=\"{os.path.splitext(temp_html_path)[0]}\""
    )

print("nbconvert ë³€í™˜ ì„±ê³µ â†’ HTML í›„ì²˜ë¦¬ ì§„í–‰")

# === Step 3: HTML íŒŒì¼ ì—´ê¸°
with open(temp_html_path, "r", encoding="utf-8") as file:
    soup = BeautifulSoup(file, "html.parser")

# === Step 4: 'Output hidden' í…ìŠ¤íŠ¸ ì œê±°
for output_hidden in soup.find_all(string=lambda text: isinstance(text, str) and "Output hidden;" in text):
    output_hidden.extract()

# === Step 5: ëª©ì°¨(TOC) ìƒì„±
toc_container = soup.new_tag("div", id="toc")
toc_title_tag = soup.new_tag("strong")
toc_title_tag.string = "ðŸ“– ëª©ì°¨"
toc_container.append(toc_title_tag)

toc_list = soup.new_tag("ul")
toc_container.append(toc_list)

header_tags = soup.find_all(["h1", "h2", "h3"])

current_h1 = None
current_h2 = None

for idx, header in enumerate(header_tags):
    if not header.has_attr("id"):
        header['id'] = f"toc_{idx}"

    link = soup.new_tag("a", href=f"#{header['id']}")
    link.string = header.get_text()

    list_item = soup.new_tag("li")
    list_item.append(link)

    if header.name == "h1":
        toc_list.append(list_item)
        current_h1 = list_item
        current_h2 = None
    elif header.name == "h2":
        if current_h1 is None:
            toc_list.append(list_item)
        else:
            if not current_h1.find('ul'):
                current_h1.append(soup.new_tag("ul"))
            current_h1.find('ul').append(list_item)
        current_h2 = list_item
    elif header.name == "h3":
        if current_h2 is not None:
            if not current_h2.find('ul'):
                current_h2.append(soup.new_tag("ul"))
            current_h2.find('ul').append(list_item)
        elif current_h1 is not None:
            if not current_h1.find('ul'):
                current_h1.append(soup.new_tag("ul"))
            current_h1.find('ul').append(list_item)
        else:
            toc_list.append(list_item)

# === Step 6: TOC ìŠ¤íƒ€ì¼ ì‚½ìž…
style_tag = soup.new_tag("style")
style_tag.string = """
#toc {
    position: fixed;
    top: 20px;
    right: 20px;
    width: 250px;
    background: #f9f9f9;
    border: 1px solid #ddd;
    padding: 10px;
    max-height: 90vh;
    overflow-y: auto;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    z-index: 1000;
    font-family: sans-serif;
    font-size: 14px;
}
#toc ul {
    list-style: none;
    padding-left: 0;
}
#toc li {
    margin: 5px 0;
}
#toc li ul {
    padding-left: 15px;
}
#toc li ul li ul {
    padding-left: 15px;
}
#toc a {
    text-decoration: none;
    color: #333;
}
#toc a:hover {
    text-decoration: underline;
}
"""
soup.head.append(style_tag)

if soup.body:
    soup.body.insert(0, toc_container)

# === Step 7: ìµœì¢… HTML ì €ìž¥
with open(html_output_path, "w", encoding="utf-8") as f:
    f.write(str(soup))

print(f"HTML íŒŒì¼ ì €ìž¥ ì™„ë£Œ: {html_output_path}")

# === Step 8: ìž„ì‹œ íŒŒì¼ ì‚­ì œ
os.remove(temp_html_path)



import os
import re
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values

# -----------------------------
# ì„¤ì •
# -----------------------------
XLSX_PATH = os.getenv("TAXONOMY_XLSX_PATH", "Result_1_retagged.xlsx")
VERSION = os.getenv("TAXONOMY_VERSION", "2026-01")  
DEFAULT_PRIORITY = int(os.getenv("TAXONOMY_PRIORITY", "100"))
DEFAULT_ACTIVE = os.getenv("TAXONOMY_IS_ACTIVE", "true").lower() in ("1", "true", "yes", "y")

DB_HOST = os.getenv("DB_HOST", "hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "csp_prd")
DB_USER = os.getenv("DB_USER", "csp_admin")
DB_PASSWORD = os.getenv("DB_PASSWORD", "hdci12#$")

SUBJECT_SHEET = "ì£¼ì œ taxonomy"
WORK_SHEET = "ìž‘ì—…ìœ í˜• taxonomy"
REQUIRED_COLS = ["ëŒ€ë¶„ë¥˜", "ì¤‘ë¶„ë¥˜", "ì‚¬ë¡€/í‚¤ì›Œë“œ"]

if not (DB_HOST and DB_NAME and DB_USER and DB_PASSWORD):
    raise RuntimeError("DB í™˜ê²½ë³€ìˆ˜ê°€ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤. DB_HOST/DB_NAME/DB_USER/DB_PASSWORDë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.")


def normalize_text(v: object) -> str:
    if v is None:
        return ""
    s = str(v).strip()
    s = re.sub(r"\s+", " ", s)
    return s


def load_sheet(xlsx_path: str, sheet_name: str) -> pd.DataFrame:
    df = pd.read_excel(xlsx_path, sheet_name=sheet_name)

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"ì‹œíŠ¸ '{sheet_name}'ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing}. í˜„ìž¬ ì»¬ëŸ¼: {list(df.columns)}")

    df = df[REQUIRED_COLS].copy()

    for c in REQUIRED_COLS:
        df[c] = df[c].map(normalize_text)

    df = df[(df["ëŒ€ë¶„ë¥˜"] != "") & (df["ì¤‘ë¶„ë¥˜"] != "") & (df["ì‚¬ë¡€/í‚¤ì›Œë“œ"] != "")]
    df = df.drop_duplicates()

    return df


def build_rows(df: pd.DataFrame, taxonomy_type: str) -> list[tuple]:
    rows = []
    for _, r in df.iterrows():
        rows.append((
            taxonomy_type,        # taxonomy_type
            r["ëŒ€ë¶„ë¥˜"],          # major
            r["ì¤‘ë¶„ë¥˜"],          # minor
            r["ì‚¬ë¡€/í‚¤ì›Œë“œ"],     # keywords
            DEFAULT_PRIORITY,     # priority
            DEFAULT_ACTIVE,       # is_active
            VERSION               # version (NULL ì›í•˜ë©´ None ë„£ì–´ë„ ë¨)
        ))
    return rows


def upsert_voc_taxonomy(rows: list[tuple]) -> int:
    if not rows:
        return 0

    conn = psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD
    )
    conn.autocommit = False
    try:
        with conn.cursor() as cur:
            sql = """
                INSERT INTO voc_taxonomy
                (taxonomy_type, major, minor, keywords, priority, is_active, version)
                VALUES %s
                ON CONFLICT (taxonomy_type, major, minor, keywords, COALESCE(version, ''))
                DO UPDATE SET
                    priority = EXCLUDED.priority,
                    is_active = EXCLUDED.is_active,
                    updated_at = now()
            """
            execute_values(cur, sql, rows, page_size=1000)

        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()

    return len(rows)


# -----------------------------
# ì‹¤í–‰
# -----------------------------
subj_df = load_sheet(XLSX_PATH, SUBJECT_SHEET)
work_df = load_sheet(XLSX_PATH, WORK_SHEET)

subj_rows = build_rows(subj_df, "SUBJECT")
work_rows = build_rows(work_df, "WORK")

total = 0
total += upsert_voc_taxonomy(subj_rows)
total += upsert_voc_taxonomy(work_rows)

print(f"ì™„ë£Œ: {total} rows upserted into voc_taxonomy")
print(f"- SUBJECT: {len(subj_rows)}")
print(f"- WORK   : {len(work_rows)}")
print(f"- VERSION: {VERSION}")




