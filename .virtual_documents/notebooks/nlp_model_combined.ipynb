import os

def load_env_file(path: str) -> None:
    if not os.path.exists(path):
        raise FileNotFoundError(f".env 파일이 없습니다: {path}")

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#") or "=" not in line:
                continue
            k, v = line.split("=", 1)
            k = k.strip()
            v = v.strip().strip('"').strip("'")
            # 이미 환경변수가 있으면 덮어쓰지 않음(원하시면 setdefault -> assignment로 변경 가능)
            os.environ.setdefault(k, v)

load_env_file("/home/ssm-user/jupyter/.env")


import os
import pandas as pd
import psycopg2
from datetime import datetime, timezone

# ==============================
# 배치 파라미터 (지금은 기본값, 나중에 env로 주입 가능)
# ==============================
BUILDING_ID = int(os.getenv("BUILDING_ID", "95"))
START_DATE  = os.getenv("START_DATE", "2025-12-01")  # inclusive
END_DATE    = os.getenv("END_DATE",   "2026-01-01")  # exclusive

# 실행 식별자(파일명 충돌 방지)
RUN_ID = os.getenv("RUN_ID", datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ"))

BASE_DIR = os.getenv("BASE_DIR", "/home/ssm-user/jupyter")
OUT_DIR = os.getenv("OUT_DIR", f"{BASE_DIR}/output/tagging")
os.makedirs(OUT_DIR, exist_ok=True)

YYYYMM = START_DATE.replace("-", "")[:6]
OUT_CSV_PATH = os.getenv(
    "OUT_CSV_PATH",
    f"{OUT_DIR}/tagged_{BUILDING_ID}_{YYYYMM}_{RUN_ID}.csv"
)

# ==============================
# DB 접속정보 (.env에서 로드)
# ==============================
DB_HOST = os.getenv("DB_HOST", "")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "")
DB_USER = os.getenv("DB_USER", "")
DB_PASSWORD = os.getenv("DB_PASSWORD", "")

if not (DB_HOST and DB_NAME and DB_USER and DB_PASSWORD):
    raise RuntimeError("DB 환경변수가 비어 있습니다. DB_HOST/DB_NAME/DB_USER/DB_PASSWORD를 설정해 주세요.")

def _db_conn():
    return psycopg2.connect(
        host=DB_HOST, port=DB_PORT, dbname=DB_NAME, user=DB_USER, password=DB_PASSWORD
    )

# ==============================
# 1) Result 1 (VOC + 최신 reply + reply_writer) DB 조회
# ==============================
RESULT1_SQL = """
SELECT
    v.id AS voc_id,
    v.building_id,
    v.voc_date,
    v.title,
    v.request_contents,

    bf.name AS building_floor_name,
    z.name AS zone_name,

    r.reply,
    r.reply_write_date,
    bug.name AS user_group_name,
    a.name AS reply_writer

FROM voc v
LEFT JOIN building_floor bf
  ON v.building_floor_id = bf.id
LEFT JOIN building_floor_zone z
  ON v.building_floor_zone_id = z.id
LEFT JOIN (
    SELECT DISTINCT ON (voc_id) *
    FROM voc_reply
    ORDER BY voc_id, reply_write_date DESC
) r
  ON v.id = r.voc_id
LEFT JOIN account_group ag
  ON r.reply_writer_id = ag.account_id
LEFT JOIN building_user_group bug
  ON ag.group_id = bug.id
LEFT JOIN account a
  ON r.reply_writer_id = a.id
WHERE v.building_id = %(building_id)s
  AND v.title NOT LIKE '%%테스트%%'
  AND v.voc_date >= %(start_date)s
  AND v.voc_date <  %(end_date)s
ORDER BY v.voc_date ASC, v.id ASC;
"""

def fetch_result1_df(building_id: int, start_date: str, end_date: str) -> pd.DataFrame:
    conn = _db_conn()
    try:
        df = pd.read_sql_query(
            RESULT1_SQL,
            conn,
            params={"building_id": building_id, "start_date": start_date, "end_date": end_date},
        )
    finally:
        conn.close()

    # 태깅에서 바로 쓰기 좋게 NaN 처리
    fill_cols = [
        "title", "request_contents", "reply", "reply_writer",
        "building_floor_name", "zone_name", "user_group_name"
    ]
    for c in fill_cols:
        if c in df.columns:
            df[c] = df[c].fillna("")

    return df

# ==============================
# 2) taxonomy DB 조회 (voc_taxonomy)
#    run_retagging 내부의 build_taxonomy는 엑셀 컬럼명(대분류/중분류/사례/키워드)을 기대하므로 rename
# ==============================
def fetch_taxonomy_dfs(version: str | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:
    conn = _db_conn()
    try:
        if version:
            tax_df = pd.read_sql_query(
                """
                SELECT taxonomy_type, major, minor, keywords, priority
                FROM voc_taxonomy
                WHERE is_active = true
                  AND version = %(version)s
                """,
                conn,
                params={"version": version},
            )
        else:
            tax_df = pd.read_sql_query(
                """
                SELECT taxonomy_type, major, minor, keywords, priority
                FROM voc_taxonomy
                WHERE is_active = true
                """,
                conn,
            )
    finally:
        conn.close()

    subj = tax_df[tax_df["taxonomy_type"] == "SUBJECT"].copy()
    work = tax_df[tax_df["taxonomy_type"] == "WORK"].copy()

    subj = subj.rename(columns={"major": "대분류", "minor": "중분류", "keywords": "사례/키워드"})
    work = work.rename(columns={"major": "대분류", "minor": "중분류", "keywords": "사례/키워드"})

    for df in (subj, work):
        for c in ["대분류", "중분류", "사례/키워드"]:
            if c in df.columns:
                df[c] = df[c].fillna("").astype(str)

    subj = subj[(subj["대분류"] != "") & (subj["중분류"] != "") & (subj["사례/키워드"] != "")]
    work = work[(work["대분류"] != "") & (work["중분류"] != "") & (work["사례/키워드"] != "")]

    return subj, work

# ==============================
# 3) 엑셀 I/O 제거: run_retagging_df
#    (기존 run_retagging의 본문에서 "엑셀 읽기/저장"만 제거한 구조)
# ==============================
def run_retagging_df(
    result_df: pd.DataFrame,
    subject_tax: pd.DataFrame,
    work_tax: pd.DataFrame,
    # 분류기 파라미터 (현재는 고정값 사용, 나중에 env로 주입 가능)
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # 사전 주입
    compound_words: dict = None,
    stop_words: set = None,
) -> pd.DataFrame:
    compound_words = compound_words or {}
    stop_words = stop_words or set()
    compound_replacers = build_compound_replacers(compound_words)

    # 필수 컬럼 체크
    for col in ["title", "request_contents", "reply"]:
        if col not in result_df.columns:
            raise KeyError(f"필수 컬럼이 없습니다: {col}")

    result_df = result_df.copy()

    # all_text 구성 (원문 보존) - 기존과 동일
    result_df.loc[:, "title_clean"] = result_df["title"].map(whitelist_text).fillna("")
    result_df.loc[:, "all_text"] = (
        result_df["request_contents"].map(whitelist_text).fillna("") + " " +
        result_df["title_clean"] + " " +
        result_df["reply"].map(whitelist_text).fillna("")
    ).str.strip()

    # taxonomy 아이템 빌드 (정규화 패턴 포함)
    subject_items = build_taxonomy(subject_tax, compound_replacers, stop_words)
    work_items    = build_taxonomy(work_tax, compound_replacers, stop_words)
    subject_etc   = find_tax_etc(subject_items)
    work_etc      = find_tax_etc(work_items)

    # 분류 실행 (기존 classify_weighted_general 그대로 사용)
    sub_major, sub_minor, sub_score = [], [], []
    work_major, work_minor, work_score = [], [], []

    for _, row in result_df.iterrows():
        all_text = row.get("all_text", "")
        title_text = row.get("title_clean", row.get("title", ""))

        mj, mn, sc = classify_weighted_general(
            all_text, subject_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj == "":
            mj, mn = (subject_etc if subject_etc is not None else ("", ""))
        sub_major.append(mj); sub_minor.append(mn); sub_score.append(sc)

        mj2, mn2, sc2 = classify_weighted_general(
            all_text, work_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj2 == "":
            mj2, mn2 = (work_etc if work_etc is not None else ("", ""))
        work_major.append(mj2); work_minor.append(mn2); work_score.append(sc2)

    # 기존 결과 컬럼명 유지
    result_df.loc[:, "주제 대분류"] = sub_major
    result_df.loc[:, "주제 중분류"] = sub_minor
    result_df.loc[:, "작업유형 대분류"] = work_major
    result_df.loc[:, "작업유형 중분류"] = work_minor
    result_df.loc[:, "주제_score"] = sub_score
    result_df.loc[:, "작업유형_score"] = work_score

    return result_df

# ==============================
# 4) 실행: DB -> 태깅 -> CSV 저장
# ==============================
compound_words, stop_words = load_text_dict_from_db()

voc_df = fetch_result1_df(BUILDING_ID, START_DATE, END_DATE)
subject_tax_df, work_tax_df = fetch_taxonomy_dfs(version=os.getenv("TAXONOMY_VERSION") or None)

print(f"[INFO] voc_df rows={len(voc_df)} cols={len(voc_df.columns)}")
print(f"[INFO] subject_tax rows={len(subject_tax_df)} / work_tax rows={len(work_tax_df)}")

tagged_df = run_retagging_df(
    result_df=voc_df,
    subject_tax=subject_tax_df,
    work_tax=work_tax_df,
    compound_words=compound_words,
    stop_words=stop_words,
    # 파라미터는 현재 고정, 나중에 env로 주입하는 형태로 바꾸면 됩니다.
)

tagged_df = tagged_df.copy()
tagged_df.loc[:, "run_id"] = RUN_ID
tagged_df.loc[:, "tagged_at"] = datetime.now(timezone.utc).isoformat()

tagged_df.to_csv(OUT_CSV_PATH, index=False, encoding="utf-8-sig")
print(f"[INFO] saved csv: {OUT_CSV_PATH}")

tagged_df.head(3)




