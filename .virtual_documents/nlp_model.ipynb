





from datetime import date 

# 배치 파라미터 
BUILDING_ID = int(os.getenv("BUILDING_ID", "307"))
START_DATE = os.getenv("START_DATE", "2025-08-01")
END_DATE = os.getenv("END_DATE", "2025-09-01")

# 입력 템플릿 


DB_HOST = 'hdcl-csp-rds-aurora-cluster.cluster-cody3sv8qrid.ap-northeast-2.rds.amazonaws.com'
DB_NAME = 'csp_prd'
DB_USER = 'csp_admin'
DB_PASSWORD = 'hdci12#$'
DB_PORT = '5432'


# 필요 패키지: pandas, numpy, openpyxl, psycopg2 (미설치 시: pip install pandas numpy openpyxl psycopg2-binary)

import pandas as pd
import numpy as np
import re, html, unicodedata
from pathlib import Path

# ===== DB에서 compound/stop 사전을 읽는 함수 (사용자 제공 코드) =====
def load_text_dict_from_db():
    import psycopg2
    try:
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()

        # compound_words: key, value 모두 strip
        cursor.execute("""
            SELECT word_key, word_value 
            FROM stat_text_dict 
            WHERE type = 'compound' AND is_active = TRUE
        """)
        compound_result = cursor.fetchall()
        compound_words = {
            row[0].strip(): row[1].strip()
            for row in compound_result if row[0] and row[1]
        }

        # stop_words
        cursor.execute("""
            SELECT word_key 
            FROM stat_text_dict 
            WHERE type = 'stop' AND is_active = TRUE
        """)
        stop_result = cursor.fetchall()
        stop_words = {row[0].strip() for row in stop_result if row[0]}

        cursor.close()
        connection.close()
        return compound_words, stop_words

    except Exception:
        # 연결 실패 시 빈 사전/셋으로 진행
        return {}, set()

# ===== 전처리 유틸 =====
def strip_html(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = str(text)
    s = html.unescape(s)
    # <script>/<style> 블록 제거
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)
    # 나머지 태그 제거
    s = re.sub(r"(?s)<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# 허용 문자 화이트리스트
ALLOWED_PATTERN = re.compile(r"[^가-힣ㄱ-ㅎㅏ-ㅣA-Za-z0-9\s\.\,\!\?\;\:\'\"\(\)\[\]\{\}\-\_\/\&\+\%\#\@]")
def whitelist_text(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = strip_html(text)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[\u200B-\u200D\uFE0E\uFE0F]", "", s)  # 제로폭/variation selector 제거
    s = ALLOWED_PATTERN.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# ===== taxonomy 파싱 =====
def detect_taxonomy_columns(df: pd.DataFrame):
    cols = list(df.columns)
    def find_first(patterns):
        for c in cols:
            for p in patterns:
                if re.search(p, str(c), re.I):
                    return c
        return None
    major_col = find_first([r"대분류"])
    minor_col = find_first([r"중분류"])
    example_cols = [c for c in cols if re.search(r"사례|예시", str(c), re.I)]
    keyword_cols = [c for c in cols if re.search(r"키워드", str(c), re.I)]
    if not example_cols and not keyword_cols:
        keyword_cols = [c for c in cols if re.search(r"패턴|룰|rule|pattern", str(c), re.I)]
    return major_col, minor_col, example_cols, keyword_cols

def row_patterns(row: pd.Series, cols):
    patterns = []
    for c in cols:
        if c is None or c not in row or pd.isna(row[c]): 
            continue
        val = str(row[c]).strip()
        if not val:
            continue
        parts = re.split(r"[,\;\|\n\r\t]+", val)
        parts = [p.strip() for p in parts if p.strip()]
        if not parts:
            parts = [val]
        patterns.extend(parts)
    return list(dict.fromkeys(patterns))  # 순서 유지 중복 제거

def find_tax_etc(items):
    # taxonomy 안에 실제 존재하는 '기타'만 허용
    for it in items:
        if it["major"] == "기타" and it["minor"] == "기타":
            return ("기타", "기타")
    for it in items:
        if it["major"] == "기타":
            return (it["major"], it["minor"] if it["minor"] else "기타")
    return None

# ===== 토큰/문장 분해 =====
SENT_SPLIT = re.compile(r"[\.!\?…\n\r]+|[。！？]")
TOKEN_RE = re.compile(r"[가-힣A-Za-z0-9]+")

# ===== 길이 가중 =====
def _length_weight(token_count: int, short_len: int, long_len: int, short_penalty: float, long_penalty: float):
    if token_count <= short_len:
        return short_penalty
    if token_count >= long_len:
        return long_penalty
    return 1.0

# ===== 복합어/불용어 정규화 파이프라인 (추가) =====
def build_compound_replacers(compound_words: dict):
    """
    compound_words의 key를 긴 순서로 정렬하여 치환용 정규식 리스트로 컴파일합니다.
    """
    if not compound_words:
        return []
    keys_sorted = sorted([k for k in compound_words.keys() if k], key=len, reverse=True)
    replacers = []
    for k in keys_sorted:
        v = compound_words[k]
        pattern = re.compile(re.escape(k), flags=re.IGNORECASE)
        replacers.append((pattern, v))
    return replacers

def apply_compound(text: str, replacers: list):
    if not text or not replacers:
        return text or ""
    s = text
    for rgx, rep in replacers:
        s = rgx.sub(rep, s)
    return s

def remove_stopwords(text: str, stop_words: set):
    """
    TOKEN_RE 기준으로 토큰화 후 불용어를 제거하고 다시 공백으로 연결합니다.
    """
    if not text:
        return ""
    tokens = TOKEN_RE.findall(text)
    if not tokens:
        return text.strip()
    if not stop_words:
        return " ".join(tokens)
    kept = [t for t in tokens if t not in stop_words]
    return " ".join(kept)

def preprocess_for_match(raw_text: str, compound_replacers: list, stop_words: set):
    """
    매칭/유사도 계산 전용 정규화 파이프라인.
    1) HTML/이상문자 제거 + 화이트리스트(whitelist_text)
    2) NFKC 정규화(whitelist_text 내부)
    3) 소문자화
    4) 복합어 치환
    5) 불용어 제거
    """
    base = whitelist_text(raw_text)
    base = base.lower()
    base = apply_compound(base, compound_replacers)
    base = remove_stopwords(base, stop_words)
    base = re.sub(r"\s+", " ", base).strip()
    return base

# ===== 중요도 기반 분류 (교체된 함수들) =====
def _overlap_with_title(sent_norm: str, title_norm: str, overlap_ratio: float):
    """
    정규화된 문장/제목을 토큰집합으로 비교하여 겹침 비율로 유사도를 판정합니다.
    """
    if not title_norm:
        return False
    stoks = set(TOKEN_RE.findall(sent_norm))
    ttoks = set(TOKEN_RE.findall(title_norm))
    if not stoks or not ttoks:
        return False
    inter = len(stoks & ttoks)
    return inter >= max(1, int(overlap_ratio * len(stoks)))

def build_taxonomy(tax_df: pd.DataFrame, compound_replacers: list = None, stop_words: set = None):
    """
    기존 items에 'norm_patterns'를 추가.
    norm_patterns는 패턴마다 preprocess_for_match를 적용한 결과입니다.
    """
    compound_replacers = compound_replacers or []
    stop_words = stop_words or set()

    major_col, minor_col, ex_cols, kw_cols = detect_taxonomy_columns(tax_df)
    items = []
    for _, row in tax_df.iterrows():
        major = str(row[major_col]).strip() if (major_col and major_col in row and not pd.isna(row[major_col])) else ""
        minor = str(row[minor_col]).strip() if (minor_col and minor_col in row and not pd.isna(row[minor_col])) else ""
        ex_patterns = row_patterns(row, ex_cols) if ex_cols else []
        kw_patterns = row_patterns(row, kw_cols) if kw_cols else []
        patterns = list(dict.fromkeys(ex_patterns + kw_patterns))

        # 패턴 정규화본
        norm_patterns = []
        for p in patterns:
            pn = preprocess_for_match(p, compound_replacers, stop_words)
            if pn:
                norm_patterns.append(pn)

        if major or minor or patterns:
            items.append({
                "major": major,
                "minor": minor,
                "patterns": patterns,
                "norm_patterns": norm_patterns
            })
    return items

def classify_weighted_general(
    all_text: str,
    items: list,
    title_text: str = None,
    # 가중치 파라미터
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # 추가 파라미터
    compound_replacers: list = None,
    stop_words: set = None
):
    """
    반환: (major, minor, score)
    문장 중요도 = 위치 가중 × 제목 유사 가중 × 길이 보정
    항목 점수 = Σ[ 문장가중치 × (패턴매칭개수 + length_bonus_scale * 매칭패턴길이합) ]
    매칭은 '정규화된 문장' 대 '정규화된 패턴(norm_patterns)'로 수행합니다.
    """
    compound_replacers = compound_replacers or []
    stop_words = stop_words or set()

    text = "" if all_text is None else str(all_text)
    sentences = [s.strip() for s in SENT_SPLIT.split(text) if s.strip()]
    if not sentences:
        sentences = [text]

    # 정규화된 제목
    title_norm = preprocess_for_match(title_text or "", compound_replacers, stop_words)

    item_scores = np.zeros(len(items), dtype=float)

    for idx_s, sent in enumerate(sentences):
        # 원문 기준 토큰 길이(길이 보정은 원문 토큰 수로 계산)
        tok_cnt = len(TOKEN_RE.findall(sent.lower()))
        # 매칭/유사도는 정규화 텍스트로
        s_norm = preprocess_for_match(sent, compound_replacers, stop_words)

        # 1) 위치 가중치: pos_max -> pos_min 선형감쇠
        step = min(idx_s, max(1, decay_steps))
        pos_w = pos_max - step * ((pos_max - pos_min) / max(1, decay_steps))
        pos_w = max(pos_min, min(pos_max, pos_w))

        # 2) 제목 유사 가중치
        title_w = (title_boost if _overlap_with_title(s_norm, title_norm, title_overlap_ratio) else 1.0)

        # 3) 길이 보정
        len_w = _length_weight(tok_cnt, short_len, long_len, short_penalty, long_penalty)

        sent_w = pos_w * title_w * len_w

        # 항목별 매칭 점수
        for i, it in enumerate(items):
            norm_ps = it.get("norm_patterns")
            if norm_ps is None:
                # 하위호환: patterns를 그때그때 정규화
                norm_ps = [preprocess_for_match(p, compound_replacers, stop_words) for p in it.get("patterns", [])]

            hits = 0
            length_bonus = 0
            for p in norm_ps:
                if not p:
                    continue
                if p in s_norm:
                    hits += 1
                    length_bonus += len(p)
            if hits > 0:
                item_scores[i] += sent_w * (hits + length_bonus_scale * length_bonus)

    if item_scores.max() > 0:
        best_i = int(item_scores.argmax())
        return items[best_i]["major"], items[best_i]["minor"], float(item_scores[best_i])
    return "", "", 0.0

# ===== 재태깅 메인 (교체된 run_retagging) =====
def run_retagging(
    input_path: Path,
    output_path: Path,
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    # 분류기 파라미터
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # 사전 주입
    compound_words: dict = None,
    stop_words: set = None
) -> pd.DataFrame:
    """
    입력 파일의 taxonomy 시트를 그대로 사용하여 Result 1 시트를 재태깅하고,
    output_path로 모든 시트를 보존하여 저장합니다.
    반환값: 갱신된 Result 1 데이터프레임
    """
    compound_words = compound_words or {}
    stop_words = stop_words or set()
    compound_replacers = build_compound_replacers(compound_words)

    xls = pd.ExcelFile(input_path)
    sheet_names = xls.sheet_names

    result_df = pd.read_excel(xls, sheet_name=result_sheet, header=0, dtype=str)
    subject_tax = pd.read_excel(xls, sheet_name=subject_tax_sheet, header=0, dtype=str)
    work_tax    = pd.read_excel(xls, sheet_name=work_tax_sheet, header=0, dtype=str)

    for col in ["title", "request_contents", "reply"]:
        if col not in result_df.columns:
            raise KeyError(f"필수 컬럼이 없습니다: {col}")

    # all_text 구성 (원문 보존). SettingWithCopyWarning 방지 위해 .loc 사용
    result_df.loc[:, "title_clean"] = result_df["title"].map(whitelist_text).fillna("")
    result_df.loc[:, "all_text"] = (
        result_df["request_contents"].map(whitelist_text).fillna("") + " " +
        result_df["title_clean"] + " " +
        result_df["reply"].map(whitelist_text).fillna("")
    ).str.strip()

    # taxonomy 아이템: 정규화 패턴 포함
    subject_items = build_taxonomy(subject_tax, compound_replacers, stop_words)
    work_items    = build_taxonomy(work_tax, compound_replacers, stop_words)
    subject_etc   = find_tax_etc(subject_items)
    work_etc      = find_tax_etc(work_items)

    # 분류
    sub_major, sub_minor, work_major, work_minor = [], [], [], []
    sub_score, work_score = [], []

    for all_text, title_text in zip(result_df["all_text"], result_df["title_clean"]):
        mj, mn, sc = classify_weighted_general(
            all_text, subject_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj == "":
            mj, mn = (subject_etc if subject_etc is not None else ("", ""))
        sub_major.append(mj); sub_minor.append(mn); sub_score.append(sc)

        mj2, mn2, sc2 = classify_weighted_general(
            all_text, work_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj2 == "":
            mj2, mn2 = (work_etc if work_etc is not None else ("", ""))
        work_major.append(mj2); work_minor.append(mn2); work_score.append(sc2)

    result_df.loc[:, "주제 대분류"] = sub_major
    result_df.loc[:, "주제 중분류"] = sub_minor
    result_df.loc[:, "작업유형 대분류"] = work_major
    result_df.loc[:, "작업유형 중분류"] = work_minor
    # 점수는 참고용
    result_df.loc[:, "주제_score"] = sub_score
    result_df.loc[:, "작업유형_score"] = work_score

    # 모든 시트 유지하여 저장
    with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
        for name in sheet_names:
            if name == result_sheet:
                result_df.to_excel(writer, index=False, sheet_name=name)
            else:
                pd.read_excel(xls, sheet_name=name).to_excel(writer, index=False, sheet_name=name)

    return result_df


input_path = Path("Result_1_retagged.xlsx")  # 입력 파일
output_path = Path("Result_1_retagged_weighted_general.xlsx")  # 출력 파일
compound_words, stop_words = load_text_dict_from_db()

df_result = run_retagging(
    input_path=input_path,
    output_path=output_path,
    # 중요도 가중치 파라미터(필요 시 조정)
    pos_max=1.5, pos_min=1.0,
    title_boost=2.0, title_overlap_ratio=0.3,
    short_len=5, long_len=60,
    short_penalty=0.9, long_penalty=0.9,
    decay_steps=5, length_bonus_scale=0.1,
    compound_words=compound_words,
    stop_words=stop_words  
)

print("저장 완료:", output_path)
# 요약 확인
print("총 행수:", len(df_result))
print("주제 대분류 채움:", (df_result['주제 대분류'].astype(str).str.strip() != '').sum())
print("작업유형 대분류 채움:", (df_result['작업유형 대분류'].astype(str).str.strip() != '').sum())






# ===== 튜닝/리포팅 유틸 모음(주피터용 진행바/로그 포함) =====
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd
import math, time
from IPython.display import display, HTML

# 0) 파라미터 객체와 검색공간
@dataclass
class ClassifierParams:
    pos_max: float = 1.5
    pos_min: float = 1.0
    title_boost: float = 2.0
    title_overlap_ratio: float = 0.3
    short_len: int = 5
    long_len: int = 60
    short_penalty: float = 0.9
    long_penalty: float = 0.9
    decay_steps: int = 5
    length_bonus_scale: float = 0.1

# 필요 시 검색 범위를 조정하세요.
SPACE = {
    "pos_max": (1.2, 2.5),
    "pos_min": (0.8, 1.1),
    "title_boost": (1.0, 4.0),
    "title_overlap_ratio": (0.1, 0.5),
    "short_len": (3, 10, "int"),
    "long_len": (40, 120, "int"),
    "short_penalty": (0.6, 1.0),
    "long_penalty": (0.6, 1.0),
    "decay_steps": (2, 10, "int"),
    "length_bonus_scale": (0.05, 0.3),
}

# 1) 공통: 데이터 준비/한 번 분류 실행/정확도 계산
def _prepare_df_for_tuning(
    input_path: Path,
    result_sheet: str,
    subject_tax_sheet: str,
    work_tax_sheet: str,
) -> Tuple[pd.DataFrame, list, list, Optional[Tuple[str,str]], Optional[Tuple[str,str]]]:
    xls = pd.ExcelFile(input_path)
    df = pd.read_excel(xls, sheet_name=result_sheet, header=0, dtype=str).copy()
    for col in ["title", "request_contents", "reply"]:
        if col not in df.columns:
            raise KeyError(f"필수 컬럼이 없습니다: {col}")
    df.loc[:, "title_clean"] = df["title"].map(whitelist_text).fillna("")
    df.loc[:, "all_text"] = (
        df["request_contents"].map(whitelist_text).fillna("") + " " +
        df["title_clean"] + " " +
        df["reply"].map(whitelist_text).fillna("")
    ).str.strip()
    subject_tax = pd.read_excel(xls, sheet_name=subject_tax_sheet, header=0, dtype=str)
    work_tax    = pd.read_excel(xls, sheet_name=work_tax_sheet, header=0, dtype=str)
    subject_items = build_taxonomy(subject_tax)
    work_items    = build_taxonomy(work_tax)
    subject_etc   = find_tax_etc(subject_items)
    work_etc      = find_tax_etc(work_items)
    return df, subject_items, work_items, subject_etc, work_etc

def _classify_df_once(
    df: pd.DataFrame,
    subject_items: list,
    work_items: list,
    subject_etc: Optional[Tuple[str, str]],
    work_etc: Optional[Tuple[str, str]],
    params: ClassifierParams,
) -> pd.DataFrame:
    out = df.copy()
    sub_major, sub_minor, sub_score = [], [], []
    work_major, work_minor, work_score = [], [], []
    for all_text, title_text in zip(out["all_text"], out["title_clean"]):
        mj, mn, sc = classify_weighted_general(
            all_text, subject_items, title_text=title_text,
            pos_max=params.pos_max, pos_min=params.pos_min,
            title_boost=params.title_boost, title_overlap_ratio=params.title_overlap_ratio,
            short_len=params.short_len, long_len=params.long_len,
            short_penalty=params.short_penalty, long_penalty=params.long_penalty,
            decay_steps=params.decay_steps, length_bonus_scale=params.length_bonus_scale
        )
        if mj == "" and subject_etc is not None:
            mj, mn = subject_etc
        sub_major.append(mj); sub_minor.append(mn); sub_score.append(sc)

        mj2, mn2, sc2 = classify_weighted_general(
            all_text, work_items, title_text=title_text,
            pos_max=params.pos_max, pos_min=params.pos_min,
            title_boost=params.title_boost, title_overlap_ratio=params.title_overlap_ratio,
            short_len=params.short_len, long_len=params.long_len,
            short_penalty=params.short_penalty, long_penalty=params.long_penalty,
            decay_steps=params.decay_steps, length_bonus_scale=params.length_bonus_scale
        )
        if mj2 == "" and work_etc is not None:
            mj2, mn2 = work_etc
        work_major.append(mj2); work_minor.append(mn2); work_score.append(sc2)

    out.loc[:, "주제 대분류_pred"] = sub_major
    out.loc[:, "주제 중분류_pred"] = sub_minor
    out.loc[:, "작업유형 대분류_pred"] = work_major
    out.loc[:, "작업유형 중분류_pred"] = work_minor
    out.loc[:, "주제_score"] = sub_score
    out.loc[:, "작업유형_score"] = work_score
    return out

def _accuracy_macro_major_minor(
    df_pred: pd.DataFrame,
    subject_major_gt: str = "주제 대분류",
    subject_minor_gt: str = "주제 중분류",
    work_major_gt: str = "작업유형 대분류",
    work_minor_gt: str = "작업유형 중분류",
    ignore_blank: bool = True,
    ignore_etc: bool = False,
) -> Dict[str, float]:
    def _pair_acc(pred_c, gt_c):
        if gt_c not in df_pred.columns:
            return np.nan
        mask = pd.Series([True]*len(df_pred))
        if ignore_blank:
            mask &= df_pred[gt_c].fillna("").astype(str).str.strip() != ""
        if ignore_etc:
            mask &= ~df_pred[gt_c].astype(str).eq("기타")
        if mask.sum() == 0:
            return np.nan
        return float((df_pred.loc[mask, pred_c].astype(str) == df_pred.loc[mask, gt_c].astype(str)).mean())
    metrics = {
        "subject_major_acc": _pair_acc("주제 대분류_pred", subject_major_gt),
        "subject_minor_acc": _pair_acc("주제 중분류_pred", subject_minor_gt),
        "work_major_acc":    _pair_acc("작업유형 대분류_pred", work_major_gt),
        "work_minor_acc":    _pair_acc("작업유형 중분류_pred", work_minor_gt),
    }
    valid_vals = [v for v in metrics.values() if not (v is None or np.isnan(v))]
    metrics["macro_avg"] = float(np.mean(valid_vals)) if valid_vals else np.nan
    return metrics

# 2) 자동 기본값 추정
def auto_seed_params(input_path: Path, result_sheet="Result 1") -> ClassifierParams:
    xls = pd.ExcelFile(input_path)
    df = pd.read_excel(xls, sheet_name=result_sheet, dtype=str)
    df.loc[:, "title_clean"] = df["title"].map(whitelist_text).fillna("")
    df.loc[:, "all_text"] = (
        df["request_contents"].map(whitelist_text).fillna("") + " " +
        df["title_clean"] + " " +
        df["reply"].map(whitelist_text).fillna("")
    ).str.strip()

    sent_lens = []
    for txt in df["all_text"]:
        sents = [s.strip() for s in SENT_SPLIT.split(str(txt)) if s.strip()]
        if not sents: sents = [str(txt)]
        for s in sents:
            sent_lens.append(len(TOKEN_RE.findall(s.lower())))
    sent_lens = np.array(sent_lens) if len(sent_lens)>0 else np.array([10])

    q25 = int(np.percentile(sent_lens, 25))
    q75 = int(np.percentile(sent_lens, 75))
    avg_sent_per_doc = float(np.mean([max(1, len([s for s in SENT_SPLIT.split(str(t)) if s.strip()])) for t in df["all_text"]]))
    decay = int(np.clip(round(avg_sent_per_doc * 0.3), 3, 8))

    avg_title_len = int(np.mean([len(TOKEN_RE.findall(str(t).lower())) for t in df["title_clean"]]))
    title_overlap = 0.25 if avg_title_len <= 5 else (0.35 if avg_title_len >= 12 else 0.3)

    return ClassifierParams(
        short_len=max(3, min(10, q25)),
        long_len=max(40, min(120, q75)),
        decay_steps=decay,
        title_overlap_ratio=title_overlap,
    )

# 3) 민감도 분석 (진행바/로그 지원)
def sensitivity_scan(
    input_path: Path,
    base_params: ClassifierParams,
    param_name: str,
    values: List,
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    gt_cols: Dict[str,str] = None,
    # 아래부터 추가 옵션(기본값 그대로 두면 이전과 동일하게 동작)
    show_progress: bool = True,      # 진행바 표시
    show_logs: bool = True,          # 로그 표시
    log_every: int = 1,              # n 스텝마다 로그 출력
    show_best_only: bool = True,     # 최고점 갱신시에만 로그
):
    # 내부 유틸(외부 의존 없이 동작)
    from IPython.display import display, HTML

    def _render_bar(done: int, total: int, width: int = 40) -> str:
        done = min(done, total)
        filled = int(width * done / max(1, total))
        return "[" + "#" * filled + "-" * (width - filled) + f"] {done}/{total}"

    def _print_html(s: str):
        display(HTML(f"<pre style='font-size:13px;line-height:1.5;margin:0'>{s}</pre>"))

    def _unsup_objective(dfp: pd.DataFrame) -> float:
        cov = 0.5*(dfp["주제 대분류_pred"].astype(str).str.strip() != "").mean() + \
              0.5*(dfp["작업유형 대분류_pred"].astype(str).str.strip() != "").mean()
        def norm_entropy(col):
            vc = dfp[col].astype(str).value_counts()
            p = (vc / vc.sum()).values
            if len(p) <= 1:
                return 0.0
            return float(-(p*np.log(p+1e-12)).sum() / math.log(len(p)))
        uniq = 0.5*norm_entropy("주제 대분류_pred") + 0.5*norm_entropy("작업유형 대분류_pred")
        return 0.7*cov + 0.3*uniq

    # 데이터 준비
    df, subject_items, work_items, subject_etc, work_etc = _prepare_df_for_tuning(
        input_path, result_sheet, subject_tax_sheet, work_tax_sheet
    )

    total = len(values)
    disp = None
    if show_progress:
        disp = display(HTML(_render_bar(0, total)), display_id=True)

    rows = []
    best_score = -1.0
    best_rec = None

    for i, v in enumerate(values, start=1):
        pdict = base_params.__dict__.copy()
        pdict[param_name] = v
        params = ClassifierParams(**pdict)

        pred = _classify_df_once(df, subject_items, work_items, subject_etc, work_etc, params)

        if gt_cols:
            metrics = _accuracy_macro_major_minor(pred, **gt_cols, ignore_blank=True, ignore_etc=False)
            score = metrics.get("macro_avg", np.nan)
            # 지도 지표가 없거나 NaN이면 무감독 점수로 보완
            if score is None or np.isnan(score):
                score = _unsup_objective(pred)
                metrics = {**metrics, "unsup_score": score}
        else:
            score = _unsup_objective(pred)
            # 무감독일 때도 참고용으로 구성 요소를 남김
            cov = 0.5*(pred["주제 대분류_pred"].astype(str).str.strip() != "").mean() + \
                  0.5*(pred["작업유형 대분류_pred"].astype(str).str.strip() != "").mean()
            def _ne(col):
                vc = pred[col].astype(str).value_counts()
                p = (vc / vc.sum()).values
                if len(p) <= 1:
                    return 0.0
                return float(-(p*np.log(p+1e-12)).sum() / math.log(len(p)))
            uniq = 0.5*_ne("주제 대분류_pred") + 0.5*_ne("작업유형 대분류_pred")
            metrics = {"macro_avg": np.nan, "covered": cov, "uniq": uniq, "unsup_score": score}

        # 결과 누적
        row = {"value": v, "score": float(score), **metrics}
        rows.append(row)

        # 로그/베스트 갱신 출력
        improved = float(score) > best_score
        if improved:
            best_score = float(score)
            best_rec = (v, params)

        if show_logs and ((not show_best_only and (i % log_every == 0)) or (show_best_only and improved)):
            if gt_cols:
                _print_html(f"[{i}/{total}] {param_name}={v}  score={round(float(score),6)}  macro_avg={metrics.get('macro_avg')}")
            else:
                _print_html(f"[{i}/{total}] {param_name}={v}  score={round(float(score),6)}  covered={round(metrics['covered'],6)} uniq={round(metrics['uniq'],6)}")

        # 진행바 갱신
        if show_progress and disp is not None:
            disp.update(HTML(_render_bar(i, total)))

    # 요약 로그
    if show_logs and best_rec is not None:
        v_best, p_best = best_rec
        _print_html(f"[best] {param_name}={v_best}  score={round(best_score,6)}  params: {p_best}")

    return pd.DataFrame(rows)


# 4) 진행바/로그 유틸
def _render_bar(done: int, total: int, width: int = 40) -> str:
    done = min(done, total)
    filled = int(width * done / max(1, total))
    return "[" + "#" * filled + "-" * (width - filled) + f"] {done}/{total}"

def _param_diff(prev: Optional[ClassifierParams], curr: ClassifierParams) -> str:
    cd = curr.__dict__
    if prev is None:
        return ", ".join([f"{k}={cd[k]}" for k in sorted(cd.keys())])
    pd_ = prev.__dict__
    diffs = []
    for k in sorted(cd.keys()):
        if cd[k] != pd_[k]:
            diffs.append(f"{k}: {pd_[k]} -> {cd[k]}")
    return ", ".join(diffs) if diffs else "(변경 없음)"

def _print_html(s: str):
    display(HTML(f"<pre style='font-size:13px;line-height:1.5;margin:0'>{s}</pre>"))

# 5) 랜덤 탐색(주피터 진행바/로그)
def random_search_tuning_jupyter(
    input_path: Path,
    trials: int = 80,
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    gt_cols: Dict[str,str] = None,
    seed: int = 42,
    log_every: int = 1,
    show_params_every: int = 1,
):
    rng = np.random.default_rng(seed)
    df, subject_items, work_items, subject_etc, work_etc = _prepare_df_for_tuning(
        input_path, result_sheet, subject_tax_sheet, work_tax_sheet
    )
    def sample_params() -> ClassifierParams:
        def pick(spec):
            if len(spec) == 2:
                lo, hi = spec; return float(rng.uniform(lo, hi))
            lo, hi, _ = spec; return int(rng.integers(lo, hi+1))
        pdict = {k: pick(v) for k, v in SPACE.items()}
        return ClassifierParams(**pdict)

    def unsup_objective(dfp: pd.DataFrame) -> float:
        cov = 0.5*(dfp["주제 대분류_pred"].astype(str).str.strip() != "").mean() + \
              0.5*(dfp["작업유형 대분류_pred"].astype(str).str.strip() != "").mean()
        def norm_entropy(col):
            vc = dfp[col].astype(str).value_counts()
            p = (vc / vc.sum()).values
            if len(p) <= 1: return 0.0
            return float(-(p*np.log(p+1e-12)).sum() / math.log(len(p)))
        uniq = 0.5*norm_entropy("주제 대분류_pred") + 0.5*norm_entropy("작업유형 대분류_pred")
        return 0.7*cov + 0.3*uniq

    best_params, best_score, best_metrics = None, -1.0, {}
    disp = display(HTML(_render_bar(0, trials)), display_id=True)

    for t in range(1, trials+1):
        params = sample_params()
        pred = _classify_df_once(df, subject_items, work_items, subject_etc, work_etc, params)
        if gt_cols:
            metrics = _accuracy_macro_major_minor(pred, **gt_cols, ignore_blank=True, ignore_etc=False)
            score = metrics.get("macro_avg", np.nan)
            if np.isnan(score):
                score = unsup_objective(pred)
                metrics = {"macro_avg": np.nan, "unsup": score}
        else:
            metrics = {}
            score = unsup_objective(pred)

        if score > best_score:
            _print_html(f"[best@{t}] score={round(float(score),6)}  diff: " + _param_diff(best_params, params))
            best_params, best_score, best_metrics = params, float(score), metrics

        if (t % log_every == 0) or (t == trials):
            disp.update(HTML(_render_bar(t, trials)))
        if (t % show_params_every == 0):
            _print_html(f"[trial {t}] params: {params}")

    return best_params, best_metrics

# 6) 하이퍼밴드(주피터 진행바/로그)
def hyperband_tune_jupyter(
    input_path: Path,
    result_sheet="Result 1",
    subject_tax_sheet="주제 taxonomy",
    work_tax_sheet="작업유형 taxonomy",
    gt_cols: Dict[str,str] = None,
    max_budget_rows: int = 4000,
    min_budget_rows: int = 250,
    eta: int = 3,
    init_candidates: int = 60,
    sent_cap_schedule: List[Optional[int]] = (4, 8, 16, None),
    seed: int = 123,
    show_params_each_eval: bool = False,
):
    rng = np.random.default_rng(seed)
    df, subject_items, work_items, subject_etc, work_etc = _prepare_df_for_tuning(
        input_path, result_sheet, subject_tax_sheet, work_tax_sheet
    )
    N = len(df)
    max_budget_rows = min(N, max_budget_rows)
    min_budget_rows = min(max_budget_rows, min_budget_rows)

    def sample_candidate():
        pdict = {}
        for k, spec in SPACE.items():
            if len(spec) == 2:
                lo, hi = spec; pdict[k] = float(rng.uniform(lo, hi))
            else:
                lo, hi, _ = spec; pdict[k] = int(rng.integers(lo, hi+1))
        return pdict

    alive = [sample_candidate() for _ in range(init_candidates)]

    budgets = []
    b = min_budget_rows
    while b < max_budget_rows:
        budgets.append(b)
        b = int(b * eta)
    if budgets[-1] != max_budget_rows:
        budgets.append(max_budget_rows)

    total_evals, tmp = 0, len(alive)
    for _ in budgets:
        total_evals += max(1, tmp)
        tmp = max(1, tmp // eta)

    done = 0
    disp = display(HTML(_render_bar(done, total_evals)), display_id=True)
    history = []
    best = {"score": -1.0, "params": None}

    for round_idx, budget in enumerate(budgets):
        sent_cap = sent_cap_schedule[min(round_idx, len(sent_cap_schedule)-1)]
        scores = []
        for pdict in alive:
            params = ClassifierParams(**pdict)
            dfx = df.sample(n=budget, random_state=17).reset_index(drop=True) if budget < len(df) else df
            if sent_cap is not None:
                def clip_text(t: str) -> str:
                    sents = [s.strip() for s in SENT_SPLIT.split(str(t)) if s.strip()]
                    if len(sents) > sent_cap: sents = sents[:sent_cap]
                    return ". ".join(sents)
                dfx = dfx.copy()
                dfx["all_text"] = dfx["all_text"].map(clip_text)

            pred = _classify_df_once(dfx, subject_items, work_items, subject_etc, work_etc, params)

            if gt_cols:
                metrics = _accuracy_macro_major_minor(pred, **gt_cols, ignore_blank=True, ignore_etc=False)
                score = metrics.get("macro_avg", np.nan)
                if np.isnan(score): score = 0.0
            else:
                cov = 0.5*(pred["주제 대분류_pred"].astype(str).str.strip() != "").mean() + \
                      0.5*(pred["작업유형 대분류_pred"].astype(str).str.strip() != "").mean()
                def norm_entropy(col):
                    vc = pred[col].astype(str).value_counts()
                    p = (vc / vc.sum()).values
                    if len(p) <= 1: return 0.0
                    return float(-(p*np.log(p+1e-12)).sum() / math.log(len(p)))
                uniq = 0.5*norm_entropy("주제 대분류_pred") + 0.5*norm_entropy("작업유형 대분류_pred")
                score = 0.7*cov + 0.3*uniq
                metrics = {"covered": cov, "uniq": uniq}

            scores.append(score)
            history.append({"round": round_idx, "budget_rows": budget, "sent_cap": sent_cap,
                            "params": pdict.copy(), "score": float(score)})

            done += 1
            if show_params_each_eval:
                _print_html(f"[r{round_idx} {done}/{total_evals}] score={round(float(score),6)} params: {params}")
            disp.update(HTML(_render_bar(done, total_evals)))

            if float(score) > best["score"]:
                prev = ClassifierParams(**best["params"]) if best["params"] else None
                _print_html(f"[best@r{round_idx} #{done}] score={round(float(score),6)}  diff: " + _param_diff(prev, params))
                best = {"score": float(score), "params": pdict.copy()}

        k = max(1, len(alive) // eta)
        top_idx = list(np.argsort(scores)[::-1][:k])
        alive = [alive[i] for i in top_idx]

    best_params = ClassifierParams(**best["params"]) if best["params"] else ClassifierParams()
    return best_params, {"best": best, "history": history}

# 7) 점수 캘리브레이션
def add_calibrated_scores(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    for c in ["주제_score", "작업유형_score"]:
        m = out[c].mean(); s = out[c].std(ddof=0) + 1e-9
        out.loc[:, c + "_z"] = (out[c] - m) / s
        mn, mx = out[c].min(), out[c].max()
        out.loc[:, c + "_minmax"] = (out[c] - mn) / (mx - mn + 1e-9)
    return out

# 8) 간단 실행 래퍼(입력값을 함수 인자로 조정)
def run_tuner_with_inputs(
    tuner: str = "hyperband",
    input_path: str = "voc_work.xlsx",
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    # 랜덤 탐색
    trials: int = 120,
    # 하이퍼밴드
    init_candidates: int = 60,
    min_budget_rows: int = 300,
    max_budget_rows: int = 4000,
    eta: int = 3,
    sent_cap_schedule: Tuple[Optional[int], ...] = (4, 8, 16, None),
    # 지도 평가 컬럼(없으면 None)
    gt_cols: Optional[Dict[str, str]] = None,
):
    input_path = Path(input_path)
    if tuner == "random":
        best_params, metrics = random_search_tuning_jupyter(
            input_path=input_path,
            trials=trials,
            result_sheet=result_sheet,
            subject_tax_sheet=subject_tax_sheet,
            work_tax_sheet=work_tax_sheet,
            gt_cols=gt_cols,
        )
        _print_html(f"[완료] 랜덤 탐색 최적 파라미터: {best_params}")
        return best_params, metrics
    else:
        best_params, info = hyperband_tune_jupyter(
            input_path=input_path,
            result_sheet=result_sheet,
            subject_tax_sheet=subject_tax_sheet,
            work_tax_sheet=work_tax_sheet,
            gt_cols=gt_cols,
            init_candidates=init_candidates,
            min_budget_rows=min_budget_rows,
            max_budget_rows=max_budget_rows,
            eta=eta,
            sent_cap_schedule=sent_cap_schedule,
        )
        _print_html(f"[완료] 하이퍼밴드 최적 파라미터: {best_params}")
        return best_params, info



# ====== 시각화/저장 확장 ======
import json
import matplotlib.pyplot as plt

def _flatten_params(p) -> dict:
    """ClassifierParams 또는 dict -> 평탄화 dict"""
    if hasattr(p, "__dict__"):
        return dict(p.__dict__)
    if isinstance(p, dict):
        return dict(p)
    return {}

def _ensure_dir(path: Path):
    path.mkdir(parents=True, exist_ok=True)

# -------------------------------
# 1) 민감도 분석 결과 시각화/저장
# -------------------------------
def plot_sensitivity(scan_df: pd.DataFrame, x_col: str = "value",
                     y_col: str = "score", title: str = None):
    """
    scan_df: sensitivity_scan의 반환 DataFrame
    x_col: 보통 "value"
    y_col: 지도일 때 "macro_avg" 또는 "score", 무감독이면 "score" 권장
    """
    if y_col not in scan_df.columns:
        # 지도 지표가 있으면 macro_avg, 아니면 covered를 기본으로 선택
        y_col = "macro_avg" if "macro_avg" in scan_df.columns else ("score" if "score" in scan_df.columns else "covered")
    plt.figure()
    scan_df.sort_values(x_col, inplace=False).plot(x=x_col, y=y_col, legend=False)
    if title:
        plt.title(title)
    plt.xlabel(x_col)
    plt.ylabel(y_col)
    plt.show()

def save_sensitivity_csv(scan_df: pd.DataFrame, save_path: Path):
    _ensure_dir(save_path.parent)
    scan_df.to_csv(save_path, index=False)

# -----------------------------------------
# 2) 랜덤 탐색: 히스토리 수집/시각화/저장
# -----------------------------------------
def random_search_tuning_jupyter(
    input_path: Path,
    trials: int = 80,
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    gt_cols: Dict[str,str] = None,
    seed: int = 42,
    log_every: int = 1,
    show_params_every: int = 1,
    # 추가
    record_history: bool = True,
    save_history_csv: Optional[Path] = None,
):
    rng = np.random.default_rng(seed)
    df, subject_items, work_items, subject_etc, work_etc = _prepare_df_for_tuning(
        input_path, result_sheet, subject_tax_sheet, work_tax_sheet
    )
    def sample_params() -> ClassifierParams:
        def pick(spec):
            if len(spec) == 2:
                lo, hi = spec; return float(rng.uniform(lo, hi))
            lo, hi, _ = spec; return int(rng.integers(lo, hi+1))
        pdict = {k: pick(v) for k, v in SPACE.items()}
        return ClassifierParams(**pdict)

    def unsup_objective(dfp: pd.DataFrame) -> float:
        cov = 0.5*(dfp["주제 대분류_pred"].astype(str).str.strip() != "").mean() + \
              0.5*(dfp["작업유형 대분류_pred"].astype(str).str.strip() != "").mean()
        def norm_entropy(col):
            vc = dfp[col].astype(str).value_counts()
            p = (vc / vc.sum()).values
            if len(p) <= 1: return 0.0
            return float(-(p*np.log(p+1e-12)).sum() / math.log(len(p)))
        uniq = 0.5*norm_entropy("주제 대분류_pred") + 0.5*norm_entropy("작업유형 대분류_pred")
        return 0.7*cov + 0.3*uniq

    best_params, best_score, best_metrics = None, -1.0, {}
    disp = display(HTML(_render_bar(0, trials)), display_id=True)
    history_rows = []

    for t in range(1, trials+1):
        params = sample_params()
        pred = _classify_df_once(df, subject_items, work_items, subject_etc, work_etc, params)
        if gt_cols:
            metrics = _accuracy_macro_major_minor(pred, **gt_cols, ignore_blank=True, ignore_etc=False)
            score = metrics.get("macro_avg", np.nan)
            if np.isnan(score):
                score = unsup_objective(pred)
                metrics = {"macro_avg": np.nan, "unsup": score}
        else:
            metrics = {}
            score = unsup_objective(pred)

        if score > best_score:
            _print_html(f"[best@{t}] score={round(float(score),6)}  diff: " + _param_diff(best_params, params))
            best_params, best_score, best_metrics = params, float(score), metrics

        if record_history:
            row = {"trial": t, "score": float(score), **{f"param_{k}": v for k, v in _flatten_params(params).items()}}
            # 주요 지도 지표도 있으면 포함
            for k in ["macro_avg", "subject_major_acc", "subject_minor_acc", "work_major_acc", "work_minor_acc"]:
                if (gt_cols is not None) and (k in metrics) and (metrics[k] is not None) and (not np.isnan(metrics[k])):
                    row[k] = float(metrics[k])
            history_rows.append(row)

        if (t % log_every == 0) or (t == trials):
            disp.update(HTML(_render_bar(t, trials)))
        if (t % show_params_every == 0):
            _print_html(f"[trial {t}] params: {params}")

    history_df = pd.DataFrame(history_rows) if record_history else pd.DataFrame()
    if record_history and (save_history_csv is not None):
        _ensure_dir(Path(save_history_csv).parent)
        history_df.to_csv(save_history_csv, index=False)

    return best_params, best_metrics, history_df

def plot_random_history(history_df: pd.DataFrame,
                        x_col: str = "trial", y_col: str = "score",
                        title: str = "Random Search: score by trial"):
    if history_df.empty:
        print("히스토리가 비어 있습니다.")
        return
    plt.figure()
    history_df.sort_values(x_col, inplace=False).plot(x=x_col, y=y_col, legend=False)
    plt.title(title)
    plt.xlabel(x_col)
    plt.ylabel(y_col)
    plt.show()

# -----------------------------------------
# 3) 하이퍼밴드: 히스토리 DataFrame/시각화/저장
# -----------------------------------------
def _hyperband_history_to_df(info: Dict) -> pd.DataFrame:
    """
    hyperband_tune_jupyter의 반환 info(dict)에서 history 리스트를 DF로 변환
    각 행은 한 번의 평가를 의미합니다.
    """
    hist = info.get("history", [])
    rows = []
    for h in hist:
        pdict = h.get("params", {})
        row = {
            "round": h.get("round"),
            "budget_rows": h.get("budget_rows"),
            "sent_cap": h.get("sent_cap"),
            "score": h.get("score"),
        }
        row.update({f"param_{k}": v for k, v in pdict.items()})
        rows.append(row)
    return pd.DataFrame(rows)

def hyperband_tune_jupyter(
    input_path: Path,
    result_sheet="Result 1",
    subject_tax_sheet="주제 taxonomy",
    work_tax_sheet="작업유형 taxonomy",
    gt_cols: Dict[str,str] = None,
    max_budget_rows: int = 4000,
    min_budget_rows: int = 250,
    eta: int = 3,
    init_candidates: int = 60,
    sent_cap_schedule: List[Optional[int]] = (4, 8, 16, None),
    seed: int = 123,
    show_params_each_eval: bool = False,
    # 추가
    save_history_csv: Optional[Path] = None,
):
    # 원래 구현 본문은 동일…(사용자님의 버전을 그대로 유지)
    # 아래에 있는 기존 함수 본문을 그대로 두고,
    # 마지막 반환 직전에 CSV 저장만 추가합니다.
    rng = np.random.default_rng(seed)
    df, subject_items, work_items, subject_etc, work_etc = _prepare_df_for_tuning(
        input_path, result_sheet, subject_tax_sheet, work_tax_sheet
    )
    N = len(df)
    max_budget_rows = min(N, max_budget_rows)
    min_budget_rows = min(max_budget_rows, min_budget_rows)

    def sample_candidate():
        pdict = {}
        for k, spec in SPACE.items():
            if len(spec) == 2:
                lo, hi = spec; pdict[k] = float(rng.uniform(lo, hi))
            else:
                lo, hi, _ = spec; pdict[k] = int(rng.integers(lo, hi+1))
        return pdict

    alive = [sample_candidate() for _ in range(init_candidates)]

    budgets = []
    b = min_budget_rows
    while b < max_budget_rows:
        budgets.append(b)
        b = int(b * eta)
    if budgets[-1] != max_budget_rows:
        budgets.append(max_budget_rows)

    total_evals, tmp = 0, len(alive)
    for _ in budgets:
        total_evals += max(1, tmp)
        tmp = max(1, tmp // eta)

    done = 0
    disp = display(HTML(_render_bar(done, total_evals)), display_id=True)
    history = []
    best = {"score": -1.0, "params": None}

    for round_idx, budget in enumerate(budgets):
        sent_cap = sent_cap_schedule[min(round_idx, len(sent_cap_schedule)-1)]
        scores = []
        for pdict in alive:
            params = ClassifierParams(**pdict)
            dfx = df.sample(n=budget, random_state=17).reset_index(drop=True) if budget < len(df) else df
            if sent_cap is not None:
                def clip_text(t: str) -> str:
                    sents = [s.strip() for s in SENT_SPLIT.split(str(t)) if s.strip()]
                    if len(sents) > sent_cap: sents = sents[:sent_cap]
                    return ". ".join(sents)
                dfx = dfx.copy()
                dfx["all_text"] = dfx["all_text"].map(clip_text)

            pred = _classify_df_once(dfx, subject_items, work_items, subject_etc, work_etc, params)

            if gt_cols:
                metrics = _accuracy_macro_major_minor(pred, **gt_cols, ignore_blank=True, ignore_etc=False)
                score = metrics.get("macro_avg", np.nan)
                if np.isnan(score): score = 0.0
            else:
                cov = 0.5*(pred["주제 대분류_pred"].astype(str).str.strip() != "").mean() + \
                      0.5*(pred["작업유형 대분류_pred"].astype(str).str.strip() != "").mean()
                def norm_entropy(col):
                    vc = pred[col].astype(str).value_counts()
                    p = (vc / vc.sum()).values
                    if len(p) <= 1: return 0.0
                    return float(-(p*np.log(p+1e-12)).sum() / math.log(len(p)))
                uniq = 0.5*norm_entropy("주제 대분류_pred") + 0.5*norm_entropy("작업유형 대분류_pred")
                score = 0.7*cov + 0.3*uniq

            scores.append(score)
            history.append({"round": round_idx, "budget_rows": budget, "sent_cap": sent_cap,
                            "params": pdict.copy(), "score": float(score)})

            done += 1
            if show_params_each_eval:
                _print_html(f"[r{round_idx} {done}/{total_evals}] score={round(float(score),6)} params: {params}")
            disp.update(HTML(_render_bar(done, total_evals)))

            if float(score) > best["score"]:
                prev = ClassifierParams(**best["params"]) if best["params"] else None
                _print_html(f"[best@r{round_idx} #{done}] score={round(float(score),6)}  diff: " + _param_diff(prev, params))
                best = {"score": float(score), "params": pdict.copy()}

        k = max(1, len(alive) // eta)
        top_idx = list(np.argsort(scores)[::-1][:k])
        alive = [alive[i] for i in top_idx]

    best_params = ClassifierParams(**best["params"]) if best["params"] else ClassifierParams()
    info = {"best": best, "history": history}

    # CSV 저장
    if save_history_csv is not None:
        df_hist = _hyperband_history_to_df(info)
        _ensure_dir(Path(save_history_csv).parent)
        df_hist.to_csv(save_history_csv, index=False)

    return best_params, info

def plot_hyperband_history(info: Dict,
                           title_time: str = "Hyperband: score by evaluation order",
                           title_budget: str = "Hyperband: score vs budget_rows"):
    df_hist = _hyperband_history_to_df(info)
    if df_hist.empty:
        print("히스토리가 비어 있습니다.")
        return
    # 1) 평가 순서 기준 시간경과 곡선
    df_hist2 = df_hist.reset_index().rename(columns={"index": "eval_order"})
    plt.figure()
    df_hist2.plot(x="eval_order", y="score", legend=False)
    plt.title(title_time)
    plt.xlabel("eval_order")
    plt.ylabel("score")
    plt.show()

    # 2) 예산(budget_rows) 대비 점수 산점
    plt.figure()
    df_hist.plot(kind="scatter", x="budget_rows", y="score")
    plt.title(title_budget)
    plt.xlabel("budget_rows")
    plt.ylabel("score")
    plt.show()

# -----------------------------------------
# 4) 최적 결과/히스토리 저장 번들
# -----------------------------------------
def save_best_bundle(save_dir: Path,
                     best_params: ClassifierParams,
                     best_metrics: Optional[Dict] = None,
                     random_history_df: Optional[pd.DataFrame] = None,
                     hyperband_info: Optional[Dict] = None,
                     sensitivity_dfs: Optional[Dict[str, pd.DataFrame]] = None):
    """
    save_dir 하위에 다음 파일을 저장합니다.
    - best_params.json
    - best_metrics.json (있으면)
    - random_history.csv (있으면)
    - hyperband_history.csv (있으면)
    - sensitivity_{name}.csv (있으면)
    """
    _ensure_dir(save_dir)
    # params
    with open(save_dir / "best_params.json", "w", encoding="utf-8") as f:
        json.dump(_flatten_params(best_params), f, ensure_ascii=False, indent=2)
    # metrics
    if best_metrics is not None:
        with open(save_dir / "best_metrics.json", "w", encoding="utf-8") as f:
            json.dump(best_metrics, f, ensure_ascii=False, indent=2)
    # histories
    if random_history_df is not None and not random_history_df.empty:
        random_history_df.to_csv(save_dir / "random_history.csv", index=False)
    if hyperband_info is not None and "history" in hyperband_info:
        df_hist = _hyperband_history_to_df(hyperband_info)
        df_hist.to_csv(save_dir / "hyperband_history.csv", index=False)
    # sensitivity
    if sensitivity_dfs:
        for name, df_s in sensitivity_dfs.items():
            df_s.to_csv(save_dir / f"sensitivity_{name}.csv", index=False)

# -----------------------------------------
# 5) 실행 래퍼에 저장/시각화 옵션 추가
# -----------------------------------------
def run_tuner_with_inputs(
    tuner: str = "hyperband",
    input_path: str = "voc_work.xlsx",
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    # 랜덤 탐색
    trials: int = 120,
    # 하이퍼밴드
    init_candidates: int = 60,
    min_budget_rows: int = 300,
    max_budget_rows: int = 4000,
    eta: int = 3,
    sent_cap_schedule: Tuple[Optional[int], ...] = (4, 8, 16, None),
    # 지도 평가 컬럼(없으면 None)
    gt_cols: Optional[Dict[str, str]] = None,
    # 추가 옵션
    save_dir: Optional[str] = None,
    visualize: bool = True,
):
    input_path = Path(input_path)
    best_params = None
    best_metrics = None
    rnd_hist_df = None
    hb_info = None

    if tuner == "random":
        best_params, best_metrics, rnd_hist_df = random_search_tuning_jupyter(
            input_path=input_path,
            trials=trials,
            result_sheet=result_sheet,
            subject_tax_sheet=subject_tax_sheet,
            work_tax_sheet=work_tax_sheet,
            gt_cols=gt_cols,
            record_history=True,
            save_history_csv=(Path(save_dir)/"random_history.csv" if save_dir else None),
        )
        _print_html(f"[완료] 랜덤 탐색 최적 파라미터: {best_params}")
        if visualize and rnd_hist_df is not None and not rnd_hist_df.empty:
            plot_random_history(rnd_hist_df)
    else:
        best_params, hb_info = hyperband_tune_jupyter(
            input_path=input_path,
            result_sheet=result_sheet,
            subject_tax_sheet=subject_tax_sheet,
            work_tax_sheet=work_tax_sheet,
            gt_cols=gt_cols,
            init_candidates=init_candidates,
            min_budget_rows=min_budget_rows,
            max_budget_rows=max_budget_rows,
            eta=eta,
            sent_cap_schedule=sent_cap_schedule,
            save_history_csv=(Path(save_dir)/"hyperband_history.csv" if save_dir else None),
        )
        _print_html(f"[완료] 하이퍼밴드 최적 파라미터: {best_params}")
        if visualize and hb_info is not None:
            plot_hyperband_history(hb_info)

    if save_dir:
        save_best_bundle(
            save_dir=Path(save_dir),
            best_params=best_params,
            best_metrics=best_metrics,
            random_history_df=rnd_hist_df,
            hyperband_info=hb_info,
        )
    return best_params, (rnd_hist_df if tuner=="random" else hb_info)






best_params, info = run_tuner_with_inputs (
    tuner = "random",
    input_path = Path("Result_1_retagged.xlsx"),
    result_sheet="Result 1",
    subject_tax_sheet="주제 taxonomy",
    work_tax_sheet="작업유형 taxonomy",
    gt_cols={
        "subject_major_gt": "주제 대분류",
        "subject_minor_gt": "주제 중분류",
        "work_major_gt": "작업유형 대분류",
        "work_minor_gt": "작업유형 중분류",
    },
    init_candidates=60, 
    min_budget_rows=300,
    max_budget_rows=4000,
    eta=3,
    sent_cap_schedule=(4, 8, 16, None),
)
best_params





from pathlib import Path

# 최적 파라미터 적용
retagged_df = run_retagging(
    input_path=Path("Result_1_retagged.xlsx"),
    output_path=Path("Result_1_retagged_best_params.xlsx"),
    pos_max=best_params.pos_max,
    pos_min=best_params.pos_min,
    title_boost=best_params.title_boost,
    title_overlap_ratio=best_params.title_overlap_ratio,
    short_len=best_params.short_len,
    long_len=best_params.long_len,
    short_penalty=best_params.short_penalty,
    long_penalty=best_params.long_penalty,
    decay_steps=best_params.decay_steps,
    length_bonus_scale=best_params.length_bonus_scale,
)

# 점수 캘리브레이션
retagged_cal = add_calibrated_scores(retagged_df)
retagged_cal.to_excel("retagged_calibrated.xlsx", index=False)






seed_params = auto_seed_params(Path("Result_1_retagged.xlsx"), result_sheet="Result 1")

scan_df = sensitivity_scan(
    input_path=Path("Result_1_retagged.xlsx"),
    base_params=seed_params,
    param_name="title_boost",
    values=[1.0, 1.5, 2.0, 2.5, 3.0],
    result_sheet="Result 1",
    subject_tax_sheet="주제 taxonomy",
    work_tax_sheet="작업유형 taxonomy",
    gt_cols=None,                  # 라벨 없으면 None 유지
    show_progress=True,            # 진행바 표시
    show_logs=True,                # 로그 출력
    log_every=1,                   # 매 스텝 로그
    show_best_only=True            # 최고점 갱신시에만 로그(깔끔)
)
scan_df.head()




