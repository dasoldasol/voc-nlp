


import os
import pandas as pd
import psycopg2

SQL = """
SELECT
    v.id AS voc_id,
    v.building_id,
    v.voc_date,
    v.title,
    v.request_contents,

    bf.name AS building_floor_name,
    z.name AS zone_name,

    r.reply,
    r.reply_write_date,
    bug.name AS user_group_name,
    a.name AS reply_writer

FROM voc v
LEFT JOIN building_floor bf
  ON v.building_floor_id = bf.id
LEFT JOIN building_floor_zone z
  ON v.building_floor_zone_id = z.id
LEFT JOIN (
    SELECT DISTINCT ON (voc_id) *
    FROM voc_reply
    ORDER BY voc_id, reply_write_date DESC
) r
  ON v.id = r.voc_id
LEFT JOIN account_group ag
  ON r.reply_writer_id = ag.account_id
LEFT JOIN building_user_group bug
  ON ag.group_id = bug.id
LEFT JOIN account a
  ON r.reply_writer_id = a.id
WHERE v.building_id = %(building_id)s
  AND v.title NOT LIKE '%%테스트%%'
  AND v.voc_date >= %(start_date)s
  AND v.voc_date <  %(end_date)s
ORDER BY v.voc_date ASC, v.id ASC;
"""

def fetch_result1_df(building_id: int, start_date: str, end_date: str) -> pd.DataFrame:
    db_host = os.getenv("DB_HOST", "")
    db_port = os.getenv("DB_PORT", "5432")
    db_name = os.getenv("DB_NAME", "")
    db_user = os.getenv("DB_USER", "")
    db_password = os.getenv("DB_PASSWORD", "")
    if not (db_host and db_name and db_user and db_password):
        raise RuntimeError("DB 환경변수가 비어 있습니다. DB_HOST/DB_NAME/DB_USER/DB_PASSWORD를 설정해 주세요.")

    conn = psycopg2.connect(
        host=db_host, port=db_port, dbname=db_name, user=db_user, password=db_password
    )
    try:
        df = pd.read_sql_query(
            SQL,
            conn,
            params={"building_id": building_id, "start_date": start_date, "end_date": end_date},
        )
    finally:
        conn.close()

    # 빈 값 정리(태깅에서 바로 쓰기 좋게)
    for c in ["title", "request_contents", "reply", "reply_writer", "building_floor_name", "zone_name", "user_group_name"]:
        if c in df.columns:
            df[c] = df[c].fillna("")

    return df




# 필요 패키지: pandas, numpy, openpyxl, psycopg2 (미설치 시: pip install pandas numpy openpyxl psycopg2-binary)

import pandas as pd
import numpy as np
import re, html, unicodedata
from pathlib import Path

# ===== DB에서 compound/stop 사전을 읽는 함수 (사용자 제공 코드) =====
def load_text_dict_from_db():
    import psycopg2
    try:
        connection = psycopg2.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, dbname=DB_NAME
        )
        cursor = connection.cursor()

        # compound_words: key, value 모두 strip
        cursor.execute("""
            SELECT word_key, word_value 
            FROM stat_text_dict 
            WHERE type = 'compound' AND is_active = TRUE
        """)
        compound_result = cursor.fetchall()
        compound_words = {
            row[0].strip(): row[1].strip()
            for row in compound_result if row[0] and row[1]
        }

        # stop_words
        cursor.execute("""
            SELECT word_key 
            FROM stat_text_dict 
            WHERE type = 'stop' AND is_active = TRUE
        """)
        stop_result = cursor.fetchall()
        stop_words = {row[0].strip() for row in stop_result if row[0]}

        cursor.close()
        connection.close()
        return compound_words, stop_words

    except Exception:
        # 연결 실패 시 빈 사전/셋으로 진행
        return {}, set()

# ===== 전처리 유틸 =====
def strip_html(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = str(text)
    s = html.unescape(s)
    # <script>/<style> 블록 제거
    s = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", s)
    # 나머지 태그 제거
    s = re.sub(r"(?s)<[^>]+>", " ", s)
    s = html.unescape(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# 허용 문자 화이트리스트
ALLOWED_PATTERN = re.compile(r"[^가-힣ㄱ-ㅎㅏ-ㅣA-Za-z0-9\s\.\,\!\?\;\:\'\"\(\)\[\]\{\}\-\_\/\&\+\%\#\@]")
def whitelist_text(text: str) -> str:
    if text is None or (isinstance(text, float) and pd.isna(text)):
        return ""
    s = strip_html(text)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[\u200B-\u200D\uFE0E\uFE0F]", "", s)  # 제로폭/variation selector 제거
    s = ALLOWED_PATTERN.sub(" ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# ===== taxonomy 파싱 =====
def detect_taxonomy_columns(df: pd.DataFrame):
    cols = list(df.columns)
    def find_first(patterns):
        for c in cols:
            for p in patterns:
                if re.search(p, str(c), re.I):
                    return c
        return None
    major_col = find_first([r"대분류"])
    minor_col = find_first([r"중분류"])
    example_cols = [c for c in cols if re.search(r"사례|예시", str(c), re.I)]
    keyword_cols = [c for c in cols if re.search(r"키워드", str(c), re.I)]
    if not example_cols and not keyword_cols:
        keyword_cols = [c for c in cols if re.search(r"패턴|룰|rule|pattern", str(c), re.I)]
    return major_col, minor_col, example_cols, keyword_cols

def row_patterns(row: pd.Series, cols):
    patterns = []
    for c in cols:
        if c is None or c not in row or pd.isna(row[c]): 
            continue
        val = str(row[c]).strip()
        if not val:
            continue
        parts = re.split(r"[,\;\|\n\r\t]+", val)
        parts = [p.strip() for p in parts if p.strip()]
        if not parts:
            parts = [val]
        patterns.extend(parts)
    return list(dict.fromkeys(patterns))  # 순서 유지 중복 제거

def find_tax_etc(items):
    # taxonomy 안에 실제 존재하는 '기타'만 허용
    for it in items:
        if it["major"] == "기타" and it["minor"] == "기타":
            return ("기타", "기타")
    for it in items:
        if it["major"] == "기타":
            return (it["major"], it["minor"] if it["minor"] else "기타")
    return None

# ===== 토큰/문장 분해 =====
SENT_SPLIT = re.compile(r"[\.!\?…\n\r]+|[。！？]")
TOKEN_RE = re.compile(r"[가-힣A-Za-z0-9]+")

# ===== 길이 가중 =====
def _length_weight(token_count: int, short_len: int, long_len: int, short_penalty: float, long_penalty: float):
    if token_count <= short_len:
        return short_penalty
    if token_count >= long_len:
        return long_penalty
    return 1.0

# ===== 복합어/불용어 정규화 파이프라인 (추가) =====
def build_compound_replacers(compound_words: dict):
    """
    compound_words의 key를 긴 순서로 정렬하여 치환용 정규식 리스트로 컴파일합니다.
    """
    if not compound_words:
        return []
    keys_sorted = sorted([k for k in compound_words.keys() if k], key=len, reverse=True)
    replacers = []
    for k in keys_sorted:
        v = compound_words[k]
        pattern = re.compile(re.escape(k), flags=re.IGNORECASE)
        replacers.append((pattern, v))
    return replacers

def apply_compound(text: str, replacers: list):
    if not text or not replacers:
        return text or ""
    s = text
    for rgx, rep in replacers:
        s = rgx.sub(rep, s)
    return s

def remove_stopwords(text: str, stop_words: set):
    """
    TOKEN_RE 기준으로 토큰화 후 불용어를 제거하고 다시 공백으로 연결합니다.
    """
    if not text:
        return ""
    tokens = TOKEN_RE.findall(text)
    if not tokens:
        return text.strip()
    if not stop_words:
        return " ".join(tokens)
    kept = [t for t in tokens if t not in stop_words]
    return " ".join(kept)

def preprocess_for_match(raw_text: str, compound_replacers: list, stop_words: set):
    """
    매칭/유사도 계산 전용 정규화 파이프라인.
    1) HTML/이상문자 제거 + 화이트리스트(whitelist_text)
    2) NFKC 정규화(whitelist_text 내부)
    3) 소문자화
    4) 복합어 치환
    5) 불용어 제거
    """
    base = whitelist_text(raw_text)
    base = base.lower()
    base = apply_compound(base, compound_replacers)
    base = remove_stopwords(base, stop_words)
    base = re.sub(r"\s+", " ", base).strip()
    return base

# ===== 중요도 기반 분류 (교체된 함수들) =====
def _overlap_with_title(sent_norm: str, title_norm: str, overlap_ratio: float):
    """
    정규화된 문장/제목을 토큰집합으로 비교하여 겹침 비율로 유사도를 판정합니다.
    """
    if not title_norm:
        return False
    stoks = set(TOKEN_RE.findall(sent_norm))
    ttoks = set(TOKEN_RE.findall(title_norm))
    if not stoks or not ttoks:
        return False
    inter = len(stoks & ttoks)
    return inter >= max(1, int(overlap_ratio * len(stoks)))

def build_taxonomy(tax_df: pd.DataFrame, compound_replacers: list = None, stop_words: set = None):
    """
    기존 items에 'norm_patterns'를 추가.
    norm_patterns는 패턴마다 preprocess_for_match를 적용한 결과입니다.
    """
    compound_replacers = compound_replacers or []
    stop_words = stop_words or set()

    major_col, minor_col, ex_cols, kw_cols = detect_taxonomy_columns(tax_df)
    items = []
    for _, row in tax_df.iterrows():
        major = str(row[major_col]).strip() if (major_col and major_col in row and not pd.isna(row[major_col])) else ""
        minor = str(row[minor_col]).strip() if (minor_col and minor_col in row and not pd.isna(row[minor_col])) else ""
        ex_patterns = row_patterns(row, ex_cols) if ex_cols else []
        kw_patterns = row_patterns(row, kw_cols) if kw_cols else []
        patterns = list(dict.fromkeys(ex_patterns + kw_patterns))

        # 패턴 정규화본
        norm_patterns = []
        for p in patterns:
            pn = preprocess_for_match(p, compound_replacers, stop_words)
            if pn:
                norm_patterns.append(pn)

        if major or minor or patterns:
            items.append({
                "major": major,
                "minor": minor,
                "patterns": patterns,
                "norm_patterns": norm_patterns
            })
    return items

def classify_weighted_general(
    all_text: str,
    items: list,
    title_text: str = None,
    # 가중치 파라미터
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # 추가 파라미터
    compound_replacers: list = None,
    stop_words: set = None
):
    """
    반환: (major, minor, score)
    문장 중요도 = 위치 가중 × 제목 유사 가중 × 길이 보정
    항목 점수 = Σ[ 문장가중치 × (패턴매칭개수 + length_bonus_scale * 매칭패턴길이합) ]
    매칭은 '정규화된 문장' 대 '정규화된 패턴(norm_patterns)'로 수행합니다.
    """
    compound_replacers = compound_replacers or []
    stop_words = stop_words or set()

    text = "" if all_text is None else str(all_text)
    sentences = [s.strip() for s in SENT_SPLIT.split(text) if s.strip()]
    if not sentences:
        sentences = [text]

    # 정규화된 제목
    title_norm = preprocess_for_match(title_text or "", compound_replacers, stop_words)

    item_scores = np.zeros(len(items), dtype=float)

    for idx_s, sent in enumerate(sentences):
        # 원문 기준 토큰 길이(길이 보정은 원문 토큰 수로 계산)
        tok_cnt = len(TOKEN_RE.findall(sent.lower()))
        # 매칭/유사도는 정규화 텍스트로
        s_norm = preprocess_for_match(sent, compound_replacers, stop_words)

        # 1) 위치 가중치: pos_max -> pos_min 선형감쇠
        step = min(idx_s, max(1, decay_steps))
        pos_w = pos_max - step * ((pos_max - pos_min) / max(1, decay_steps))
        pos_w = max(pos_min, min(pos_max, pos_w))

        # 2) 제목 유사 가중치
        title_w = (title_boost if _overlap_with_title(s_norm, title_norm, title_overlap_ratio) else 1.0)

        # 3) 길이 보정
        len_w = _length_weight(tok_cnt, short_len, long_len, short_penalty, long_penalty)

        sent_w = pos_w * title_w * len_w

        # 항목별 매칭 점수
        for i, it in enumerate(items):
            norm_ps = it.get("norm_patterns")
            if norm_ps is None:
                # 하위호환: patterns를 그때그때 정규화
                norm_ps = [preprocess_for_match(p, compound_replacers, stop_words) for p in it.get("patterns", [])]

            hits = 0
            length_bonus = 0
            for p in norm_ps:
                if not p:
                    continue
                if p in s_norm:
                    hits += 1
                    length_bonus += len(p)
            if hits > 0:
                item_scores[i] += sent_w * (hits + length_bonus_scale * length_bonus)

    if item_scores.max() > 0:
        best_i = int(item_scores.argmax())
        return items[best_i]["major"], items[best_i]["minor"], float(item_scores[best_i])
    return "", "", 0.0

# ===== 재태깅 메인 (교체된 run_retagging) =====
def run_retagging(
    input_path: Path,
    output_path: Path,
    result_sheet: str = "Result 1",
    subject_tax_sheet: str = "주제 taxonomy",
    work_tax_sheet: str = "작업유형 taxonomy",
    # 분류기 파라미터
    pos_max: float = 1.5,
    pos_min: float = 1.0,
    title_boost: float = 2.0,
    title_overlap_ratio: float = 0.3,
    short_len: int = 5,
    long_len: int = 60,
    short_penalty: float = 0.9,
    long_penalty: float = 0.9,
    decay_steps: int = 5,
    length_bonus_scale: float = 0.1,
    # 사전 주입
    compound_words: dict = None,
    stop_words: set = None
) -> pd.DataFrame:
    """
    입력 파일의 taxonomy 시트를 그대로 사용하여 Result 1 시트를 재태깅하고,
    output_path로 모든 시트를 보존하여 저장합니다.
    반환값: 갱신된 Result 1 데이터프레임
    """
    compound_words = compound_words or {}
    stop_words = stop_words or set()
    compound_replacers = build_compound_replacers(compound_words)

    xls = pd.ExcelFile(input_path)
    sheet_names = xls.sheet_names

    result_df = pd.read_excel(xls, sheet_name=result_sheet, header=0, dtype=str)
    subject_tax = pd.read_excel(xls, sheet_name=subject_tax_sheet, header=0, dtype=str)
    work_tax    = pd.read_excel(xls, sheet_name=work_tax_sheet, header=0, dtype=str)

    for col in ["title", "request_contents", "reply"]:
        if col not in result_df.columns:
            raise KeyError(f"필수 컬럼이 없습니다: {col}")

    # all_text 구성 (원문 보존). SettingWithCopyWarning 방지 위해 .loc 사용
    result_df.loc[:, "title_clean"] = result_df["title"].map(whitelist_text).fillna("")
    result_df.loc[:, "all_text"] = (
        result_df["request_contents"].map(whitelist_text).fillna("") + " " +
        result_df["title_clean"] + " " +
        result_df["reply"].map(whitelist_text).fillna("")
    ).str.strip()

    # taxonomy 아이템: 정규화 패턴 포함
    subject_items = build_taxonomy(subject_tax, compound_replacers, stop_words)
    work_items    = build_taxonomy(work_tax, compound_replacers, stop_words)
    subject_etc   = find_tax_etc(subject_items)
    work_etc      = find_tax_etc(work_items)

    # 분류
    sub_major, sub_minor, work_major, work_minor = [], [], [], []
    sub_score, work_score = [], []

    for all_text, title_text in zip(result_df["all_text"], result_df["title_clean"]):
        mj, mn, sc = classify_weighted_general(
            all_text, subject_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj == "":
            mj, mn = (subject_etc if subject_etc is not None else ("", ""))
        sub_major.append(mj); sub_minor.append(mn); sub_score.append(sc)

        mj2, mn2, sc2 = classify_weighted_general(
            all_text, work_items, title_text=title_text,
            pos_max=pos_max, pos_min=pos_min,
            title_boost=title_boost, title_overlap_ratio=title_overlap_ratio,
            short_len=short_len, long_len=long_len,
            short_penalty=short_penalty, long_penalty=long_penalty,
            decay_steps=decay_steps, length_bonus_scale=length_bonus_scale,
            compound_replacers=compound_replacers, stop_words=stop_words
        )
        if mj2 == "":
            mj2, mn2 = (work_etc if work_etc is not None else ("", ""))
        work_major.append(mj2); work_minor.append(mn2); work_score.append(sc2)

    result_df.loc[:, "주제 대분류"] = sub_major
    result_df.loc[:, "주제 중분류"] = sub_minor
    result_df.loc[:, "작업유형 대분류"] = work_major
    result_df.loc[:, "작업유형 중분류"] = work_minor
    # 점수는 참고용
    result_df.loc[:, "주제_score"] = sub_score
    result_df.loc[:, "작업유형_score"] = work_score

    # 모든 시트 유지하여 저장
    with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
        for name in sheet_names:
            if name == result_sheet:
                result_df.to_excel(writer, index=False, sheet_name=name)
            else:
                pd.read_excel(xls, sheet_name=name).to_excel(writer, index=False, sheet_name=name)

    return result_df
